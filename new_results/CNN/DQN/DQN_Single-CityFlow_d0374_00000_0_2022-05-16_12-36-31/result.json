{"episode_reward_max": 81202.0, "episode_reward_min": 76740.0, "episode_reward_mean": 78934.66666666667, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0], "episode_lengths": [300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.29097546587933554, "mean_inference_ms": 2.2103407761671923, "mean_action_processing_ms": 0.07487081743025041, "mean_env_wait_ms": 7.875651865453274, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 1000, "timesteps_this_iter": 32, "agent_timesteps_total": 1000, "timers": {"load_time_ms": 0.187, "load_throughput": 171196.082, "learn_time_ms": 56.695, "learn_throughput": 564.424}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.642088413238525, "mean_q": -0.03934118151664734, "min_q": -0.24921518564224243, "max_q": 0.16789372265338898, "cur_lr": 0.6}, "td_error": [-302.037353515625, -316.3406982421875, -334.0383605957031, -303.9997863769531, -340.2201232910156, -56.21808624267578, -135.99891662597656, -278.4127502441406, -198.33444213867188, -260.17144775390625, -222.1431427001953, -308.0350036621094, -176.33380126953125, -287.9997863769531, -316.41204833984375, -302.14239501953125, -306.337158203125, -329.99688720703125, -258.413330078125, -298.1410827636719, -290.2168273925781, -290.2181091308594, -288.33453369140625, -330.3321533203125, -216.2156982421875, -334.4112243652344, -314.3359375, -105.99974822998047, -358.21978759765625, -2.17659068107605, -295.9982604980469, -308.33221435546875], "mean_td_error": -264.57867431640625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 1000, "num_agent_steps_sampled": 1000, "num_steps_trained": 32, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 32, "last_target_update_ts": 1000, "num_target_updates": 1}, "done": false, "episodes_total": 3, "training_iteration": 1, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-36-51", "timestamp": 1652704611, "time_this_iter_s": 10.7895348072052, "time_total_s": 10.7895348072052, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 10.7895348072052, "timesteps_since_restore": 32, "iterations_since_restore": 1, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 24.362499999999997, "ram_util_percent": 11.8125}}
{"episode_reward_max": 85116.0, "episode_reward_min": 76740.0, "episode_reward_mean": 80592.66666666667, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0], "episode_lengths": [300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2908167846461144, "mean_inference_ms": 2.245529907268096, "mean_action_processing_ms": 0.0751439699961607, "mean_env_wait_ms": 8.070439837318377, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 2000, "timesteps_this_iter": 32, "agent_timesteps_total": 2000, "timers": {"load_time_ms": 0.22, "load_throughput": 145477.702, "learn_time_ms": 35.038, "learn_throughput": 913.287}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.013952482491731644, "mean_q": 18.359561920166016, "min_q": -13.31595516204834, "max_q": 35.986148834228516, "cur_lr": 0.6}, "td_error": [-59190744.0, -59190780.0, -59190780.0, -59190768.0, -59190832.0, -59190756.0, -59190732.0, -59190680.0, -59190780.0, -59190780.0, -59190776.0, -59190804.0, -59190832.0, -59190836.0, -59190768.0, -59190756.0, -59190832.0, -59190732.0, -59190748.0, -59190788.0, -59190756.0, -59190756.0, -59190660.0, -59190768.0, -59190780.0, -59190768.0, -59190748.0, -59190744.0, -59190776.0, -59190524.0, -59190776.0, -59190776.0], "mean_td_error": -59190764.0, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 2000, "num_agent_steps_sampled": 2000, "num_steps_trained": 8032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 8032, "last_target_update_ts": 2000, "num_target_updates": 3}, "done": false, "episodes_total": 6, "training_iteration": 2, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-37-13", "timestamp": 1652704633, "time_this_iter_s": 22.15822434425354, "time_total_s": 32.94775915145874, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 32.94775915145874, "timesteps_since_restore": 64, "iterations_since_restore": 2, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.270967741935483, "ram_util_percent": 11.99677419354839}}
{"episode_reward_max": 88956.0, "episode_reward_min": 76740.0, "episode_reward_mean": 82690.6, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2882947186076986, "mean_inference_ms": 2.254916857414416, "mean_action_processing_ms": 0.07476077982822102, "mean_env_wait_ms": 8.198175148375844, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 3000, "timesteps_this_iter": 32, "agent_timesteps_total": 3000, "timers": {"load_time_ms": 0.178, "load_throughput": 179771.937, "learn_time_ms": 26.97, "learn_throughput": 1186.511}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.09910166263580322, "mean_q": 160.99795532226562, "min_q": 96.48888397216797, "max_q": 260.8193359375, "cur_lr": 0.6}, "td_error": [-270.3922119140625, -291.49395751953125, -307.8978271484375, -312.8570556640625, -378.51800537109375, -390.78558349609375, -315.49395751953125, -325.18988037109375, -382.78558349609375, -415.5942077636719, -254.45513916015625, -331.8978271484375, -400.51800537109375, -222.8570556640625, -270.8570556640625, -285.49395751953125, -381.5942077636719, -291.18988037109375, -295.5942077636719, -354.51800537109375, -430.78558349609375, -397.18988037109375, -408.51800537109375, -407.5942077636719, -389.18988037109375, -262.45513916015625, -428.51800537109375, -408.51800537109375, -371.5942077636719, -288.8570556640625, -242.45513916015625, -249.1898651123047], "mean_td_error": -336.4015197753906, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 3000, "num_agent_steps_sampled": 3000, "num_steps_trained": 16032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 16032, "last_target_update_ts": 3000, "num_target_updates": 5}, "done": false, "episodes_total": 10, "training_iteration": 3, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-37-35", "timestamp": 1652704655, "time_this_iter_s": 21.43983793258667, "time_total_s": 54.38759708404541, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 54.38759708404541, "timesteps_since_restore": 96, "iterations_since_restore": 3, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 24.312903225806448, "ram_util_percent": 11.903225806451607}}
{"episode_reward_max": 89568.0, "episode_reward_min": 76740.0, "episode_reward_mean": 83802.46153846153, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.28729157616371487, "mean_inference_ms": 2.260463558386351, "mean_action_processing_ms": 0.07462420946534087, "mean_env_wait_ms": 8.220534561306728, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 4000, "timesteps_this_iter": 32, "agent_timesteps_total": 4000, "timers": {"load_time_ms": 0.239, "load_throughput": 133696.312, "learn_time_ms": 36.693, "learn_throughput": 872.091}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.08841519057750702, "mean_q": 270.64056396484375, "min_q": 176.41445922851562, "max_q": 430.00469970703125, "cur_lr": 0.6}, "td_error": [-374.1322326660156, -428.21270751953125, -474.276611328125, -480.873046875, -438.276611328125, -512.5439453125, -265.97003173828125, -318.873046875, -303.97003173828125, -370.1322326660156, -352.169677734375, -487.8379821777344, -159.97006225585938, -376.1322326660156, -466.1322326660156, -452.5439453125, -472.276611328125, -264.24774169921875, -462.5439453125, -287.97003173828125, -487.8379821777344, -350.169677734375, -372.21270751953125, -364.169677734375, -346.169677734375, -224.24774169921875, -372.169677734375, -258.24774169921875, -466.5439453125, -512.5439453125, -313.97003173828125, -386.21270751953125], "mean_td_error": -381.36187744140625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 4000, "num_agent_steps_sampled": 4000, "num_steps_trained": 24032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 24032, "last_target_update_ts": 4000, "num_target_updates": 7}, "evaluation": {"episode_reward_max": 31502.0, "episode_reward_min": 31502.0, "episode_reward_mean": 31502.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [31502.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.15333482989441122, "mean_inference_ms": 1.7809954988600012, "mean_action_processing_ms": 0.06441658121406833, "mean_env_wait_ms": 1.2996418531550917, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 13, "training_iteration": 4, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-37-57", "timestamp": 1652704677, "time_this_iter_s": 22.46364140510559, "time_total_s": 76.851238489151, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 76.851238489151, "timesteps_since_restore": 128, "iterations_since_restore": 4, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.646875, "ram_util_percent": 12.009374999999999}}
{"episode_reward_max": 89568.0, "episode_reward_min": 76740.0, "episode_reward_mean": 84025.875, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.28588915053041336, "mean_inference_ms": 2.262346362604887, "mean_action_processing_ms": 0.07447280135062759, "mean_env_wait_ms": 8.195487431115613, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 5000, "timesteps_this_iter": 32, "agent_timesteps_total": 5000, "timers": {"load_time_ms": 0.235, "load_throughput": 135957.99, "learn_time_ms": 35.686, "learn_throughput": 896.719}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.07992822676897049, "mean_q": 379.102783203125, "min_q": 254.70730590820312, "max_q": 632.3645629882812, "cur_lr": 0.6}, "td_error": [-403.4733581542969, -362.9131164550781, -485.4733581542969, -520.9130859375, -534.9130859375, -216.5313720703125, -454.455078125, -516.7200317382812, -470.9131164550781, -486.72003173828125, -504.455078125, -568.1885986328125, -294.9131164550781, -576.825439453125, -70.5313720703125, -498.72003173828125, -536.825439453125, -258.5313720703125, -82.5313720703125, -445.4733581542969, -349.4733581542969, -448.455078125, -514.825439453125, -476.9131164550781, -447.4733581542969, -471.4733581542969, -592.1885986328125, -385.8316650390625, -206.5313720703125, -498.455078125, -588.825439453125, -426.9131164550781], "mean_td_error": -428.0431213378906, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 5000, "num_agent_steps_sampled": 5000, "num_steps_trained": 32032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 32032, "last_target_update_ts": 5000, "num_target_updates": 9}, "done": false, "episodes_total": 16, "training_iteration": 5, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-38-17", "timestamp": 1652704697, "time_this_iter_s": 20.29292392730713, "time_total_s": 97.14416241645813, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 97.14416241645813, "timesteps_since_restore": 160, "iterations_since_restore": 5, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.306896551724137, "ram_util_percent": 12.168965517241377}}
{"episode_reward_max": 89568.0, "episode_reward_min": 76740.0, "episode_reward_mean": 84443.1, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.283190581561065, "mean_inference_ms": 2.2526731923906294, "mean_action_processing_ms": 0.07401565608795362, "mean_env_wait_ms": 8.107312170236932, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 6000, "timesteps_this_iter": 32, "agent_timesteps_total": 6000, "timers": {"load_time_ms": 0.238, "load_throughput": 134378.983, "learn_time_ms": 35.849, "learn_throughput": 892.624}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.09984508156776428, "mean_q": 477.0299072265625, "min_q": 338.57940673828125, "max_q": 859.05908203125, "cur_lr": 0.6}, "td_error": [-610.0977783203125, -638.0978393554688, -527.7775268554688, -477.04443359375, -529.0750732421875, -597.6832275390625, -601.6832275390625, -686.3361206054688, -610.3907470703125, -349.04437255859375, -545.3734130859375, -579.0750732421875, -698.3361206054688, -630.0978393554688, -343.6832580566406, -513.7775268554688, -227.8564453125, -465.04437255859375, -195.8564453125, -493.04443359375, -616.0978393554688, -686.3361206054688, -507.77752685546875, -618.0978393554688, -121.85638427734375, -549.777587890625, -646.0978393554688, -630.3907470703125, -601.777587890625, -713.37353515625, -419.04437255859375, -656.3360595703125], "mean_td_error": -533.9480590820312, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 6000, "num_agent_steps_sampled": 6000, "num_steps_trained": 40032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 40032, "last_target_update_ts": 6000, "num_target_updates": 11}, "done": false, "episodes_total": 20, "training_iteration": 6, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-38-35", "timestamp": 1652704715, "time_this_iter_s": 17.515552759170532, "time_total_s": 114.65971517562866, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 114.65971517562866, "timesteps_since_restore": 192, "iterations_since_restore": 6, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.676000000000002, "ram_util_percent": 12.331999999999999}}
{"episode_reward_max": 91740.0, "episode_reward_min": 76740.0, "episode_reward_mean": 84836.78260869565, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.28138011250067124, "mean_inference_ms": 2.246017530529538, "mean_action_processing_ms": 0.07372164987617645, "mean_env_wait_ms": 8.02565770950407, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 7000, "timesteps_this_iter": 32, "agent_timesteps_total": 7000, "timers": {"load_time_ms": 0.222, "load_throughput": 143871.506, "learn_time_ms": 32.982, "learn_throughput": 970.231}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.08283604681491852, "mean_q": 641.6280517578125, "min_q": 420.60406494140625, "max_q": 1084.833740234375, "cur_lr": 0.6}, "td_error": [-623.5831909179688, -625.4478759765625, -801.5831909179688, -788.4996948242188, -214.57958984375, -731.092529296875, -678.3256225585938, -186.57958984375, -827.5831909179688, -629.7504272460938, -862.8092651367188, -779.092529296875, -200.57958984375, -779.4478759765625, -563.4478759765625, -777.4478759765625, -761.092529296875, -373.69146728515625, -659.6914672851562, -222.57958984375, -218.57958984375, -753.7504272460938, -190.57958984375, -683.5831909179688, -674.3256225585938, -778.4996948242188, -841.5831909179688, -733.092529296875, -858.8092651367188, -749.7504272460938, -228.57958984375, -705.092529296875], "mean_td_error": -609.4727783203125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 7000, "num_agent_steps_sampled": 7000, "num_steps_trained": 48032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 48032, "last_target_update_ts": 7000, "num_target_updates": 13}, "done": false, "episodes_total": 23, "training_iteration": 7, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-38-53", "timestamp": 1652704733, "time_this_iter_s": 18.323455333709717, "time_total_s": 132.98317050933838, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 132.98317050933838, "timesteps_since_restore": 224, "iterations_since_restore": 7, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.657692307692308, "ram_util_percent": 12.461538461538462}}
{"episode_reward_max": 91740.0, "episode_reward_min": 76740.0, "episode_reward_mean": 84575.53846153847, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.279485698597408, "mean_inference_ms": 2.2376040180654786, "mean_action_processing_ms": 0.0733958733890683, "mean_env_wait_ms": 7.9276190012076695, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 8000, "timesteps_this_iter": 32, "agent_timesteps_total": 8000, "timers": {"load_time_ms": 0.286, "load_throughput": 111885.402, "learn_time_ms": 35.609, "learn_throughput": 898.641}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.07019732892513275, "mean_q": 725.0172729492188, "min_q": 504.61328125, "max_q": 1333.793701171875, "cur_lr": 0.6}, "td_error": [-992.0015869140625, -214.8211669921875, -1022.0015869140625, -198.8211669921875, -798.4633178710938, -915.9439086914062, -989.9439086914062, -839.0411987304688, -911.79248046875, -930.938720703125, -605.0411987304688, -1006.0015869140625, -224.8211669921875, -774.97802734375, -74.8211669921875, -744.4633178710938, -899.79248046875, -907.79248046875, -1004.0015869140625, -886.97802734375, -915.79248046875, -911.79248046875, -984.0015869140625, -901.0411987304688, -670.4633178710938, -933.79248046875, -226.8211669921875, -717.9551391601562, -907.9439086914062, -867.0411987304688, -863.0411987304688, -852.97802734375], "mean_td_error": -771.7225952148438, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 8000, "num_agent_steps_sampled": 8000, "num_steps_trained": 56032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 56032, "last_target_update_ts": 8000, "num_target_updates": 15}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.1515790745739929, "mean_inference_ms": 1.7715032009435772, "mean_action_processing_ms": 0.06472965246825765, "mean_env_wait_ms": 1.2826145984567145, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 26, "training_iteration": 8, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-39-11", "timestamp": 1652704751, "time_this_iter_s": 17.750333547592163, "time_total_s": 150.73350405693054, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 150.73350405693054, "timesteps_since_restore": 256, "iterations_since_restore": 8, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.372, "ram_util_percent": 12.623999999999999}}
{"episode_reward_max": 91740.0, "episode_reward_min": 67848.0, "episode_reward_mean": 83414.33333333333, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.27755196402120863, "mean_inference_ms": 2.230821524533587, "mean_action_processing_ms": 0.07314930429859319, "mean_env_wait_ms": 7.786831848852378, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 9000, "timesteps_this_iter": 32, "agent_timesteps_total": 9000, "timers": {"load_time_ms": 0.241, "load_throughput": 132849.379, "learn_time_ms": 36.995, "learn_throughput": 864.987}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.06587071716785431, "mean_q": 900.5352172851562, "min_q": 592.7459106445312, "max_q": 1582.0882568359375, "cur_lr": 0.6}, "td_error": [-1045.1513671875, -1043.982666015625, -1023.9827270507812, -896.9224853515625, -1089.14111328125, -215.3828125, -1033.982666015625, -843.1513671875, -1029.982666015625, -211.3828125, -131.3828125, -211.3828125, -1095.14111328125, -1031.982666015625, -819.9827270507812, -868.9224853515625, -171.3828125, -999.9827270507812, -1041.93701171875, -978.3307495117188, -1127.14111328125, -870.9224853515625, -1190.72509765625, -987.93701171875, -866.9224853515625, -991.9827270507812, 367.5340576171875, -1010.7159423828125, -736.9224853515625, -169.3828125, -1071.1513671875, -1006.7159423828125], "mean_td_error": -795.2023315429688, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 9000, "num_agent_steps_sampled": 9000, "num_steps_trained": 64032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 64032, "last_target_update_ts": 9000, "num_target_updates": 17}, "done": false, "episodes_total": 30, "training_iteration": 9, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-39-29", "timestamp": 1652704769, "time_this_iter_s": 17.955509424209595, "time_total_s": 168.68901348114014, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 168.68901348114014, "timesteps_since_restore": 288, "iterations_since_restore": 9, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.51153846153846, "ram_util_percent": 12.75}}
{"episode_reward_max": 91740.0, "episode_reward_min": 47806.0, "episode_reward_mean": 80737.09090909091, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.27626322748827714, "mean_inference_ms": 2.2270122752812376, "mean_action_processing_ms": 0.07301791410718292, "mean_env_wait_ms": 7.663018409079172, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 10000, "timesteps_this_iter": 32, "agent_timesteps_total": 10000, "timers": {"load_time_ms": 0.233, "load_throughput": 137054.762, "learn_time_ms": 36.908, "learn_throughput": 867.02}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.10316728055477142, "mean_q": 1029.2125244140625, "min_q": 687.8824462890625, "max_q": 1829.302490234375, "cur_lr": 0.6}, "td_error": [-1109.802734375, -1091.321044921875, -1028.3033447265625, -1217.43017578125, -193.0262451171875, -1149.321044921875, -1145.8026123046875, -931.4301147460938, -1121.321044921875, -973.802734375, -1159.212890625, -1079.802734375, -1028.3033447265625, -221.026123046875, -121.0262451171875, -1141.321044921875, -1177.212890625, -1165.321044921875, -1116.916748046875, -1342.4462890625, -109.0262451171875, -1220.916748046875, -1187.43017578125, -1126.4462890625, -1088.30322265625, -1127.802734375, -1111.802734375, -177.0262451171875, -1090.916748046875, -51.0262451171875, -1162.4227294921875, -1302.4462890625], "mean_td_error": -945.9285888671875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 10000, "num_agent_steps_sampled": 10000, "num_steps_trained": 72032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 72032, "last_target_update_ts": 10000, "num_target_updates": 19}, "done": false, "episodes_total": 33, "training_iteration": 10, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-39-45", "timestamp": 1652704785, "time_this_iter_s": 15.63987398147583, "time_total_s": 184.32888746261597, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 184.32888746261597, "timesteps_since_restore": 320, "iterations_since_restore": 10, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.96818181818182, "ram_util_percent": 12.881818181818181}}
{"episode_reward_max": 91740.0, "episode_reward_min": 37012.0, "episode_reward_mean": 77252.72222222222, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2750424993439967, "mean_inference_ms": 2.2230628581115424, "mean_action_processing_ms": 0.07288984642084723, "mean_env_wait_ms": 7.523901954798711, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 11000, "timesteps_this_iter": 32, "agent_timesteps_total": 11000, "timers": {"load_time_ms": 0.233, "load_throughput": 137489.99, "learn_time_ms": 34.838, "learn_throughput": 918.526}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.06614509224891663, "mean_q": 1165.3115234375, "min_q": 782.819091796875, "max_q": 2092.828125, "cur_lr": 0.6}, "td_error": [-1278.37939453125, -1220.9405517578125, -211.89599609375, -1163.4599609375, -175.89599609375, -1248.9405517578125, -1098.2276611328125, -211.89599609375, -1011.8154296875, -1388.72607421875, -1412.72607421875, -1189.4599609375, -1461.905029296875, -1052.51318359375, -1234.9405517578125, -1355.8153076171875, -1244.2275390625, -1507.905029296875, -197.89599609375, -1085.4599609375, -1163.4599609375, -1216.9405517578125, -1304.5130615234375, -1332.5130615234375, -191.89599609375, -1163.4599609375, -1030.9405517578125, -1336.72607421875, -207.89599609375, -1370.72607421875, -1318.37939453125, -1408.72607421875], "mean_td_error": -1056.22509765625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 11000, "num_agent_steps_sampled": 11000, "num_steps_trained": 80032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 80032, "last_target_update_ts": 11000, "num_target_updates": 21}, "done": false, "episodes_total": 36, "training_iteration": 11, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-39-59", "timestamp": 1652704799, "time_this_iter_s": 14.416985511779785, "time_total_s": 198.74587297439575, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 198.74587297439575, "timesteps_since_restore": 352, "iterations_since_restore": 11, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 21.790476190476188, "ram_util_percent": 13.038095238095238}}
{"episode_reward_max": 91740.0, "episode_reward_min": 34198.0, "episode_reward_mean": 73263.95, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2734029958885994, "mean_inference_ms": 2.217098039961388, "mean_action_processing_ms": 0.07270076482283254, "mean_env_wait_ms": 7.334643808024964, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 12000, "timesteps_this_iter": 32, "agent_timesteps_total": 12000, "timers": {"load_time_ms": 0.131, "load_throughput": 244387.706, "learn_time_ms": 19.44, "learn_throughput": 1646.086}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.06582982838153839, "mean_q": 1235.9691162109375, "min_q": 867.5355834960938, "max_q": 2351.8193359375, "cur_lr": 0.6}, "td_error": [-1386.691162109375, -142.6328125, -1640.91650390625, -1460.7518310546875, -1544.934814453125, -1443.63916015625, -1428.7518310546875, -1477.1343994140625, -186.6328125, -1299.013916015625, -1528.934814453125, -1157.63916015625, -1229.1343994140625, -1422.691162109375, -1706.91650390625, -1532.91650390625, -1410.934814453125, -1470.7518310546875, 63.3671875, -1344.691162109375, -78.6328125, -1528.934814453125, -1561.1343994140625, -1586.934814453125, -1342.7518310546875, -1420.691162109375, -1511.787353515625, 83.3671875, -1420.7518310546875, -1532.934814453125, -1233.013916015625, -1334.91650390625], "mean_td_error": -1225.670654296875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 12000, "num_agent_steps_sampled": 12000, "num_steps_trained": 88032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 88032, "last_target_update_ts": 12000, "num_target_updates": 23}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13822164969491904, "mean_inference_ms": 1.5812825151077252, "mean_action_processing_ms": 0.05726200362024508, "mean_env_wait_ms": 1.158486725090611, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 40, "training_iteration": 12, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-40-14", "timestamp": 1652704814, "time_this_iter_s": 14.251718044281006, "time_total_s": 212.99759101867676, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 212.99759101867676, "timesteps_since_restore": 384, "iterations_since_restore": 12, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.705000000000002, "ram_util_percent": 13.159999999999997}}
{"episode_reward_max": 91740.0, "episode_reward_min": 34198.0, "episode_reward_mean": 70987.58139534884, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.27225764176658146, "mean_inference_ms": 2.212998172307715, "mean_action_processing_ms": 0.07257342283251034, "mean_env_wait_ms": 7.194703170935346, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 13000, "timesteps_this_iter": 32, "agent_timesteps_total": 13000, "timers": {"load_time_ms": 0.234, "load_throughput": 136719.698, "learn_time_ms": 34.772, "learn_throughput": 920.289}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.06670135259628296, "mean_q": 1511.2615966796875, "min_q": 962.2677001953125, "max_q": 2595.720947265625, "cur_lr": 0.6}, "td_error": [-1511.6212158203125, -1813.3621826171875, -243.908935546875, -1777.3621826171875, -1541.553466796875, -1495.554931640625, -1722.1212158203125, -1498.29443359375, -1658.1212158203125, -181.908935546875, -1620.1212158203125, -1524.29443359375, -1605.096923828125, -1641.553466796875, -1726.1212158203125, -1603.096923828125, -1817.3621826171875, -1625.9066162109375, -213.908935546875, -1525.554931640625, -1708.1212158203125, -1404.29443359375, -209.908935546875, -1535.6212158203125, -1607.6212158203125, -213.908935546875, -1204.29443359375, -221.908935546875, -187.908935546875, -187.908935546875, -1393.554931640625, -1607.9066162109375], "mean_td_error": -1244.6807861328125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 13000, "num_agent_steps_sampled": 13000, "num_steps_trained": 96032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 96032, "last_target_update_ts": 13000, "num_target_updates": 25}, "done": false, "episodes_total": 43, "training_iteration": 13, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-40-28", "timestamp": 1652704828, "time_this_iter_s": 14.462106943130493, "time_total_s": 227.45969796180725, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 227.45969796180725, "timesteps_since_restore": 416, "iterations_since_restore": 13, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 21.728571428571428, "ram_util_percent": 13.319047619047618}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 68691.78260869565, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.27124357002347077, "mean_inference_ms": 2.20958028791574, "mean_action_processing_ms": 0.07249432012914016, "mean_env_wait_ms": 7.056003124435459, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 14000, "timesteps_this_iter": 32, "agent_timesteps_total": 14000, "timers": {"load_time_ms": 0.135, "load_throughput": 237259.551, "learn_time_ms": 20.738, "learn_throughput": 1543.034}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.06149034947156906, "mean_q": 1480.552490234375, "min_q": 1042.1494140625, "max_q": 2840.927490234375, "cur_lr": 0.6}, "td_error": [-1555.223876953125, -1598.1812744140625, -1428.1871337890625, -198.60498046875, -92.60498046875, -1578.016845703125, -1780.6556396484375, -1602.016845703125, -1749.1326904296875, -1655.223876953125, -1697.223876953125, -1752.1871337890625, -1542.6556396484375, -1610.1812744140625, -1785.1326904296875, -1718.1871337890625, -1817.463623046875, -1484.1812744140625, -1673.223876953125, -1748.1871337890625, -1764.1871337890625, -1746.1871337890625, -1949.383056640625, -1636.016845703125, -1370.1812744140625, -196.60498046875, -1819.463623046875, -1521.223876953125, -1726.1871337890625, -1810.6556396484375, -1746.1871337890625, -198.60498046875], "mean_td_error": -1485.9798583984375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 14000, "num_agent_steps_sampled": 14000, "num_steps_trained": 104032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 104032, "last_target_update_ts": 14000, "num_target_updates": 27}, "done": false, "episodes_total": 46, "training_iteration": 14, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-40-43", "timestamp": 1652704843, "time_this_iter_s": 14.880870819091797, "time_total_s": 242.34056878089905, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 242.34056878089905, "timesteps_since_restore": 448, "iterations_since_restore": 14, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.366666666666667, "ram_util_percent": 13.447619047619046}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 66287.84, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.27000184863257615, "mean_inference_ms": 2.205215562913404, "mean_action_processing_ms": 0.0724040738203236, "mean_env_wait_ms": 6.878538085276873, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 15000, "timesteps_this_iter": 32, "agent_timesteps_total": 15000, "timers": {"load_time_ms": 0.252, "load_throughput": 127100.121, "learn_time_ms": 39.501, "learn_throughput": 810.106}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.06647883355617523, "mean_q": 1694.9749755859375, "min_q": 1152.7288818359375, "max_q": 3096.1826171875, "cur_lr": 0.6}, "td_error": [-1715.1944580078125, -1867.907470703125, -1895.422607421875, -1740.4107666015625, -1577.422607421875, -1766.4107666015625, -1448.5089111328125, -179.4921875, -157.4921875, -1863.422607421875, -1926.9967041015625, -1729.422607421875, -2096.94580078125, -1956.9967041015625, -1849.1944580078125, -219.4921875, -2098.94580078125, -1696.5089111328125, -1933.422607421875, -1811.59716796875, -115.4921875, -1750.4107666015625, -171.4921875, -1740.4107666015625, -1865.422607421875, -1938.9967041015625, -1664.5089111328125, -1883.422607421875, 120.5078125, -1703.59716796875, -1788.9459228515625, -1912.9967041015625], "mean_td_error": -1498.3248291015625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 15000, "num_agent_steps_sampled": 15000, "num_steps_trained": 112032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 112032, "last_target_update_ts": 15000, "num_target_updates": 29}, "done": false, "episodes_total": 50, "training_iteration": 15, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-40-57", "timestamp": 1652704857, "time_this_iter_s": 14.08647108078003, "time_total_s": 256.4270398616791, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 256.4270398616791, "timesteps_since_restore": 480, "iterations_since_restore": 15, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 21.84, "ram_util_percent": 13.570000000000002}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 64881.622641509435, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2689989390567257, "mean_inference_ms": 2.200892538416714, "mean_action_processing_ms": 0.07229461378403794, "mean_env_wait_ms": 6.751037894047565, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 16000, "timesteps_this_iter": 32, "agent_timesteps_total": 16000, "timers": {"load_time_ms": 0.221, "load_throughput": 144990.524, "learn_time_ms": 33.696, "learn_throughput": 949.661}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0625687912106514, "mean_q": 1847.1591796875, "min_q": 1261.064697265625, "max_q": 3350.25244140625, "cur_lr": 0.6}, "td_error": [-2132.3017578125, -205.114013671875, -1986.702392578125, -2065.630126953125, -1941.630126953125, -2061.630126953125, -153.114013671875, -2038.702392578125, -1850.3411865234375, -201.114013671875, -2004.702392578125, -2069.630126953125, -2234.3017578125, -2110.447998046875, -2232.3017578125, -1894.702392578125, -1769.630126953125, -1882.3411865234375, -1892.3411865234375, -1923.1849365234375, -1.114013671875, -1624.3411865234375, -1983.263427734375, -2013.263427734375, -193.114013671875, -2088.447998046875, -1861.1849365234375, -1913.1849365234375, -1873.7001953125, -201.114013671875, -1876.3411865234375, -2085.7001953125], "mean_td_error": -1636.394775390625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 16000, "num_agent_steps_sampled": 16000, "num_steps_trained": 120032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 120032, "last_target_update_ts": 16000, "num_target_updates": 31}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13612569321402898, "mean_inference_ms": 1.5777755438735541, "mean_action_processing_ms": 0.05725996381138683, "mean_env_wait_ms": 1.1630903969795676, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 53, "training_iteration": 16, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-41-10", "timestamp": 1652704870, "time_this_iter_s": 12.968641757965088, "time_total_s": 269.39568161964417, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 269.39568161964417, "timesteps_since_restore": 512, "iterations_since_restore": 16, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.33157894736842, "ram_util_percent": 13.736842105263158}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 63595.892857142855, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2680155722492754, "mean_inference_ms": 2.1964557490339276, "mean_action_processing_ms": 0.07218370291300917, "mean_env_wait_ms": 6.6278497874525915, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 17000, "timesteps_this_iter": 32, "agent_timesteps_total": 17000, "timers": {"load_time_ms": 0.2, "load_throughput": 159763.99, "learn_time_ms": 30.389, "learn_throughput": 1053.023}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.055143531411886215, "mean_q": 1787.5709228515625, "min_q": 1371.38232421875, "max_q": 3576.087158203125, "cur_lr": 0.6}, "td_error": [-2050.5146484375, -2229.505859375, -2204.81884765625, -2361.321044921875, -2030.5888671875, -1797.795654296875, -2257.321044921875, -1786.5146484375, -2103.7744140625, -2219.505859375, -1979.505859375, -2139.795654296875, -118.6162109375, -2103.795654296875, -2241.505859375, -2151.7744140625, -2139.7744140625, 141.3837890625, -2204.81884765625, -2115.321044921875, -2045.795654296875, -134.6162109375, -2034.5888671875, -1996.5146484375, -2107.795654296875, -2030.5146484375, -1888.81884765625, -2197.505859375, -2187.7744140625, -1972.5146484375, -2054.81884765625, -1841.795654296875], "mean_td_error": -1893.382568359375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 17000, "num_agent_steps_sampled": 17000, "num_steps_trained": 128032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 128032, "last_target_update_ts": 17000, "num_target_updates": 33}, "done": false, "episodes_total": 56, "training_iteration": 17, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-41-24", "timestamp": 1652704884, "time_this_iter_s": 13.554200649261475, "time_total_s": 282.94988226890564, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 282.94988226890564, "timesteps_since_restore": 544, "iterations_since_restore": 17, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.226315789473684, "ram_util_percent": 13.9}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 61866.433333333334, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.266589709413579, "mean_inference_ms": 2.188956603270384, "mean_action_processing_ms": 0.07198064262645501, "mean_env_wait_ms": 6.471250990236476, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 18000, "timesteps_this_iter": 32, "agent_timesteps_total": 18000, "timers": {"load_time_ms": 0.126, "load_throughput": 253193.224, "learn_time_ms": 19.487, "learn_throughput": 1642.149}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.049641139805316925, "mean_q": 2195.18115234375, "min_q": 1482.689697265625, "max_q": 3828.33056640625, "cur_lr": 0.6}, "td_error": [-166.875732421875, -2126.017578125, -2038.0177001953125, 157.124267578125, -1967.6251220703125, -2330.3466796875, -46.875732421875, -2265.360595703125, -2229.360595703125, -2149.2216796875, -2306.96484375, -2306.3466796875, -180.875732421875, -2212.7998046875, -2282.96484375, -2337.2216796875, -2093.625, -2514.5166015625, -196.875732421875, -2097.625, -202.875732421875, -2094.017578125, -2346.5166015625, -2093.625, -2201.360595703125, -2235.360595703125, 17.124267578125, -2290.96484375, -2300.3466796875, -2359.2216796875, -2103.625, -2095.625], "mean_td_error": -1749.9627685546875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 18000, "num_agent_steps_sampled": 18000, "num_steps_trained": 136032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 136032, "last_target_update_ts": 18000, "num_target_updates": 35}, "done": false, "episodes_total": 60, "training_iteration": 18, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-41-35", "timestamp": 1652704895, "time_this_iter_s": 10.646666765213013, "time_total_s": 293.59654903411865, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 293.59654903411865, "timesteps_since_restore": 576, "iterations_since_restore": 18, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.866666666666667, "ram_util_percent": 14.013333333333332}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 60694.793650793654, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.265529675196751, "mean_inference_ms": 2.1831120869083778, "mean_action_processing_ms": 0.07181934203073267, "mean_env_wait_ms": 6.359778027941697, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 19000, "timesteps_this_iter": 32, "agent_timesteps_total": 19000, "timers": {"load_time_ms": 0.13, "load_throughput": 245460.366, "learn_time_ms": 19.466, "learn_throughput": 1643.876}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.05654257535934448, "mean_q": 2229.403076171875, "min_q": 1594.4046630859375, "max_q": 4066.739990234375, "cur_lr": 0.6}, "td_error": [-2324.4765625, -194.135986328125, -2442.99365234375, -2216.72509765625, -68.135986328125, -194.135986328125, -2598.47119140625, -2257.509765625, -2292.72509765625, -2480.47119140625, -2336.4765625, -2606.47119140625, -2438.99365234375, 101.8642578125, -2244.4765625, -158.135986328125, -2648.47119140625, -2064.724609375, -2501.3896484375, -2222.4765625, -2359.509765625, -2650.47119140625, -2614.47119140625, -2155.73291015625, -2253.509765625, -2386.4765625, -2612.47119140625, -192.135986328125, -2008.476318359375, -2324.4765625, -2268.4765625, -2505.3896484375], "mean_td_error": -1953.785400390625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 19000, "num_agent_steps_sampled": 19000, "num_steps_trained": 144032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 144032, "last_target_update_ts": 19000, "num_target_updates": 37}, "done": false, "episodes_total": 63, "training_iteration": 19, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-41-47", "timestamp": 1652704907, "time_this_iter_s": 12.099016189575195, "time_total_s": 305.69556522369385, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 305.69556522369385, "timesteps_since_restore": 608, "iterations_since_restore": 19, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.305555555555557, "ram_util_percent": 14.149999999999999}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 59705.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.26437311069746, "mean_inference_ms": 2.176157743103272, "mean_action_processing_ms": 0.07162013088783242, "mean_env_wait_ms": 6.2519404887883265, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 20000, "timesteps_this_iter": 32, "agent_timesteps_total": 20000, "timers": {"load_time_ms": 0.139, "load_throughput": 229903.611, "learn_time_ms": 21.301, "learn_throughput": 1502.302}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.041839227080345154, "mean_q": 2303.11572265625, "min_q": 1707.4244384765625, "max_q": 4309.4951171875, "cur_lr": 0.6}, "td_error": [-2485.527587890625, -2285.910400390625, -2614.19677734375, -200.4970703125, -2347.910400390625, -2469.527587890625, -2612.079345703125, -2780.56787109375, -2084.63623046875, -2794.56787109375, -2486.56787109375, -2574.079345703125, -92.4970703125, -2398.63623046875, -2167.69287109375, 77.5029296875, -2580.079345703125, -2644.19677734375, -2531.44091796875, -2219.527587890625, -2299.44091796875, -2582.079345703125, -2416.63623046875, -2774.56787109375, -2802.56787109375, -2342.63623046875, 4059.4951171875, -2586.079345703125, -2491.69287109375, -2256.63623046875, -2610.079345703125, 153.5029296875], "mean_td_error": -1976.314208984375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 20000, "num_agent_steps_sampled": 20000, "num_steps_trained": 152032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 152032, "last_target_update_ts": 20000, "num_target_updates": 39}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13761215413275915, "mean_inference_ms": 1.6052189229092226, "mean_action_processing_ms": 0.05879217906446158, "mean_env_wait_ms": 1.187266547389224, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 66, "training_iteration": 20, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-41-57", "timestamp": 1652704917, "time_this_iter_s": 10.62295413017273, "time_total_s": 316.3185193538666, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 316.3185193538666, "timesteps_since_restore": 640, "iterations_since_restore": 20, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.0, "ram_util_percent": 14.293333333333338}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 58455.54285714286, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2629642161278357, "mean_inference_ms": 2.1679009943093965, "mean_action_processing_ms": 0.07138852360167558, "mean_env_wait_ms": 6.116313704647107, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 21000, "timesteps_this_iter": 32, "agent_timesteps_total": 21000, "timers": {"load_time_ms": 0.191, "load_throughput": 167772.16, "learn_time_ms": 30.191, "learn_throughput": 1059.907}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.06731177121400833, "mean_q": 2698.16796875, "min_q": 1815.62451171875, "max_q": 4549.6142578125, "cur_lr": 0.6}, "td_error": [-2379.64306640625, -104.23486328125, -2800.36767578125, -2854.224609375, -226.23486328125, -2866.224609375, -2429.073486328125, -2671.262451171875, -2451.073486328125, -2729.262451171875, -2697.262451171875, -2599.285400390625, -2455.073486328125, -6.23486328125, -2439.073486328125, -184.23486328125, -2636.939697265625, -108.23486328125, -206.23486328125, -2928.224609375, -166.23486328125, -2445.073486328125, -2625.38427734375, -2692.939697265625, -2906.224609375, -2453.64306640625, -2215.073486328125, -2627.285400390625, -2726.36767578125, -26.23486328125, -2589.285400390625, -2457.64306640625], "mean_td_error": -1990.743408203125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 21000, "num_agent_steps_sampled": 21000, "num_steps_trained": 160032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 160032, "last_target_update_ts": 21000, "num_target_updates": 41}, "done": false, "episodes_total": 70, "training_iteration": 21, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-42-12", "timestamp": 1652704932, "time_this_iter_s": 14.535466432571411, "time_total_s": 330.853985786438, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 330.853985786438, "timesteps_since_restore": 672, "iterations_since_restore": 21, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.395238095238096, "ram_util_percent": 14.433333333333334}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 57627.643835616436, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.26201966144931166, "mean_inference_ms": 2.1625756569493504, "mean_action_processing_ms": 0.07123870430158333, "mean_env_wait_ms": 6.020111118147503, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 22000, "timesteps_this_iter": 32, "agent_timesteps_total": 22000, "timers": {"load_time_ms": 0.254, "load_throughput": 126215.655, "learn_time_ms": 38.564, "learn_throughput": 829.788}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.049921512603759766, "mean_q": 2784.92041015625, "min_q": 1938.1490478515625, "max_q": 4776.630859375, "cur_lr": 0.6}, "td_error": [-2654.390625, -2806.186767578125, -2570.31298828125, -2566.31298828125, -6.1220703125, -2663.35546875, -2691.35546875, -2477.1044921875, -2723.35546875, -2782.186767578125, -2779.1044921875, -2387.35546875, -2586.31298828125, 91.8779296875, -2773.1044921875, -2695.35546875, -2743.1044921875, 135.8779296875, -90.1220703125, -2791.1044921875, -2621.1044921875, -2398.31298828125, -2605.35546875, 47.8779296875, -2685.31591796875, -206.1220703125, -2775.35546875, -2719.31591796875, -2651.1044921875, -86.1220703125, -2766.60400390625, -2697.31591796875], "mean_td_error": -2085.08251953125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 22000, "num_agent_steps_sampled": 22000, "num_steps_trained": 168032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 168032, "last_target_update_ts": 22000, "num_target_updates": 43}, "done": false, "episodes_total": 73, "training_iteration": 22, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-42-27", "timestamp": 1652704947, "time_this_iter_s": 14.717588663101196, "time_total_s": 345.5715744495392, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 345.5715744495392, "timesteps_since_restore": 704, "iterations_since_restore": 22, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.695238095238096, "ram_util_percent": 14.571428571428575}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 56712.71052631579, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2610587577116699, "mean_inference_ms": 2.157036389715452, "mean_action_processing_ms": 0.07107940569214131, "mean_env_wait_ms": 5.927204816446305, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 23000, "timesteps_this_iter": 32, "agent_timesteps_total": 23000, "timers": {"load_time_ms": 0.14, "load_throughput": 228377.962, "learn_time_ms": 20.484, "learn_throughput": 1562.177}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.040600262582302094, "mean_q": 2812.09814453125, "min_q": 2041.4586181640625, "max_q": 5006.25341796875, "cur_lr": 0.6}, "td_error": [-2641.053955078125, -2682.61572265625, -2856.111328125, -2842.92333984375, -2647.053955078125, -2834.61572265625, -2890.92333984375, -3102.63037109375, -2830.970703125, -2736.61572265625, -2912.111328125, -11.83544921875, -2734.9638671875, -2858.970703125, -3014.970703125, -2834.92333984375, -2872.111328125, -2882.111328125, -2806.61572265625, -139.83544921875, -2844.111328125, -2492.9638671875, -43.83544921875, 164.16455078125, -2756.61572265625, 156.16455078125, -2994.970703125, -2666.9638671875, -2986.92333984375, -2900.92333984375, -175.83544921875, -2894.92333984375], "mean_td_error": -2299.11572265625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 23000, "num_agent_steps_sampled": 23000, "num_steps_trained": 176032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 176032, "last_target_update_ts": 23000, "num_target_updates": 45}, "done": false, "episodes_total": 76, "training_iteration": 23, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-42-38", "timestamp": 1652704958, "time_this_iter_s": 11.40244460105896, "time_total_s": 356.97401905059814, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 356.97401905059814, "timesteps_since_restore": 736, "iterations_since_restore": 23, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.262500000000003, "ram_util_percent": 14.73125}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 56027.65, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2598101535430737, "mean_inference_ms": 2.1496588213454646, "mean_action_processing_ms": 0.07086484268772789, "mean_env_wait_ms": 5.809821945533491, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 24000, "timesteps_this_iter": 32, "agent_timesteps_total": 24000, "timers": {"load_time_ms": 0.173, "load_throughput": 185000.314, "learn_time_ms": 27.553, "learn_throughput": 1161.408}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.044425737112760544, "mean_q": 3047.101318359375, "min_q": 2154.02294921875, "max_q": 5232.083984375, "cur_lr": 0.6}, "td_error": [-2925.056640625, -3260.087890625, -2684.5205078125, -2886.028076171875, -2780.5205078125, -3106.60986328125, -2903.056640625, -170.02685546875, -2910.028076171875, 67.97314453125, -2869.349609375, -2773.335205078125, -2923.056640625, -2770.12744140625, -186.02685546875, -3102.60986328125, -2556.5205078125, -3017.349609375, -172.02685546875, -3210.087890625, -2715.349609375, -3015.349609375, 163.97314453125, -2978.12744140625, -222.02685546875, -3240.087890625, -3122.60986328125, -168.02685546875, -2885.056640625, -2782.5205078125, -2731.335205078125, -2745.335205078125], "mean_td_error": -2299.384765625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 24000, "num_agent_steps_sampled": 24000, "num_steps_trained": 184032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 184032, "last_target_update_ts": 24000, "num_target_updates": 47}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13420146283409715, "mean_inference_ms": 1.5547362914818779, "mean_action_processing_ms": 0.05728148672727661, "mean_env_wait_ms": 1.162689570120346, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 80, "training_iteration": 24, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-42-51", "timestamp": 1652704971, "time_this_iter_s": 12.808168411254883, "time_total_s": 369.782187461853, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 369.782187461853, "timesteps_since_restore": 768, "iterations_since_restore": 24, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 21.333333333333332, "ram_util_percent": 14.855555555555558}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 55425.20481927711, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2589337918166355, "mean_inference_ms": 2.1445683319803406, "mean_action_processing_ms": 0.07071570023266509, "mean_env_wait_ms": 5.726299428464384, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 25000, "timesteps_this_iter": 32, "agent_timesteps_total": 25000, "timers": {"load_time_ms": 0.131, "load_throughput": 243456.789, "learn_time_ms": 20.434, "learn_throughput": 1566.05}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.06295491009950638, "mean_q": 3264.22998046875, "min_q": 2275.1689453125, "max_q": 5449.3017578125, "cur_lr": 0.6}, "td_error": [-3329.89990234375, -3343.89990234375, -2800.8515625, -3040.796142578125, -191.76708984375, -155.76708984375, -201.76708984375, -157.76708984375, -3011.93212890625, -2834.8515625, -3067.563720703125, -3339.89990234375, -117.76708984375, -2831.8203125, -3116.796142578125, -3072.796142578125, -2798.8515625, -191.76708984375, 5083.3017578125, -3163.89990234375, -3084.796142578125, -3066.796142578125, -2895.8203125, -3035.287109375, -2923.287109375, -3064.796142578125, 1909.1689453125, -3135.563720703125, -2832.8515625, -53.76708984375, -3217.93212890625, -2871.8203125], "mean_td_error": -1998.772216796875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 25000, "num_agent_steps_sampled": 25000, "num_steps_trained": 192032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 192032, "last_target_update_ts": 25000, "num_target_updates": 49}, "done": false, "episodes_total": 83, "training_iteration": 25, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-43-05", "timestamp": 1652704985, "time_this_iter_s": 13.669355154037476, "time_total_s": 383.4515426158905, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 383.4515426158905, "timesteps_since_restore": 800, "iterations_since_restore": 25, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.345, "ram_util_percent": 15.005}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 54814.976744186046, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2581058066888921, "mean_inference_ms": 2.1398296446625884, "mean_action_processing_ms": 0.0705768821883574, "mean_env_wait_ms": 5.646025678715971, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 26000, "timesteps_this_iter": 32, "agent_timesteps_total": 26000, "timers": {"load_time_ms": 0.205, "load_throughput": 156321.603, "learn_time_ms": 31.59, "learn_throughput": 1012.965}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03909066691994667, "mean_q": 3191.95068359375, "min_q": 2523.298583984375, "max_q": 5666.1181640625, "cur_lr": 0.6}, "td_error": [-186.7041015625, -3106.86474609375, -3158.38134765625, -3311.523681640625, -2866.20166015625, -3162.86474609375, -3156.5791015625, -3098.5791015625, -3301.523681640625, -3080.20166015625, -2996.20166015625, -3004.20166015625, -3048.20166015625, 67.2958984375, -3152.86474609375, -3214.86474609375, -3080.20166015625, -2987.523681640625, -3331.523681640625, 149.2958984375, -3010.20166015625, -3052.20166015625, -160.7041015625, -3166.5791015625, -2645.42578125, -2938.20166015625, -3032.20166015625, -2917.42578125, -2867.42578125, -2966.20166015625, 157.2958984375, -2964.20166015625], "mean_td_error": -2580.99658203125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 26000, "num_agent_steps_sampled": 26000, "num_steps_trained": 200032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 200032, "last_target_update_ts": 26000, "num_target_updates": 51}, "done": false, "episodes_total": 86, "training_iteration": 26, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-43-19", "timestamp": 1652704999, "time_this_iter_s": 13.661027908325195, "time_total_s": 397.1125705242157, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 397.1125705242157, "timesteps_since_restore": 832, "iterations_since_restore": 26, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.185000000000002, "ram_util_percent": 15.139999999999997}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 53990.75555555556, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.25710733824384374, "mean_inference_ms": 2.1342764120491604, "mean_action_processing_ms": 0.07041278709261803, "mean_env_wait_ms": 5.544464111989753, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 27000, "timesteps_this_iter": 32, "agent_timesteps_total": 27000, "timers": {"load_time_ms": 0.141, "load_throughput": 226719.135, "learn_time_ms": 22.118, "learn_throughput": 1446.797}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03017675317823887, "mean_q": 3447.755126953125, "min_q": 2501.8603515625, "max_q": 5879.28466796875, "cur_lr": 0.6}, "td_error": [-3209.81982421875, -3036.920166015625, -3113.888671875, 15.80712890625, -3355.888671875, -3403.888671875, -3165.888671875, -3523.6171875, -80.19287109375, -3305.888671875, -3233.81982421875, -60.19287109375, -3219.81982421875, -3337.888671875, -3513.6171875, -3248.811767578125, -3157.89111328125, -3260.811767578125, 161.80712890625, -3129.89111328125, -2756.552734375, -3273.321044921875, 127.80712890625, -3197.81982421875, 95.80712890625, -3173.89111328125, -3387.888671875, -3089.81982421875, 9.80712890625, -3271.321044921875, -3056.920166015625, -3187.89111328125], "mean_td_error": -2510.722900390625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 27000, "num_agent_steps_sampled": 27000, "num_steps_trained": 208032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 208032, "last_target_update_ts": 27000, "num_target_updates": 53}, "done": false, "episodes_total": 90, "training_iteration": 27, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-43-33", "timestamp": 1652705013, "time_this_iter_s": 14.490608930587769, "time_total_s": 411.60317945480347, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 411.60317945480347, "timesteps_since_restore": 864, "iterations_since_restore": 27, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.424999999999994, "ram_util_percent": 15.265}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 53344.51612903226, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2564219175955745, "mean_inference_ms": 2.130563856361209, "mean_action_processing_ms": 0.07030458405309468, "mean_env_wait_ms": 5.471955390019173, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 28000, "timesteps_this_iter": 32, "agent_timesteps_total": 28000, "timers": {"load_time_ms": 0.225, "load_throughput": 142285.305, "learn_time_ms": 31.626, "learn_throughput": 1011.815}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.07015350461006165, "mean_q": 3833.716796875, "min_q": 2628.03515625, "max_q": 6088.37890625, "cur_lr": 0.6}, "td_error": [-191.9794921875, -3317.64111328125, -3338.88623046875, -89.9794921875, -3103.865478515625, -3197.865478515625, -3228.40087890625, -79.9794921875, 102.0205078125, -3346.88623046875, -3106.068359375, -3086.068359375, -3086.068359375, -3327.64111328125, -3254.782958984375, 5738.37890625, -3246.782958984375, -3335.64111328125, -49.9794921875, -3330.88623046875, -181.9794921875, -3254.782958984375, -3256.27978515625, -3276.782958984375, -3246.782958984375, -77.9794921875, -3594.3232421875, -3336.88623046875, -3640.3232421875, -3262.782958984375, -3306.27978515625, -141.9794921875], "mean_td_error": -2201.7548828125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 28000, "num_agent_steps_sampled": 28000, "num_steps_trained": 216032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 216032, "last_target_update_ts": 28000, "num_target_updates": 55}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12697939756993507, "mean_inference_ms": 1.4778230940370771, "mean_action_processing_ms": 0.054430019736346716, "mean_env_wait_ms": 1.1176493325612478, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 93, "training_iteration": 28, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-43-48", "timestamp": 1652705028, "time_this_iter_s": 14.912348985671997, "time_total_s": 426.51552844047546, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 426.51552844047546, "timesteps_since_restore": 896, "iterations_since_restore": 28, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.12272727272727, "ram_util_percent": 15.418181818181818}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 52764.291666666664, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2557291306789224, "mean_inference_ms": 2.126683566734268, "mean_action_processing_ms": 0.07018986010913317, "mean_env_wait_ms": 5.4017980307078295, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 29000, "timesteps_this_iter": 32, "agent_timesteps_total": 29000, "timers": {"load_time_ms": 0.231, "load_throughput": 138812.419, "learn_time_ms": 35.431, "learn_throughput": 903.172}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.038973718881607056, "mean_q": 3782.636962890625, "min_q": 2751.0546875, "max_q": 6287.283203125, "cur_lr": 0.6}, "td_error": [-3214.593994140625, -3135.278076171875, -3542.8017578125, -76.90283203125, -3337.30517578125, 11.09716796875, -3212.593994140625, -3139.278076171875, -3382.97265625, -3418.016845703125, -3372.016845703125, -3236.593994140625, -3218.593994140625, -174.90283203125, -3431.13134765625, -3402.016845703125, -3560.8017578125, -3109.92724609375, -3709.13134765625, -3387.30517578125, -3400.97265625, -3602.8017578125, -86.90283203125, -3404.016845703125, -144.90283203125, -3178.016845703125, -3413.92724609375, -2865.278076171875, -94.90283203125, -3226.593994140625, 7.09716796875, -3125.278076171875], "mean_td_error": -2612.111328125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 29000, "num_agent_steps_sampled": 29000, "num_steps_trained": 224032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 224032, "last_target_update_ts": 29000, "num_target_updates": 57}, "done": false, "episodes_total": 96, "training_iteration": 29, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-44-00", "timestamp": 1652705040, "time_this_iter_s": 11.360323429107666, "time_total_s": 437.87585186958313, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 437.87585186958313, "timesteps_since_restore": 928, "iterations_since_restore": 29, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 21.7875, "ram_util_percent": 15.55625}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 52339.28, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [76740.0, 78862.0, 81202.0, 81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2548924469781178, "mean_inference_ms": 2.1221948423432337, "mean_action_processing_ms": 0.07005760078484607, "mean_env_wait_ms": 5.313184971138777, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 30000, "timesteps_this_iter": 32, "agent_timesteps_total": 30000, "timers": {"load_time_ms": 0.226, "load_throughput": 141759.324, "learn_time_ms": 32.253, "learn_throughput": 992.168}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.06920047849416733, "mean_q": 3894.13525390625, "min_q": 2869.765625, "max_q": 6488.845703125, "cur_lr": 0.6}, "td_error": [-3469.896484375, -194.9921875, -3388.70068359375, -3501.896484375, -3314.792236328125, -3447.896484375, -3229.38720703125, -3227.38720703125, 103.0078125, -3218.70068359375, -182.9921875, -3436.994140625, -42.9921875, -3448.576416015625, -40.9921875, -3644.072265625, -3698.64697265625, -3248.70068359375, -234.9921875, -3404.70068359375, -3848.072265625, -3388.70068359375, -3370.994140625, -44.9921875, -3330.64697265625, -3586.64697265625, -3392.70068359375, -3798.072265625, -2966.792236328125, -3270.576416015625, -3464.994140625, -3276.994140625], "mean_td_error": -2687.95263671875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 30000, "num_agent_steps_sampled": 30000, "num_steps_trained": 232032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 232032, "last_target_update_ts": 30000, "num_target_updates": 59}, "done": false, "episodes_total": 100, "training_iteration": 30, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-44-15", "timestamp": 1652705055, "time_this_iter_s": 15.527240991592407, "time_total_s": 453.40309286117554, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 453.40309286117554, "timesteps_since_restore": 960, "iterations_since_restore": 30, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.277272727272727, "ram_util_percent": 15.686363636363636}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 51185.76, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [81702.0, 85116.0, 79934.0, 85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2531723182292186, "mean_inference_ms": 2.1160452495614326, "mean_action_processing_ms": 0.06980930844088885, "mean_env_wait_ms": 5.170761945928905, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 31000, "timesteps_this_iter": 32, "agent_timesteps_total": 31000, "timers": {"load_time_ms": 0.129, "load_throughput": 248920.119, "learn_time_ms": 19.466, "learn_throughput": 1643.933}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.053250689059495926, "mean_q": 3847.65673828125, "min_q": 2997.22900390625, "max_q": 6682.94921875, "cur_lr": 0.6}, "td_error": [-3560.91064453125, -3557.955322265625, -3538.77392578125, -3336.419921875, -3848.76318359375, -3115.83935546875, -3522.91064453125, -3774.76318359375, -3453.955322265625, -3176.419921875, -3465.955322265625, -185.04296875, -3210.77392578125, -3522.77392578125, -3501.955322265625, -3772.84423828125, -3287.83935546875, 102.95703125, -183.04296875, -3770.76318359375, -3497.955322265625, -3408.419921875, -3495.955322265625, -3276.419921875, -3546.91064453125, 6408.94921875, -3157.83935546875, -179.04296875, -3526.91064453125, -3463.955322265625, -3712.76318359375, -3500.77392578125], "mean_td_error": -2751.33544921875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 31000, "num_agent_steps_sampled": 31000, "num_steps_trained": 240032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 240032, "last_target_update_ts": 31000, "num_target_updates": 61}, "done": false, "episodes_total": 103, "training_iteration": 31, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-44-27", "timestamp": 1652705067, "time_this_iter_s": 12.048307418823242, "time_total_s": 465.4514002799988, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 465.4514002799988, "timesteps_since_restore": 992, "iterations_since_restore": 31, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.13529411764706, "ram_util_percent": 15.823529411764707}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 49904.14, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [85136.0, 88956.0, 83924.0, 85334.0, 86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2514222598395959, "mean_inference_ms": 2.1074721721743406, "mean_action_processing_ms": 0.0695349010690133, "mean_env_wait_ms": 5.015105491556624, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 32000, "timesteps_this_iter": 32, "agent_timesteps_total": 32000, "timers": {"load_time_ms": 0.234, "load_throughput": 136622.28, "learn_time_ms": 35.331, "learn_throughput": 905.714}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.056255243718624115, "mean_q": 4166.44091796875, "min_q": 3122.959228515625, "max_q": 6886.30712890625, "cur_lr": 0.6}, "td_error": [-3648.634765625, -10.966796875, -3584.805419921875, -3428.48974609375, -3587.60595703125, -3658.634765625, 33.033203125, -80.966796875, -3618.634765625, -3560.923828125, -3428.48974609375, -124.966796875, -14.966796875, -3806.923828125, -3752.314697265625, -3628.805419921875, -3920.314697265625, -3557.60595703125, -3424.48974609375, -3204.357421875, -3334.357421875, -3356.634765625, -3774.923828125, -3940.314697265625, -3302.634765625, -3824.923828125, -3382.357421875, -3562.805419921875, -3613.60595703125, -26.966796875, -3776.314697265625, -40.966796875], "mean_td_error": -2810.833251953125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 32000, "num_agent_steps_sampled": 32000, "num_steps_trained": 248032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 248032, "last_target_update_ts": 32000, "num_target_updates": 63}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12880491743679595, "mean_inference_ms": 1.50611022272392, "mean_action_processing_ms": 0.05555857920934637, "mean_env_wait_ms": 1.1351816557884613, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 106, "training_iteration": 32, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-44-40", "timestamp": 1652705080, "time_this_iter_s": 12.637065172195435, "time_total_s": 478.0884654521942, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 478.0884654521942, "timesteps_since_restore": 1024, "iterations_since_restore": 32, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.41111111111111, "ram_util_percent": 15.972222222222221}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 47990.52, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [86274.0, 86684.0, 89568.0, 84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.24926029634224414, "mean_inference_ms": 2.0958190183691925, "mean_action_processing_ms": 0.06919556819014025, "mean_env_wait_ms": 4.800393103265059, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 33000, "timesteps_this_iter": 32, "agent_timesteps_total": 33000, "timers": {"load_time_ms": 0.227, "load_throughput": 141088.75, "learn_time_ms": 33.735, "learn_throughput": 948.558}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.024264581501483917, "mean_q": 4976.37841796875, "min_q": 3245.45263671875, "max_q": 7092.31396484375, "cur_lr": 0.6}, "td_error": [161.5732421875, -124.4267578125, -3287.966796875, -3981.2880859375, -3639.306884765625, -184.4267578125, -3741.306884765625, -3905.306884765625, -3671.6328125, 139.5732421875, -3620.5927734375, -3689.6328125, -3711.2451171875, -3505.966796875, -3558.5927734375, 115.5732421875, -3723.6328125, -8.4267578125, -64.4267578125, -74.4267578125, -148.4267578125, 113.5732421875, -200.4267578125, -3683.6328125, -3095.72509765625, -3881.2880859375, -3466.5927734375, -3393.72509765625, -3627.306884765625, -3867.306884765625, -162.4267578125, 97.5732421875], "mean_td_error": -2168.487548828125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 33000, "num_agent_steps_sampled": 33000, "num_steps_trained": 256032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 256032, "last_target_update_ts": 33000, "num_target_updates": 65}, "done": false, "episodes_total": 110, "training_iteration": 33, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-44-50", "timestamp": 1652705090, "time_this_iter_s": 10.02100396156311, "time_total_s": 488.1094694137573, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 488.1094694137573, "timesteps_since_restore": 1056, "iterations_since_restore": 33, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.686666666666667, "ram_util_percent": 16.12666666666666}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 46578.28, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [84416.0, 88170.0, 82396.0, 85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.24760797864337306, "mean_inference_ms": 2.0863608783029606, "mean_action_processing_ms": 0.06892783652059056, "mean_env_wait_ms": 4.640625356731364, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 34000, "timesteps_this_iter": 32, "agent_timesteps_total": 34000, "timers": {"load_time_ms": 0.227, "load_throughput": 140822.294, "learn_time_ms": 34.853, "learn_throughput": 918.13}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03896409645676613, "mean_q": 4675.3974609375, "min_q": 3365.826416015625, "max_q": 7295.5322265625, "cur_lr": 0.6}, "td_error": [-3572.90087890625, -4079.464599609375, -3779.97265625, -31.7587890625, -3727.97265625, -3718.4189453125, -175.7587890625, -179.7587890625, -3810.04150390625, -3789.06591796875, -3779.06591796875, -3607.06591796875, -3425.560546875, -3660.4189453125, -3731.97265625, -3479.560546875, -165.7587890625, 178.2412109375, -3913.464599609375, -3942.04150390625, -21.7587890625, -3576.90087890625, -3540.90087890625, 178.2412109375, -3542.90087890625, -3751.464599609375, -3715.97265625, 6927.5322265625, -3799.560546875, 178.2412109375, -3640.04150390625, -3972.04150390625], "mean_td_error": -2458.415771484375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 34000, "num_agent_steps_sampled": 34000, "num_steps_trained": 264032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 264032, "last_target_update_ts": 34000, "num_target_updates": 67}, "done": false, "episodes_total": 113, "training_iteration": 34, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-45-01", "timestamp": 1652705101, "time_this_iter_s": 10.48663067817688, "time_total_s": 498.5961000919342, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 498.5961000919342, "timesteps_since_restore": 1088, "iterations_since_restore": 34, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.219999999999995, "ram_util_percent": 16.26666666666667}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 45183.74, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [85042.0, 87486.0, 86098.0, 85822.0, 91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.24607440272196995, "mean_inference_ms": 2.0771403321104303, "mean_action_processing_ms": 0.06867053811187653, "mean_env_wait_ms": 4.485934767658101, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 35000, "timesteps_this_iter": 32, "agent_timesteps_total": 35000, "timers": {"load_time_ms": 0.129, "load_throughput": 248966.292, "learn_time_ms": 19.678, "learn_throughput": 1626.178}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.02679166942834854, "mean_q": 4960.2509765625, "min_q": 3481.86328125, "max_q": 7481.16796875, "cur_lr": 0.6}, "td_error": [-171.2607421875, -3797.90869140625, -3489.5693359375, -4158.5654296875, -3533.295654296875, 104.7392578125, -3761.87646484375, -195.2607421875, 154.7392578125, -4031.0771484375, 80.7392578125, -3623.553466796875, -3755.87646484375, -3868.5654296875, -4047.0771484375, -163.2607421875, -3805.90869140625, -3597.295654296875, -3801.847900390625, -51.2607421875, -4140.5654296875, -3861.553466796875, -3515.5693359375, -3775.847900390625, -185.2607421875, -3523.5693359375, -143.2607421875, -4100.5654296875, -3535.5693359375, 110.7392578125, -3845.553466796875, -3759.87646484375], "mean_td_error": -2618.427978515625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 35000, "num_agent_steps_sampled": 35000, "num_steps_trained": 272032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 272032, "last_target_update_ts": 35000, "num_target_updates": 69}, "done": false, "episodes_total": 116, "training_iteration": 35, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-45-14", "timestamp": 1652705114, "time_this_iter_s": 13.532306671142578, "time_total_s": 512.1284067630768, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 512.1284067630768, "timesteps_since_restore": 1120, "iterations_since_restore": 35, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.478947368421053, "ram_util_percent": 16.394736842105264}}
{"episode_reward_max": 91740.0, "episode_reward_min": 29448.0, "episode_reward_mean": 43175.92, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [91740.0, 89140.0, 81504.0, 84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.24428979586630317, "mean_inference_ms": 2.066791752402704, "mean_action_processing_ms": 0.06838229669817482, "mean_env_wait_ms": 4.291352005062773, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 36000, "timesteps_this_iter": 32, "agent_timesteps_total": 36000, "timers": {"load_time_ms": 0.246, "load_throughput": 130081.148, "learn_time_ms": 36.763, "learn_throughput": 870.446}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.058062177151441574, "mean_q": 4551.1123046875, "min_q": 3601.372802734375, "max_q": 7656.44140625, "cur_lr": 0.6}, "td_error": [-4195.9921875, -4127.9921875, -3905.8359375, -3561.77099609375, -4193.9921875, 7.07666015625, -3949.991943359375, -3897.8359375, -4096.3583984375, 7408.44140625, -4058.3583984375, -3634.0009765625, -3870.3583984375, -4165.9921875, -4209.9921875, -3814.0009765625, -160.92333984375, 7378.44140625, -3505.47314453125, -3811.586181640625, -3781.586181640625, -4179.9921875, -4122.3583984375, -186.92333984375, -3519.47314453125, -4143.9921875, -3529.77099609375, -82.92333984375, -3640.0009765625, -3519.47314453125, -4017.991943359375, -4010.3583984375], "mean_td_error": -2721.9169921875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 36000, "num_agent_steps_sampled": 36000, "num_steps_trained": 280032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 280032, "last_target_update_ts": 36000, "num_target_updates": 71}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13152540546574004, "mean_inference_ms": 1.545619540901283, "mean_action_processing_ms": 0.056830833065028716, "mean_env_wait_ms": 1.1544679544979182, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 120, "training_iteration": 36, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-45-27", "timestamp": 1652705127, "time_this_iter_s": 12.784148454666138, "time_total_s": 524.9125552177429, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 524.9125552177429, "timesteps_since_restore": 1152, "iterations_since_restore": 36, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.53888888888889, "ram_util_percent": 16.550000000000004}}
{"episode_reward_max": 84054.0, "episode_reward_min": 29448.0, "episode_reward_mean": 41772.28, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [84054.0, 81640.0, 82024.0, 79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.24303679110253498, "mean_inference_ms": 2.0594095274895583, "mean_action_processing_ms": 0.06817921517090386, "mean_env_wait_ms": 4.152547402736672, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 37000, "timesteps_this_iter": 32, "agent_timesteps_total": 37000, "timers": {"load_time_ms": 0.13, "load_throughput": 246316.256, "learn_time_ms": 19.649, "learn_throughput": 1628.579}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03200848773121834, "mean_q": 4918.31298828125, "min_q": 3725.76171875, "max_q": 7837.17138671875, "cur_lr": 0.6}, "td_error": [-3532.28125, -3801.142578125, -3690.28125, -3574.75537109375, -3860.85888671875, -4308.6865234375, -63.27685546875, -3695.32470703125, -3905.32470703125, 20.72314453125, -3612.75537109375, -3827.142578125, -19.27685546875, -3805.32470703125, -4276.6865234375, -3933.32470703125, -3951.32470703125, 16.72314453125, -3846.85888671875, 3990.1669921875, -3951.32470703125, -3691.32470703125, -3986.6923828125, 7569.17138671875, -3856.85888671875, -73.27685546875, -4225.84912109375, -4298.6865234375, -3900.85888671875, -3864.6923828125, -149.27685546875, -3588.75537109375], "mean_td_error": -2552.982421875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 37000, "num_agent_steps_sampled": 37000, "num_steps_trained": 288032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 288032, "last_target_update_ts": 37000, "num_target_updates": 73}, "done": false, "episodes_total": 123, "training_iteration": 37, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-45-41", "timestamp": 1652705141, "time_this_iter_s": 13.494469404220581, "time_total_s": 538.4070246219635, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 538.4070246219635, "timesteps_since_restore": 1184, "iterations_since_restore": 37, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 24.075, "ram_util_percent": 16.699999999999996}}
{"episode_reward_max": 79464.0, "episode_reward_min": 29448.0, "episode_reward_mean": 40462.28, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [79464.0, 78612.0, 77542.0, 67848.0, 61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.24190432760082342, "mean_inference_ms": 2.0528398212269714, "mean_action_processing_ms": 0.06800218765993742, "mean_env_wait_ms": 4.021883377919515, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 38000, "timesteps_this_iter": 32, "agent_timesteps_total": 38000, "timers": {"load_time_ms": 0.262, "load_throughput": 122349.798, "learn_time_ms": 35.579, "learn_throughput": 899.409}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03210969269275665, "mean_q": 5028.6806640625, "min_q": 3861.3876953125, "max_q": 8007.7109375, "cur_lr": 0.6}, "td_error": [155.4990234375, 157.4990234375, -3887.2900390625, -3684.0966796875, -4209.115234375, -3467.55615234375, -3867.2900390625, -3678.0966796875, -3650.548828125, -4181.115234375, -88.5009765625, 155.4990234375, -92.5009765625, -3943.2900390625, 17.4990234375, -4241.115234375, -3955.06640625, -4042.82421875, -3886.548828125, -4017.19921875, -3503.55615234375, -3937.2900390625, -4116.82421875, -3929.2900390625, -3629.55615234375, -4304.82421875, -3969.19921875, -4065.114990234375, -3936.548828125, -3961.06640625, 157.4990234375, -3953.06640625], "mean_td_error": -3048.59375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 38000, "num_agent_steps_sampled": 38000, "num_steps_trained": 296032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 296032, "last_target_update_ts": 38000, "num_target_updates": 75}, "done": false, "episodes_total": 126, "training_iteration": 38, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-45-54", "timestamp": 1652705154, "time_this_iter_s": 13.744035720825195, "time_total_s": 552.1510603427887, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 552.1510603427887, "timesteps_since_restore": 1216, "iterations_since_restore": 38, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.152631578947364, "ram_util_percent": 16.831578947368417}}
{"episode_reward_max": 61838.0, "episode_reward_min": 29448.0, "episode_reward_mean": 38955.84, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [61838.0, 52250.0, 47806.0, 42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.24034873110475335, "mean_inference_ms": 2.043176888042242, "mean_action_processing_ms": 0.06772879278059768, "mean_env_wait_ms": 3.858312569701178, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 39000, "timesteps_this_iter": 32, "agent_timesteps_total": 39000, "timers": {"load_time_ms": 0.133, "load_throughput": 240662.951, "learn_time_ms": 20.015, "learn_throughput": 1598.831}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.025133874267339706, "mean_q": 5446.04443359375, "min_q": 3991.411865234375, "max_q": 8197.7109375, "cur_lr": 0.6}, "td_error": [-4287.8056640625, -3793.5556640625, -4248.794921875, 177.50341796875, -4047.55712890625, -3983.88525390625, -3707.12451171875, -3623.12451171875, -162.49609375, 111.50341796875, -3931.271484375, 31.50341796875, -3805.5556640625, -48.49609375, -3961.271484375, -66.49609375, -86.49609375, 135.50341796875, -4035.55712890625, -146.49609375, -3805.5556640625, -3765.5556640625, -4366.794921875, -3679.12451171875, -4093.55712890625, -4334.794921875, -4087.55712890625, -3965.271484375, -3661.12451171875, -4344.794921875, -3613.55615234375, 3928.64990234375], "mean_td_error": -2602.156494140625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 39000, "num_agent_steps_sampled": 39000, "num_steps_trained": 304032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 304032, "last_target_update_ts": 39000, "num_target_updates": 77}, "done": false, "episodes_total": 130, "training_iteration": 39, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-46-05", "timestamp": 1652705165, "time_this_iter_s": 11.138587236404419, "time_total_s": 563.2896475791931, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 563.2896475791931, "timesteps_since_restore": 1248, "iterations_since_restore": 39, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.375, "ram_util_percent": 16.95}}
{"episode_reward_max": 51184.0, "episode_reward_min": 29448.0, "episode_reward_mean": 38384.26, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [42218.0, 37544.0, 37012.0, 38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.23924319045982417, "mean_inference_ms": 2.035994048568531, "mean_action_processing_ms": 0.0675231074038657, "mean_env_wait_ms": 3.7480831717373477, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 40000, "timesteps_this_iter": 32, "agent_timesteps_total": 40000, "timers": {"load_time_ms": 0.223, "load_throughput": 143702.064, "learn_time_ms": 34.348, "learn_throughput": 931.639}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.025218239054083824, "mean_q": 5786.7119140625, "min_q": 4133.94921875, "max_q": 8381.5634765625, "cur_lr": 0.6}, "td_error": [-3652.169921875, -3722.169921875, 175.1767578125, -4382.4375, -3966.3935546875, -4025.80126953125, 171.1767578125, -3816.3935546875, -3865.912109375, -3884.3935546875, -3921.80126953125, 97.1767578125, -4144.72021484375, -86.8232421875, 85.1767578125, -4049.80126953125, -4193.060546875, -32.8232421875, 175.1767578125, -3729.80126953125, -4313.060546875, -138.8232421875, -156.8232421875, -4361.060546875, -4422.4375, 73.1767578125, -4418.4375, -4363.060546875, 111.1767578125, -4115.060546875, -4004.3935546875, -3522.169921875], "mean_td_error": -2637.5498046875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 40000, "num_agent_steps_sampled": 40000, "num_steps_trained": 312032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 312032, "last_target_update_ts": 40000, "num_target_updates": 79}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.1331597556673817, "mean_inference_ms": 1.570992770094905, "mean_action_processing_ms": 0.05789424371258571, "mean_env_wait_ms": 1.1726940762635192, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 133, "training_iteration": 40, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-46-21", "timestamp": 1652705181, "time_this_iter_s": 15.359093189239502, "time_total_s": 578.6487407684326, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 578.6487407684326, "timesteps_since_restore": 1280, "iterations_since_restore": 40, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.081818181818182, "ram_util_percent": 17.104545454545455}}
{"episode_reward_max": 51184.0, "episode_reward_min": 29448.0, "episode_reward_mean": 38307.9, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [38818.0, 37172.0, 39272.0, 34198.0, 46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2381664897447899, "mean_inference_ms": 2.028888937559629, "mean_action_processing_ms": 0.06731744182869642, "mean_env_wait_ms": 3.6497628600142265, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 41000, "timesteps_this_iter": 32, "agent_timesteps_total": 41000, "timers": {"load_time_ms": 0.241, "load_throughput": 132586.909, "learn_time_ms": 38.643, "learn_throughput": 828.101}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.032463978976011276, "mean_q": 5863.875, "min_q": 4268.19677734375, "max_q": 8552.287109375, "cur_lr": 0.6}, "td_error": [-4434.73095703125, -4086.515625, -80.640625, -3755.0263671875, -4002.515625, -3934.35205078125, 169.359375, -3711.0263671875, 167.359375, -3794.35205078125, -4064.35205078125, -4006.515625, -88.640625, 29.359375, -4094.51220703125, -3840.51220703125, -3948.51220703125, -4138.35205078125, -4122.515625, -4377.4296875, 169.359375, -3890.86083984375, -172.640625, -4110.2900390625, -3818.51220703125, 169.359375, -3892.51220703125, -3970.51220703125, 83.359375, 169.359375, -3902.86083984375, -4080.515625], "mean_td_error": -2730.052978515625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 41000, "num_agent_steps_sampled": 41000, "num_steps_trained": 320032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 320032, "last_target_update_ts": 41000, "num_target_updates": 81}, "done": false, "episodes_total": 136, "training_iteration": 41, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-46-33", "timestamp": 1652705193, "time_this_iter_s": 12.01993441581726, "time_total_s": 590.6686751842499, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 590.6686751842499, "timesteps_since_restore": 1312, "iterations_since_restore": 41, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.294117647058822, "ram_util_percent": 17.235294117647058}}
{"episode_reward_max": 51184.0, "episode_reward_min": 29448.0, "episode_reward_mean": 38225.82, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [46022.0, 39520.0, 36366.0, 29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.23684555127696436, "mean_inference_ms": 2.0200721611825174, "mean_action_processing_ms": 0.06706231742563432, "mean_env_wait_ms": 3.5320292254609225, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 42000, "timesteps_this_iter": 32, "agent_timesteps_total": 42000, "timers": {"load_time_ms": 0.237, "load_throughput": 135204.722, "learn_time_ms": 34.92, "learn_throughput": 916.377}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.030438637360930443, "mean_q": 5629.7431640625, "min_q": 4393.9990234375, "max_q": 8715.7021484375, "cur_lr": 0.6}, "td_error": [-3785.77880859375, -3973.05908203125, -4504.623046875, -3771.77880859375, 97.080078125, -4400.68896484375, -4129.05908203125, -4176.68896484375, -3801.77880859375, -4298.68896484375, 8469.7021484375, -4438.68896484375, -4129.05908203125, -4492.623046875, 123.080078125, -4185.65380859375, -4109.05908203125, -4095.94970703125, -3657.77880859375, 149.080078125, -4063.94970703125, -158.919921875, -4099.94970703125, 163.080078125, -4061.94970703125, -4079.94970703125, -4013.72607421875, -3937.9970703125, -3459.77880859375, -3777.77880859375, -40.919921875, -4173.65380859375], "mean_td_error": -2900.547119140625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 42000, "num_agent_steps_sampled": 42000, "num_steps_trained": 328032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 328032, "last_target_update_ts": 42000, "num_target_updates": 83}, "done": false, "episodes_total": 140, "training_iteration": 42, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-46-46", "timestamp": 1652705206, "time_this_iter_s": 13.39150333404541, "time_total_s": 604.0601785182953, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 604.0601785182953, "timesteps_since_restore": 1344, "iterations_since_restore": 42, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.778947368421054, "ram_util_percent": 17.368421052631575}}
{"episode_reward_max": 51184.0, "episode_reward_min": 29448.0, "episode_reward_mean": 38159.46, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0, 36660.0, 41248.0, 45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.23591202417160134, "mean_inference_ms": 2.013727159645571, "mean_action_processing_ms": 0.0668810759161215, "mean_env_wait_ms": 3.4520662765731123, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 43000, "timesteps_this_iter": 32, "agent_timesteps_total": 43000, "timers": {"load_time_ms": 0.132, "load_throughput": 242576.772, "learn_time_ms": 20.109, "learn_throughput": 1591.33}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.02791420742869377, "mean_q": 6007.57861328125, "min_q": 4537.96142578125, "max_q": 8874.72265625, "cur_lr": 0.6}, "td_error": [157.41015625, -108.58984375, -4517.35107421875, -4116.1005859375, -4070.1005859375, -3814.841796875, -4513.35107421875, -3648.841796875, -3968.841796875, 99.41015625, 103.41015625, -4122.1005859375, -4493.35107421875, -4139.71142578125, -4155.71142578125, -3980.841796875, -4498.2783203125, -3910.841796875, 115.41015625, -4112.1005859375, 61.41015625, -3596.64306640625, -4367.35107421875, -3994.841796875, -3956.841796875, 61.41015625, -3778.841796875, -4485.35107421875, -4090.1005859375, -4167.8603515625, -68.58984375, -88.58984375], "mean_td_error": -2942.734619140625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 43000, "num_agent_steps_sampled": 43000, "num_steps_trained": 336032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 336032, "last_target_update_ts": 43000, "num_target_updates": 85}, "done": false, "episodes_total": 143, "training_iteration": 43, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-47-01", "timestamp": 1652705221, "time_this_iter_s": 14.495108366012573, "time_total_s": 618.5552868843079, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 618.5552868843079, "timesteps_since_restore": 1376, "iterations_since_restore": 43, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.452380952380953, "ram_util_percent": 17.519047619047623}}
{"episode_reward_max": 51184.0, "episode_reward_min": 29448.0, "episode_reward_mean": 38293.8, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [45516.0, 34698.0, 35752.0, 38604.0, 37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.23499631120706532, "mean_inference_ms": 2.0074176954702923, "mean_action_processing_ms": 0.0666879164285093, "mean_env_wait_ms": 3.3792903865052146, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 44000, "timesteps_this_iter": 32, "agent_timesteps_total": 44000, "timers": {"load_time_ms": 0.222, "load_throughput": 144304.621, "learn_time_ms": 34.432, "learn_throughput": 929.376}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03767445310950279, "mean_q": 5881.27734375, "min_q": 4668.87646484375, "max_q": 9025.607421875, "cur_lr": 0.6}, "td_error": [-102.2099609375, -4455.04345703125, -4015.24609375, -3836.31396484375, -3694.31396484375, -4203.0302734375, -3820.31396484375, -4219.48046875, 75.7900390625, -4201.0302734375, -3867.76806640625, -4191.76806640625, -3957.24609375, -3732.31396484375, 159.7900390625, -180.2099609375, -4529.04345703125, -4216.94091796875, -4533.04345703125, -4155.5283203125, -4486.94091796875, -3985.0302734375, -4235.76806640625, -4203.0302734375, -4073.5283203125, -148.2099609375, -44.2099609375, -68.2099609375, -4229.0302734375, -3969.24609375, -4507.04345703125, -4175.76806640625], "mean_td_error": -3243.7900390625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 44000, "num_agent_steps_sampled": 44000, "num_steps_trained": 344032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 344032, "last_target_update_ts": 44000, "num_target_updates": 87}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13402330987347288, "mean_inference_ms": 1.5830593519375493, "mean_action_processing_ms": 0.05826600498591218, "mean_env_wait_ms": 1.1797241643138887, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 146, "training_iteration": 44, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-47-16", "timestamp": 1652705236, "time_this_iter_s": 15.323234558105469, "time_total_s": 633.8785214424133, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 633.8785214424133, "timesteps_since_restore": 1408, "iterations_since_restore": 44, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.413636363636364, "ram_util_percent": 17.659090909090903}}
{"episode_reward_max": 51184.0, "episode_reward_min": 29448.0, "episode_reward_mean": 38398.22, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37314.0, 48954.0, 38066.0, 40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.23376331065129263, "mean_inference_ms": 1.9987687615371914, "mean_action_processing_ms": 0.06641565559562525, "mean_env_wait_ms": 3.290403361088803, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 45000, "timesteps_this_iter": 32, "agent_timesteps_total": 45000, "timers": {"load_time_ms": 0.225, "load_throughput": 142255.144, "learn_time_ms": 34.581, "learn_throughput": 925.36}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.05891057103872299, "mean_q": 5877.60498046875, "min_q": 4818.134765625, "max_q": 9169.17578125, "cur_lr": 0.6}, "td_error": [-4216.95068359375, -4250.95068359375, -4255.419921875, -4249.419921875, -161.859375, -3987.419921875, -4247.419921875, -4135.34765625, -4276.95068359375, -3665.11376953125, -4069.419921875, -4109.34765625, -4204.37744140625, -4390.900390625, -3971.845703125, -4081.34765625, -4508.900390625, 122.140625, -4276.95068359375, -4287.419921875, -81.859375, -189.859375, -4019.845703125, -4486.900390625, -89.859375, -3971.845703125, -4188.95068359375, -3951.34765625, -4128.37744140625, -4448.900390625, -4204.95068359375, -47.859375], "mean_td_error": -3407.367919921875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 45000, "num_agent_steps_sampled": 45000, "num_steps_trained": 352032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 352032, "last_target_update_ts": 45000, "num_target_updates": 89}, "done": false, "episodes_total": 150, "training_iteration": 45, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-47-27", "timestamp": 1652705247, "time_this_iter_s": 10.298239469528198, "time_total_s": 644.1767609119415, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 644.1767609119415, "timesteps_since_restore": 1440, "iterations_since_restore": 45, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.686666666666667, "ram_util_percent": 17.820000000000004}}
{"episode_reward_max": 51184.0, "episode_reward_min": 29448.0, "episode_reward_mean": 38342.56, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [40752.0, 41658.0, 40234.0, 39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2329464867708215, "mean_inference_ms": 1.9931148163230512, "mean_action_processing_ms": 0.06624006128403993, "mean_env_wait_ms": 3.2294119884860195, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 46000, "timesteps_this_iter": 32, "agent_timesteps_total": 46000, "timers": {"load_time_ms": 0.224, "load_throughput": 143043.513, "learn_time_ms": 34.465, "learn_throughput": 928.479}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.02507946267724037, "mean_q": 6709.32666015625, "min_q": 4948.234375, "max_q": 9321.0390625, "cur_lr": 0.6}, "td_error": [-3890.146484375, 62.349609375, -4069.958984375, -4267.958984375, -4498.455078125, -61.650390625, -3826.1162109375, -4126.3095703125, 102.349609375, -4512.455078125, -4014.146484375, 168.349609375, 76.349609375, -4504.455078125, -77.650390625, 168.349609375, -3944.146484375, 164.349609375, -4142.3095703125, -71.650390625, -3834.1162109375, -4217.8505859375, -4263.958984375, 154.349609375, 98.349609375, -3752.1162109375, -3922.3095703125, -3828.1162109375, -3834.1162109375, -4239.8505859375, -4522.455078125, -3842.1162109375], "mean_td_error": -2664.675537109375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 46000, "num_agent_steps_sampled": 46000, "num_steps_trained": 360032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 360032, "last_target_update_ts": 46000, "num_target_updates": 91}, "done": false, "episodes_total": 153, "training_iteration": 46, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-47-41", "timestamp": 1652705261, "time_this_iter_s": 13.842185020446777, "time_total_s": 658.0189459323883, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 658.0189459323883, "timesteps_since_restore": 1472, "iterations_since_restore": 46, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.76842105263158, "ram_util_percent": 17.94736842105263}}
{"episode_reward_max": 51184.0, "episode_reward_min": 29448.0, "episode_reward_mean": 38392.24, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [39676.0, 37742.0, 34764.0, 38434.0, 32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.23217650406194523, "mean_inference_ms": 1.9877954599695231, "mean_action_processing_ms": 0.06607170994227057, "mean_env_wait_ms": 3.172974207298527, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 47000, "timesteps_this_iter": 32, "agent_timesteps_total": 47000, "timers": {"load_time_ms": 0.223, "load_throughput": 143502.329, "learn_time_ms": 34.914, "learn_throughput": 916.53}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03692379221320152, "mean_q": 6529.73779296875, "min_q": 5029.9912109375, "max_q": 9481.9775390625, "cur_lr": 0.6}, "td_error": [-3728.6669921875, -24.0810546875, -3824.6669921875, -44.0810546875, -4004.4365234375, -4554.0673828125, -4036.5654296875, -3984.5654296875, -4590.0673828125, -4210.15673828125, -3926.197265625, 147.9189453125, -4576.0673828125, -3892.6669921875, -4268.197265625, 29.9189453125, -4282.15673828125, 9173.9775390625, -4311.806640625, -4613.4384765625, -68.0810546875, -4527.4384765625, -72.0810546875, 175.9189453125, -4133.806640625, -4274.15673828125, -4435.4384765625, 177.9189453125, -4118.4365234375, -4247.806640625, -4608.0673828125, -3824.6669921875], "mean_td_error": -2733.6318359375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 47000, "num_agent_steps_sampled": 47000, "num_steps_trained": 368032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 368032, "last_target_update_ts": 47000, "num_target_updates": 93}, "done": false, "episodes_total": 156, "training_iteration": 47, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-47-54", "timestamp": 1652705274, "time_this_iter_s": 13.39857268333435, "time_total_s": 671.4175186157227, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 671.4175186157227, "timesteps_since_restore": 1504, "iterations_since_restore": 47, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.465, "ram_util_percent": 18.090000000000003}}
{"episode_reward_max": 51184.0, "episode_reward_min": 29448.0, "episode_reward_mean": 38384.7, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [32998.0, 34362.0, 44426.0, 41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.23133526210540048, "mean_inference_ms": 1.9823312144177925, "mean_action_processing_ms": 0.06590077744372551, "mean_env_wait_ms": 3.1039561951160533, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 48000, "timesteps_this_iter": 32, "agent_timesteps_total": 48000, "timers": {"load_time_ms": 0.208, "load_throughput": 153990.05, "learn_time_ms": 30.246, "learn_throughput": 1057.997}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.02813054248690605, "mean_q": 6370.42578125, "min_q": 5159.46142578125, "max_q": 9616.51953125, "cur_lr": 0.6}, "td_error": [-4346.21875, -4330.21875, -4043.51171875, 54.0146484375, 116.0146484375, -4679.04345703125, 154.0146484375, -191.9853515625, -195.9853515625, -3845.251953125, -4178.9228515625, -4200.369140625, 152.0146484375, -4336.21875, -4607.04345703125, -4288.63720703125, -4344.369140625, -4589.04345703125, -4600.45751953125, -4250.369140625, -4084.9228515625, -4334.21875, -4170.9228515625, -3693.251953125, -3615.251953125, -4583.04345703125, -4606.45751953125, -3946.9228515625, -195.9853515625, -4512.45751953125, -4621.04345703125, -4522.45751953125], "mean_td_error": -3357.4541015625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 48000, "num_agent_steps_sampled": 48000, "num_steps_trained": 376032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 376032, "last_target_update_ts": 48000, "num_target_updates": 95}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13529324127680328, "mean_inference_ms": 1.5975772854487984, "mean_action_processing_ms": 0.05879576952382612, "mean_env_wait_ms": 1.1944266962561467, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 160, "training_iteration": 48, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-48-10", "timestamp": 1652705290, "time_this_iter_s": 16.33525776863098, "time_total_s": 687.7527763843536, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 687.7527763843536, "timesteps_since_restore": 1536, "iterations_since_restore": 48, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.45652173913044, "ram_util_percent": 18.2304347826087}}
{"episode_reward_max": 51184.0, "episode_reward_min": 29448.0, "episode_reward_mean": 38420.52, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [41832.0, 42566.0, 32360.0, 42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.23075073830280732, "mean_inference_ms": 1.9785681216432152, "mean_action_processing_ms": 0.06578201816739603, "mean_env_wait_ms": 3.0559081853313432, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 49000, "timesteps_this_iter": 32, "agent_timesteps_total": 49000, "timers": {"load_time_ms": 0.138, "load_throughput": 231130.925, "learn_time_ms": 20.383, "learn_throughput": 1569.923}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.032384131103754044, "mean_q": 6515.72705078125, "min_q": 5278.68310546875, "max_q": 9755.0546875, "cur_lr": 0.6}, "td_error": [59.5390625, -4158.28076171875, -4039.92529296875, -4293.75634765625, -46.4609375, -3951.17236328125, -4610.83251953125, -4552.5966796875, -66.4609375, 95.5390625, -3879.83984375, -4486.5966796875, 139.5390625, -4277.17236328125, -4275.75634765625, -3999.92529296875, -4450.5966796875, -196.4609375, -4059.75634765625, -4582.5966796875, -4291.61376953125, 159.5390625, -4632.83251953125, -4355.75634765625, -4007.75634765625, -4119.75634765625, -4339.75634765625, -3717.92529296875, -4142.28076171875, -4271.17236328125, -4279.17236328125, -4251.17236328125], "mean_td_error": -3308.8505859375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 49000, "num_agent_steps_sampled": 49000, "num_steps_trained": 384032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 384032, "last_target_update_ts": 49000, "num_target_updates": 97}, "done": false, "episodes_total": 163, "training_iteration": 49, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-48-22", "timestamp": 1652705302, "time_this_iter_s": 11.29569387435913, "time_total_s": 699.0484702587128, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 699.0484702587128, "timesteps_since_restore": 1568, "iterations_since_restore": 49, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.61875, "ram_util_percent": 18.35}}
{"episode_reward_max": 51184.0, "episode_reward_min": 29448.0, "episode_reward_mean": 38507.2, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [42300.0, 35644.0, 36216.0, 37198.0, 44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2303053998745939, "mean_inference_ms": 1.9759912347027866, "mean_action_processing_ms": 0.06570141085491658, "mean_env_wait_ms": 3.011712280092159, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 50000, "timesteps_this_iter": 32, "agent_timesteps_total": 50000, "timers": {"load_time_ms": 0.149, "load_throughput": 214954.721, "learn_time_ms": 22.132, "learn_throughput": 1445.865}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.042112600058317184, "mean_q": 6662.2919921875, "min_q": 5406.876953125, "max_q": 9901.638671875, "cur_lr": 0.6}, "td_error": [-4012.181640625, -73.2685546875, -3835.6611328125, -4143.87939453125, 172.7314453125, -4315.28759765625, -4316.70849609375, -3599.6611328125, -4263.13916015625, -4173.13916015625, -4526.70849609375, -4370.0302734375, -4644.0302734375, -51.2685546875, -4331.28759765625, -4153.87939453125, -4520.0302734375, -4650.0302734375, 5829.24609375, -3979.87939453125, -4004.181640625, -4612.0302734375, -73.2685546875, -4580.70849609375, -3861.6611328125, -175.2685546875, -183.2685546875, -4612.70849609375, -4325.28759765625, -4159.87939453125, -4233.13916015625, 106.7314453125], "mean_td_error": -3021.02392578125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 50000, "num_agent_steps_sampled": 50000, "num_steps_trained": 392032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 392032, "last_target_update_ts": 50000, "num_target_updates": 99}, "done": false, "episodes_total": 166, "training_iteration": 50, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-48-36", "timestamp": 1652705316, "time_this_iter_s": 14.561148405075073, "time_total_s": 713.6096186637878, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 713.6096186637878, "timesteps_since_restore": 1600, "iterations_since_restore": 50, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.014285714285716, "ram_util_percent": 18.500000000000007}}
{"episode_reward_max": 51184.0, "episode_reward_min": 29448.0, "episode_reward_mean": 38614.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [44126.0, 31206.0, 39598.0, 29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22971265943241442, "mean_inference_ms": 1.9723961656932938, "mean_action_processing_ms": 0.0655854888580985, "mean_env_wait_ms": 2.9563878840116344, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 51000, "timesteps_this_iter": 32, "agent_timesteps_total": 51000, "timers": {"load_time_ms": 0.128, "load_throughput": 250266.135, "learn_time_ms": 19.78, "learn_throughput": 1617.816}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.029332904145121574, "mean_q": 7239.185546875, "min_q": 5524.77978515625, "max_q": 10040.18359375, "cur_lr": 0.6}, "td_error": [-4275.69384765625, -4268.802734375, -3849.8935546875, -3811.8935546875, -131.224609375, 30.775390625, -3825.8935546875, 9766.18359375, 70.775390625, 170.775390625, -3849.8935546875, -3847.8935546875, -61.224609375, -4336.62841796875, -4232.802734375, -3797.8935546875, -4558.62841796875, -4086.08056640625, -4210.55224609375, -3603.8935546875, -163.224609375, 156.775390625, -161.224609375, -4618.62841796875, -4601.3076171875, -3799.8935546875, -4344.55224609375, -4688.62841796875, -4631.3076171875, -169.224609375, -4120.802734375, -4585.3076171875], "mean_td_error": -2576.1787109375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 51000, "num_agent_steps_sampled": 51000, "num_steps_trained": 400032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 400032, "last_target_update_ts": 51000, "num_target_updates": 101}, "done": false, "episodes_total": 170, "training_iteration": 51, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-48-49", "timestamp": 1652705329, "time_this_iter_s": 12.569339036941528, "time_total_s": 726.1789577007294, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 726.1789577007294, "timesteps_since_restore": 1632, "iterations_since_restore": 51, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 24.13888888888889, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 51184.0, "episode_reward_min": 29448.0, "episode_reward_mean": 38575.64, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0, 38016.0, 35884.0, 38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22925450529392818, "mean_inference_ms": 1.9695225116815316, "mean_action_processing_ms": 0.06549285097929498, "mean_env_wait_ms": 2.91739483919694, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 52000, "timesteps_this_iter": 32, "agent_timesteps_total": 52000, "timers": {"load_time_ms": 0.131, "load_throughput": 243545.142, "learn_time_ms": 19.938, "learn_throughput": 1604.97}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0679117813706398, "mean_q": 8219.9326171875, "min_q": 5645.64892578125, "max_q": 10175.240234375, "cur_lr": 0.6}, "td_error": [168.6162109375, -3808.19677734375, 168.6162109375, -25.3837890625, -3903.88671875, 72.6162109375, 166.6162109375, -4696.97509765625, -4597.419921875, 168.6162109375, -39.3837890625, -4317.85791015625, -4369.11181640625, -4235.419921875, 168.6162109375, -4450.97509765625, -4313.85791015625, -4333.85791015625, -69.3837890625, -3857.88671875, 114.6162109375, -4246.8564453125, -4499.419921875, -117.3837890625, -169.3837890625, -3833.88671875, -101.3837890625, -73.3837890625, -79.3837890625, -83.3837890625, -4672.97509765625, -49.3837890625], "mean_td_error": -1997.44091796875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 52000, "num_agent_steps_sampled": 52000, "num_steps_trained": 408032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 408032, "last_target_update_ts": 52000, "num_target_updates": 103}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.1313668646833219, "mean_inference_ms": 1.546678467305371, "mean_action_processing_ms": 0.05691713383490537, "mean_env_wait_ms": 1.163119568148811, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 173, "training_iteration": 52, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-49-03", "timestamp": 1652705343, "time_this_iter_s": 13.92561936378479, "time_total_s": 740.1045770645142, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 740.1045770645142, "timesteps_since_restore": 1664, "iterations_since_restore": 52, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.025, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 51184.0, "episode_reward_min": 29714.0, "episode_reward_mean": 38689.04, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [38510.0, 44284.0, 51184.0, 38068.0, 38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2288561648373807, "mean_inference_ms": 1.967052355378519, "mean_action_processing_ms": 0.06541334425919301, "mean_env_wait_ms": 2.881181422333148, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 53000, "timesteps_this_iter": 32, "agent_timesteps_total": 53000, "timers": {"load_time_ms": 0.218, "load_throughput": 146605.929, "learn_time_ms": 32.77, "learn_throughput": 976.499}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.041710879653692245, "mean_q": 7432.1865234375, "min_q": 5753.34912109375, "max_q": 10301.3525390625, "cur_lr": 0.6}, "td_error": [-4342.73974609375, -4216.6875, -4618.99267578125, -3536.8544921875, -4682.99267578125, 159.0107421875, 157.0107421875, 137.0107421875, 73.0107421875, -68.9892578125, -4720.99267578125, -3896.8544921875, -208.9892578125, -4580.1650390625, -4106.2509765625, -42.9892578125, -4446.99267578125, -4210.40771484375, -4143.47998046875, -4126.2509765625, 59.0107421875, -4234.40771484375, -4350.73974609375, -4576.99267578125, -3872.8544921875, -70.9892578125, -4558.1650390625, -4704.99267578125, -3720.8544921875, -4574.1650390625, -4048.2509765625, -188.9892578125], "mean_td_error": -2945.843017578125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 53000, "num_agent_steps_sampled": 53000, "num_steps_trained": 416032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 416032, "last_target_update_ts": 53000, "num_target_updates": 105}, "done": false, "episodes_total": 176, "training_iteration": 53, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-49-16", "timestamp": 1652705356, "time_this_iter_s": 12.609762907028198, "time_total_s": 752.7143399715424, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 752.7143399715424, "timesteps_since_restore": 1696, "iterations_since_restore": 53, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.583333333333332, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 49940.0, "episode_reward_min": 29714.0, "episode_reward_mean": 38550.28, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [38600.0, 36352.0, 43128.0, 41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22840633585804576, "mean_inference_ms": 1.9644681001896176, "mean_action_processing_ms": 0.06533022132200901, "mean_env_wait_ms": 2.8358657512918652, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 54000, "timesteps_this_iter": 32, "agent_timesteps_total": 54000, "timers": {"load_time_ms": 0.213, "load_throughput": 150198.89, "learn_time_ms": 32.763, "learn_throughput": 976.713}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.02463354542851448, "mean_q": 8422.4326171875, "min_q": 5840.99853515625, "max_q": 10423.7861328125, "cur_lr": 0.6}, "td_error": [-192.41015625, 31.58984375, -76.41015625, 153.58984375, -4156.64306640625, -178.41015625, -3538.36962890625, 87.58984375, 97.58984375, -4353.087890625, -3880.36962890625, -48.41015625, -3676.36962890625, 157.58984375, -4036.44140625, -158.41015625, -4265.982421875, -4151.982421875, -4733.19775390625, -3848.44140625, -182.41015625, -46.41015625, -4751.19775390625, -3578.36962890625, 93.58984375, -4152.64306640625, 57.58984375, 3.58984375, -3892.36962890625, -204.41015625, -3810.36962890625, -4576.04833984375], "mean_td_error": -2056.451416015625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 54000, "num_agent_steps_sampled": 54000, "num_steps_trained": 424032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 424032, "last_target_update_ts": 54000, "num_target_updates": 107}, "done": false, "episodes_total": 180, "training_iteration": 54, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-49-31", "timestamp": 1652705371, "time_this_iter_s": 15.12781047821045, "time_total_s": 767.8421504497528, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 767.8421504497528, "timesteps_since_restore": 1728, "iterations_since_restore": 54, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.754545454545454, "ram_util_percent": 18.60000000000001}}
{"episode_reward_max": 49940.0, "episode_reward_min": 29714.0, "episode_reward_mean": 38588.8, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [41882.0, 37218.0, 34696.0, 32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22808562307562774, "mean_inference_ms": 1.962563270416117, "mean_action_processing_ms": 0.06526929534728629, "mean_env_wait_ms": 2.8037740225293835, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 55000, "timesteps_this_iter": 32, "agent_timesteps_total": 55000, "timers": {"load_time_ms": 0.266, "load_throughput": 120385.441, "learn_time_ms": 38.535, "learn_throughput": 830.412}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.09258001297712326, "mean_q": 8947.6650390625, "min_q": 6147.2958984375, "max_q": 10510.5654296875, "cur_lr": 0.6}, "td_error": [-21.779296875, -4290.73046875, -181.779296875, -91.779296875, -4398.73046875, -4214.8564453125, -3855.43359375, 52.220703125, -19.779296875, -189.779296875, -117.779296875, -79.779296875, -4174.7353515625, -27.779296875, -83.779296875, 12.220703125, 142.220703125, -4178.7353515625, -4236.73046875, -4280.8564453125, -45.779296875, -93.779296875, -4427.048828125, 116.220703125, -95.779296875, -197.779296875, -91.779296875, -4404.7353515625, 126.220703125, -4280.7353515625, -91.779296875, -4310.8564453125], "mean_td_error": -1626.11767578125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 55000, "num_agent_steps_sampled": 55000, "num_steps_trained": 432032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 432032, "last_target_update_ts": 55000, "num_target_updates": 109}, "done": false, "episodes_total": 183, "training_iteration": 55, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-49-44", "timestamp": 1652705384, "time_this_iter_s": 13.526843309402466, "time_total_s": 781.3689937591553, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 781.3689937591553, "timesteps_since_restore": 1760, "iterations_since_restore": 55, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.95263157894737, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 49940.0, "episode_reward_min": 29714.0, "episode_reward_mean": 38565.04, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [32546.0, 40140.0, 35212.0, 37182.0, 32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22776977231662837, "mean_inference_ms": 1.9606303188764684, "mean_action_processing_ms": 0.06520730286860528, "mean_env_wait_ms": 2.773358768643088, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 56000, "timesteps_this_iter": 32, "agent_timesteps_total": 56000, "timers": {"load_time_ms": 0.234, "load_throughput": 136887.025, "learn_time_ms": 34.989, "learn_throughput": 914.583}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.04200654476881027, "mean_q": 9012.1767578125, "min_q": 5962.7421875, "max_q": 10585.681640625, "cur_lr": 0.6}, "td_error": [37.0498046875, 113.0498046875, -28.9501953125, -76.9501953125, 85.0498046875, 143.0498046875, 9.0498046875, 81.0498046875, -104.9501953125, 135.0498046875, 29.0498046875, 65.0498046875, -3737.66845703125, -4803.8896484375, -4429.69677734375, -4156.71044921875, 143.0498046875, -98.9501953125, 121.0498046875, -4370.71044921875, -4592.67333984375, 71.0498046875, -2.9501953125, 143.0498046875, 39.0498046875, -204.9501953125, -4174.8896484375, -4222.8896484375, -4148.134765625, -4479.8896484375, -4030.71044921875, -4265.69677734375], "mean_td_error": -1584.8927001953125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 56000, "num_agent_steps_sampled": 56000, "num_steps_trained": 440032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 440032, "last_target_update_ts": 56000, "num_target_updates": 111}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13079469585895426, "mean_inference_ms": 1.545174239335018, "mean_action_processing_ms": 0.05689435730488974, "mean_env_wait_ms": 1.159038012494816, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 186, "training_iteration": 56, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-49-58", "timestamp": 1652705398, "time_this_iter_s": 13.628072023391724, "time_total_s": 794.997065782547, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 794.997065782547, "timesteps_since_restore": 1792, "iterations_since_restore": 56, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.215789473684207, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 49940.0, "episode_reward_min": 29714.0, "episode_reward_mean": 38671.6, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [32500.0, 35976.0, 33396.0, 37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22733019014463815, "mean_inference_ms": 1.957815825926947, "mean_action_processing_ms": 0.06511840603806335, "mean_env_wait_ms": 2.734824743198568, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 57000, "timesteps_this_iter": 32, "agent_timesteps_total": 57000, "timers": {"load_time_ms": 0.13, "load_throughput": 246723.765, "learn_time_ms": 20.385, "learn_throughput": 1569.75}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.029047606512904167, "mean_q": 9254.0234375, "min_q": 6301.248046875, "max_q": 10671.921875, "cur_lr": 0.6}, "td_error": [150.8349609375, -3867.736328125, -79.1650390625, 150.8349609375, -93.1650390625, 116.8349609375, -89.1650390625, -4138.033203125, -4214.71923828125, -63.1650390625, -4225.484375, 150.8349609375, 78.8349609375, -101.1650390625, -127.1650390625, 2.8349609375, -4132.033203125, -51.1650390625, 86.8349609375, -131.1650390625, -4353.8388671875, -85.1650390625, -4473.8388671875, 20.8349609375, -4425.8388671875, -4052.18798828125, -89.1650390625, -4040.033203125, -4569.8388671875, 10415.921875, -99.1650390625, -159.1650390625], "mean_td_error": -1140.2176513671875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 57000, "num_agent_steps_sampled": 57000, "num_steps_trained": 448032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 448032, "last_target_update_ts": 57000, "num_target_updates": 113}, "done": false, "episodes_total": 190, "training_iteration": 57, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-50-12", "timestamp": 1652705412, "time_this_iter_s": 13.48842167854309, "time_total_s": 808.4854874610901, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 808.4854874610901, "timesteps_since_restore": 1824, "iterations_since_restore": 57, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.294999999999998, "ram_util_percent": 18.689999999999998}}
{"episode_reward_max": 49940.0, "episode_reward_min": 29714.0, "episode_reward_mean": 38811.52, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37474.0, 33390.0, 33468.0, 38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22697233365339892, "mean_inference_ms": 1.9553784543861967, "mean_action_processing_ms": 0.06503946287893814, "mean_env_wait_ms": 2.707298870178019, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 58000, "timesteps_this_iter": 32, "agent_timesteps_total": 58000, "timers": {"load_time_ms": 0.227, "load_throughput": 141118.419, "learn_time_ms": 33.618, "learn_throughput": 951.883}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.029676400125026703, "mean_q": 9822.5439453125, "min_q": 6367.54296875, "max_q": 10735.3515625, "cur_lr": 0.6}, "td_error": [10485.3515625, 61.3701171875, 65.3701171875, -4183.4970703125, -4296.4384765625, -4147.4853515625, -94.6298828125, -98.6298828125, -102.6298828125, 113.3701171875, 59.3701171875, -90.6298828125, 83.3701171875, 71.3701171875, 123.3701171875, 83.3701171875, 67.3701171875, -54.6298828125, 75.3701171875, -3586.4609375, -4488.4384765625, -56.6298828125, -54.6298828125, 139.3701171875, 145.3701171875, -4065.4970703125, -48.6298828125, 105.3701171875, 145.3701171875, -4572.4384765625, -66.6298828125, -56.6298828125], "mean_td_error": -570.0006713867188, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 58000, "num_agent_steps_sampled": 58000, "num_steps_trained": 456032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 456032, "last_target_update_ts": 58000, "num_target_updates": 115}, "done": false, "episodes_total": 193, "training_iteration": 58, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-50-23", "timestamp": 1652705423, "time_this_iter_s": 10.940314292907715, "time_total_s": 819.4258017539978, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 819.4258017539978, "timesteps_since_restore": 1856, "iterations_since_restore": 58, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.826666666666664, "ram_util_percent": 18.686666666666664}}
{"episode_reward_max": 49940.0, "episode_reward_min": 29714.0, "episode_reward_mean": 38935.9, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [38674.0, 49940.0, 39750.0, 40192.0, 39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22666982274116992, "mean_inference_ms": 1.9534051039917313, "mean_action_processing_ms": 0.0649768453213285, "mean_env_wait_ms": 2.6816108482010956, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 59000, "timesteps_this_iter": 32, "agent_timesteps_total": 59000, "timers": {"load_time_ms": 0.144, "load_throughput": 222104.465, "learn_time_ms": 22.071, "learn_throughput": 1449.88}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.04218875989317894, "mean_q": 9845.3974609375, "min_q": 6090.91943359375, "max_q": 10783.5390625, "cur_lr": 0.6}, "td_error": [59.1171875, -124.8828125, -114.8828125, -112.8828125, -4360.87060546875, -32.8828125, 81.1171875, -84.8828125, -118.8828125, -106.8828125, 91.1171875, 49.1171875, 77.1171875, 119.1171875, 121.1171875, 75.1171875, -76.8828125, 10509.5390625, -4230.87060546875, -4270.587890625, -4719.50244140625, -108.8828125, -4486.587890625, 10563.5390625, -4012.015625, -84.8828125, -78.8828125, 21.1171875, -20.8828125, -102.8828125, -4256.2841796875, -82.8828125], "mean_td_error": -306.935791015625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 59000, "num_agent_steps_sampled": 59000, "num_steps_trained": 464032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 464032, "last_target_update_ts": 59000, "num_target_updates": 117}, "done": false, "episodes_total": 196, "training_iteration": 59, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-50-37", "timestamp": 1652705437, "time_this_iter_s": 14.480679273605347, "time_total_s": 833.9064810276031, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 833.9064810276031, "timesteps_since_restore": 1888, "iterations_since_restore": 59, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.347619047619048, "ram_util_percent": 18.666666666666668}}
{"episode_reward_max": 47462.0, "episode_reward_min": 29714.0, "episode_reward_mean": 38870.08, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [39984.0, 45580.0, 35888.0, 36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.226276868880771, "mean_inference_ms": 1.9507350138345032, "mean_action_processing_ms": 0.06489241516435992, "mean_env_wait_ms": 2.6487237151584693, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 60000, "timesteps_this_iter": 32, "agent_timesteps_total": 60000, "timers": {"load_time_ms": 0.232, "load_throughput": 137970.526, "learn_time_ms": 34.856, "learn_throughput": 918.061}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.06843066215515137, "mean_q": 10320.54296875, "min_q": 6678.701171875, "max_q": 10840.806640625, "cur_lr": 0.6}, "td_error": [130.5078125, 130.5078125, -93.4921875, 110.5078125, 0.5078125, -9.4921875, 132.5078125, -9.4921875, -4275.59765625, 32.5078125, 130.5078125, -71.4921875, -101.4921875, 132.5078125, 128.5078125, 72.5078125, 130.5078125, 10556.806640625, -71.4921875, 130.5078125, 22.5078125, 10586.806640625, 72.5078125, 34.5078125, 102.5078125, -45.4921875, -65.4921875, -4157.59765625, -4117.59765625, 26.5078125, -4143.59765625, 30.5078125], "mean_td_error": 172.8883056640625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 60000, "num_agent_steps_sampled": 60000, "num_steps_trained": 472032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 472032, "last_target_update_ts": 60000, "num_target_updates": 119}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.1314608580905632, "mean_inference_ms": 1.5611852494591316, "mean_action_processing_ms": 0.05745818895171733, "mean_env_wait_ms": 1.1713716671801069, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 200, "training_iteration": 60, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-50-54", "timestamp": 1652705454, "time_this_iter_s": 17.13592028617859, "time_total_s": 851.0424013137817, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 851.0424013137817, "timesteps_since_restore": 1920, "iterations_since_restore": 60, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.376000000000005, "ram_util_percent": 18.604}}
{"episode_reward_max": 47462.0, "episode_reward_min": 29520.0, "episode_reward_mean": 38716.48, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36804.0, 36490.0, 45296.0, 42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22601667953388804, "mean_inference_ms": 1.94901715259005, "mean_action_processing_ms": 0.06483753959188893, "mean_env_wait_ms": 2.6253578481915762, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 61000, "timesteps_this_iter": 32, "agent_timesteps_total": 61000, "timers": {"load_time_ms": 0.287, "load_throughput": 111559.91, "learn_time_ms": 37.403, "learn_throughput": 855.552}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.07604356855154037, "mean_q": 10227.126953125, "min_q": 6163.30615234375, "max_q": 10884.974609375, "cur_lr": 0.6}, "td_error": [130.3349609375, 112.3349609375, 18.3349609375, 130.3349609375, 26.3349609375, -109.6650390625, 126.3349609375, -135.6650390625, 70.3349609375, -3918.6748046875, -93.6650390625, -41.6650390625, -3920.6748046875, 130.3349609375, -15.6650390625, -79.6650390625, 8.3349609375, -117.6650390625, -79.6650390625, 128.3349609375, 60.3349609375, 64.3349609375, -135.6650390625, -95.6650390625, -4404.9140625, -4607.33349609375, 66.3349609375, -69.6650390625, -4147.873046875, 70.3349609375, 28.3349609375, 124.3349609375], "mean_td_error": -646.2008056640625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 61000, "num_agent_steps_sampled": 61000, "num_steps_trained": 480032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 480032, "last_target_update_ts": 61000, "num_target_updates": 121}, "done": false, "episodes_total": 203, "training_iteration": 61, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-51-08", "timestamp": 1652705468, "time_this_iter_s": 13.39093542098999, "time_total_s": 864.4333367347717, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 864.4333367347717, "timesteps_since_restore": 1952, "iterations_since_restore": 61, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.27894736842105, "ram_util_percent": 18.61052631578948}}
{"episode_reward_max": 47462.0, "episode_reward_min": 29520.0, "episode_reward_mean": 38600.36, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [42264.0, 33272.0, 36726.0, 39726.0, 42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2258061261225291, "mean_inference_ms": 1.947743265545754, "mean_action_processing_ms": 0.06479670627421086, "mean_env_wait_ms": 2.6032108267535494, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 62000, "timesteps_this_iter": 32, "agent_timesteps_total": 62000, "timers": {"load_time_ms": 0.263, "load_throughput": 121475.0, "learn_time_ms": 38.726, "learn_throughput": 826.324}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.13336431980133057, "mean_q": 10683.2119140625, "min_q": 6786.91552734375, "max_q": 10939.1953125, "cur_lr": 0.6}, "td_error": [134.44921875, -71.55078125, -31.55078125, 38.44921875, -4015.83056640625, -69.55078125, -61.55078125, -107.55078125, -67.55078125, 136.44921875, -81.55078125, 136.44921875, 10685.1953125, -113.55078125, -71.55078125, -111.55078125, -65.55078125, -121.55078125, 66.44921875, 104.44921875, -77.55078125, -4028.720703125, -111.55078125, -103.55078125, -71.55078125, -99.55078125, -149.55078125, 96.44921875, -53.55078125, -105.55078125, -3.55078125, -133.55078125], "mean_td_error": 45.92723083496094, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 62000, "num_agent_steps_sampled": 62000, "num_steps_trained": 488032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 488032, "last_target_update_ts": 62000, "num_target_updates": 123}, "done": false, "episodes_total": 206, "training_iteration": 62, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-51-23", "timestamp": 1652705483, "time_this_iter_s": 14.889286279678345, "time_total_s": 879.3226230144501, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 879.3226230144501, "timesteps_since_restore": 1984, "iterations_since_restore": 62, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.31904761904762, "ram_util_percent": 18.633333333333336}}
{"episode_reward_max": 47462.0, "episode_reward_min": 29520.0, "episode_reward_mean": 38560.06, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [42982.0, 38140.0, 40180.0, 37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22558599590952766, "mean_inference_ms": 1.9466350236417116, "mean_action_processing_ms": 0.06476088101124508, "mean_env_wait_ms": 2.575308429684542, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 63000, "timesteps_this_iter": 32, "agent_timesteps_total": 63000, "timers": {"load_time_ms": 0.132, "load_throughput": 242357.761, "learn_time_ms": 20.426, "learn_throughput": 1566.628}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.06670527905225754, "mean_q": 10864.6611328125, "min_q": 6935.291015625, "max_q": 10991.416015625, "cur_lr": 0.6}, "td_error": [139.9169921875, 123.9169921875, 53.9169921875, 81.9169921875, 141.9169921875, -120.0830078125, -60.0830078125, -68.0830078125, -78.0830078125, 141.9169921875, -110.0830078125, 10717.416015625, -64.0830078125, -4.0830078125, 121.9169921875, 141.9169921875, 10735.416015625, 73.9169921875, -126.0830078125, -126.0830078125, 139.9169921875, 81.9169921875, 29.9169921875, -64.0830078125, -106.0830078125, -64.0830078125, 113.9169921875, 119.9169921875, 141.9169921875, 121.9169921875, -3970.2080078125, 141.9169921875], "mean_td_error": 575.1317749023438, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 63000, "num_agent_steps_sampled": 63000, "num_steps_trained": 496032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 496032, "last_target_update_ts": 63000, "num_target_updates": 125}, "done": false, "episodes_total": 210, "training_iteration": 63, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-51-35", "timestamp": 1652705495, "time_this_iter_s": 12.177221775054932, "time_total_s": 891.499844789505, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 891.499844789505, "timesteps_since_restore": 2016, "iterations_since_restore": 63, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.45, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 47462.0, "episode_reward_min": 29520.0, "episode_reward_mean": 38493.68, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37246.0, 38612.0, 39670.0, 37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2254509622018907, "mean_inference_ms": 1.9460836787528453, "mean_action_processing_ms": 0.06474229305304877, "mean_env_wait_ms": 2.555492873480741, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 64000, "timesteps_this_iter": 32, "agent_timesteps_total": 64000, "timers": {"load_time_ms": 0.228, "load_throughput": 140586.287, "learn_time_ms": 36.515, "learn_throughput": 876.349}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.001694355858489871, "mean_q": 10360.57421875, "min_q": 6266.4833984375, "max_q": 11049.6953125, "cur_lr": 0.6}, "td_error": [41.0654296875, -4192.00390625, -90.9345703125, 133.0654296875, -4334.00390625, 10797.6953125, -4163.8447265625, -110.9345703125, 69.0654296875, -104.9345703125, -8.9345703125, -4774.146484375, 10801.6953125, -28.9345703125, 121.0654296875, 61.0654296875, -112.9345703125, -66.9345703125, 10797.6953125, -70.9345703125, 69.0654296875, -4442.56787109375, 71.0654296875, -90.9345703125, 121.0654296875, 139.0654296875, -128.9345703125, 59.0654296875, -74.9345703125, 139.0654296875, 61.0654296875, -36.9345703125], "mean_td_error": 332.7528076171875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 64000, "num_agent_steps_sampled": 64000, "num_steps_trained": 504032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 504032, "last_target_update_ts": 64000, "num_target_updates": 127}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13226023616206767, "mean_inference_ms": 1.5757471738917408, "mean_action_processing_ms": 0.05801527034836594, "mean_env_wait_ms": 1.1841729888566408, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 213, "training_iteration": 64, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-51-48", "timestamp": 1652705508, "time_this_iter_s": 12.877682447433472, "time_total_s": 904.3775272369385, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 904.3775272369385, "timesteps_since_restore": 2048, "iterations_since_restore": 64, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 25.244444444444447, "ram_util_percent": 18.616666666666667}}
{"episode_reward_max": 47462.0, "episode_reward_min": 29520.0, "episode_reward_mean": 38489.34, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37636.0, 36094.0, 35136.0, 34800.0, 38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22531605200523253, "mean_inference_ms": 1.9454994701458586, "mean_action_processing_ms": 0.06472169680000173, "mean_env_wait_ms": 2.5364641173774807, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 65000, "timesteps_this_iter": 32, "agent_timesteps_total": 65000, "timers": {"load_time_ms": 0.237, "load_throughput": 134837.983, "learn_time_ms": 38.052, "learn_throughput": 840.948}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0012073572725057602, "mean_q": 10317.7265625, "min_q": 6902.80029296875, "max_q": 11061.3291015625, "cur_lr": 0.6}, "td_error": [-122.1796875, 117.8203125, 115.8203125, -86.1796875, 115.8203125, -84.1796875, -4091.80078125, 117.8203125, -114.1796875, -4061.80078125, 85.8203125, -138.1796875, 109.8203125, 117.8203125, 87.8203125, 117.8203125, -90.1796875, 115.8203125, 103.8203125, -3661.11962890625, -110.1796875, -134.1796875, 117.8203125, -3715.11962890625, -134.1796875, -4098.70849609375, -84.1796875, -94.1796875, 103.8203125, -66.1796875, 57.8203125, -4061.80078125], "mean_td_error": -733.2194213867188, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 65000, "num_agent_steps_sampled": 65000, "num_steps_trained": 512032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 512032, "last_target_update_ts": 65000, "num_target_updates": 129}, "done": false, "episodes_total": 216, "training_iteration": 65, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-52-01", "timestamp": 1652705521, "time_this_iter_s": 12.556036233901978, "time_total_s": 916.9335634708405, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 916.9335634708405, "timesteps_since_restore": 2080, "iterations_since_restore": 65, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.105555555555554, "ram_util_percent": 18.65555555555555}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 38637.12, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [38752.0, 43804.0, 39464.0, 36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22518821929249078, "mean_inference_ms": 1.9452158475809338, "mean_action_processing_ms": 0.06471123239988379, "mean_env_wait_ms": 2.512456866611668, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 66000, "timesteps_this_iter": 32, "agent_timesteps_total": 66000, "timers": {"load_time_ms": 0.224, "load_throughput": 142906.44, "learn_time_ms": 35.618, "learn_throughput": 898.428}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.004096743185073137, "mean_q": 10805.3408203125, "min_q": 6616.0966796875, "max_q": 11074.9697265625, "cur_lr": 0.6}, "td_error": [71.408203125, 31.408203125, 63.408203125, 59.408203125, -124.591796875, 109.408203125, -110.591796875, 115.408203125, 91.408203125, -126.591796875, 113.408203125, -124.591796875, -30.591796875, 10828.9697265625, -30.591796875, -6.591796875, 19.408203125, 10824.9697265625, -80.591796875, -84.591796875, 117.408203125, -4071.8232421875, -24.591796875, -4465.46484375, 105.408203125, 47.408203125, -158.591796875, -80.591796875, -66.591796875, 99.408203125, -56.591796875, 51.408203125], "mean_td_error": 409.56500244140625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 66000, "num_agent_steps_sampled": 66000, "num_steps_trained": 520032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 520032, "last_target_update_ts": 66000, "num_target_updates": 131}, "done": false, "episodes_total": 220, "training_iteration": 66, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-52-16", "timestamp": 1652705536, "time_this_iter_s": 15.374354362487793, "time_total_s": 932.3079178333282, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 932.3079178333282, "timesteps_since_restore": 2112, "iterations_since_restore": 66, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.627272727272725, "ram_util_percent": 18.63181818181818}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 38634.5, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36804.0, 39692.0, 40222.0, 36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22511544529326422, "mean_inference_ms": 1.9451578783505041, "mean_action_processing_ms": 0.0647079103128132, "mean_env_wait_ms": 2.4952961266680718, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 67000, "timesteps_this_iter": 32, "agent_timesteps_total": 67000, "timers": {"load_time_ms": 0.242, "load_throughput": 132208.164, "learn_time_ms": 35.602, "learn_throughput": 898.832}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0018626288510859013, "mean_q": 10685.380859375, "min_q": 6283.34228515625, "max_q": 11087.04296875, "cur_lr": 0.6}, "td_error": [-53.19921875, 56.80078125, -123.19921875, -111.19921875, 116.80078125, 68.80078125, 10829.04296875, -131.19921875, 114.80078125, 78.80078125, -3584.2958984375, -145.19921875, -121.19921875, 10839.04296875, 114.80078125, 56.80078125, -33.19921875, 56.80078125, 116.80078125, 114.80078125, -81.19921875, -145.19921875, -145.19921875, 94.80078125, 14.80078125, 116.80078125, -4836.89990234375, -4435.62158203125, 116.80078125, -113.19921875, -141.19921875, -35.19921875], "mean_td_error": 270.96533203125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 67000, "num_agent_steps_sampled": 67000, "num_steps_trained": 528032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 528032, "last_target_update_ts": 67000, "num_target_updates": 133}, "done": false, "episodes_total": 223, "training_iteration": 67, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-52-32", "timestamp": 1652705552, "time_this_iter_s": 15.638253211975098, "time_total_s": 947.9461710453033, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 947.9461710453033, "timesteps_since_restore": 2144, "iterations_since_restore": 67, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.85909090909091, "ram_util_percent": 18.609090909090916}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 38702.82, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36444.0, 43908.0, 36344.0, 36126.0, 32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22502622178489798, "mean_inference_ms": 1.9449216027113152, "mean_action_processing_ms": 0.06469664628720462, "mean_env_wait_ms": 2.478758753155286, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 68000, "timesteps_this_iter": 32, "agent_timesteps_total": 68000, "timers": {"load_time_ms": 0.132, "load_throughput": 242840.109, "learn_time_ms": 20.089, "learn_throughput": 1592.898}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.005297119729220867, "mean_q": 10164.4462890625, "min_q": 6291.9345703125, "max_q": 11107.451171875, "cur_lr": 0.6}, "td_error": [-4250.94140625, 45.275390625, -4556.69091796875, -102.724609375, 105.275390625, -124.724609375, -76.724609375, -4042.94140625, -4750.2412109375, -4282.2861328125, -112.724609375, -116.724609375, -132.724609375, -80.724609375, -8.724609375, 93.275390625, -118.724609375, -108.724609375, -98.724609375, -112.724609375, -4456.69091796875, 123.275390625, -3949.44921875, 43.275390625, -112.724609375, -120.724609375, -142.724609375, 107.275390625, 121.275390625, 121.275390625, -74.724609375, 89.275390625], "mean_td_error": -971.4174194335938, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 68000, "num_agent_steps_sampled": 68000, "num_steps_trained": 536032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 536032, "last_target_update_ts": 68000, "num_target_updates": 135}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.1292977466744036, "mean_inference_ms": 1.5387077514762109, "mean_action_processing_ms": 0.05664621654900119, "mean_env_wait_ms": 1.165403943042198, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 226, "training_iteration": 68, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-52-43", "timestamp": 1652705563, "time_this_iter_s": 10.860044240951538, "time_total_s": 958.8062152862549, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 958.8062152862549, "timesteps_since_restore": 2176, "iterations_since_restore": 68, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.775, "ram_util_percent": 18.674999999999997}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 38662.36, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [32538.0, 32374.0, 39824.0, 37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2249040917162481, "mean_inference_ms": 1.944560324869325, "mean_action_processing_ms": 0.06467891836806522, "mean_env_wait_ms": 2.457587776250698, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 69000, "timesteps_this_iter": 32, "agent_timesteps_total": 69000, "timers": {"load_time_ms": 0.129, "load_throughput": 247497.193, "learn_time_ms": 20.137, "learn_throughput": 1589.124}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0016905569937080145, "mean_q": 10352.76171875, "min_q": 6639.81640625, "max_q": 11127.3115234375, "cur_lr": 0.6}, "td_error": [-83.923828125, 50.076171875, 48.076171875, 62.076171875, -3799.26220703125, 54.076171875, -4331.11572265625, -57.923828125, 122.076171875, -83.923828125, -77.923828125, -4297.11572265625, 120.076171875, 18.076171875, 60.076171875, -129.923828125, 90.076171875, -137.923828125, 92.076171875, 64.076171875, -147.923828125, -4601.4189453125, -117.923828125, -67.923828125, -79.923828125, -113.923828125, -97.923828125, -123.923828125, -4172.48046875, -4093.724609375, 94.076171875, 30.076171875], "mean_td_error": -803.4730224609375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 69000, "num_agent_steps_sampled": 69000, "num_steps_trained": 544032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 544032, "last_target_update_ts": 69000, "num_target_updates": 137}, "done": false, "episodes_total": 230, "training_iteration": 69, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-52-52", "timestamp": 1652705572, "time_this_iter_s": 8.816442251205444, "time_total_s": 967.6226575374603, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 967.6226575374603, "timesteps_since_restore": 2208, "iterations_since_restore": 69, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 21.658333333333335, "ram_util_percent": 18.658333333333328}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 38871.06, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37530.0, 36300.0, 35308.0, 32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22480794183167138, "mean_inference_ms": 1.944261427375742, "mean_action_processing_ms": 0.06466393502110583, "mean_env_wait_ms": 2.4424356356302837, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 70000, "timesteps_this_iter": 32, "agent_timesteps_total": 70000, "timers": {"load_time_ms": 0.247, "load_throughput": 129603.832, "learn_time_ms": 35.313, "learn_throughput": 906.177}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.001281936652958393, "mean_q": 10507.55078125, "min_q": 6765.6630859375, "max_q": 11150.646484375, "cur_lr": 0.6}, "td_error": [-49.662109375, 120.337890625, -3740.04833984375, 120.337890625, -125.662109375, 10910.646484375, -75.662109375, -4146.5771484375, -99.662109375, 114.337890625, 58.337890625, 108.337890625, -4468.6455078125, 100.337890625, -53.662109375, -17.662109375, 120.337890625, 120.337890625, -83.662109375, -117.662109375, 120.337890625, 112.337890625, -111.662109375, 8.337890625, 120.337890625, -4106.5771484375, -51.662109375, -25.662109375, -4153.53662109375, 40.337890625, 10900.646484375, -81.662109375], "mean_td_error": 48.948638916015625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 70000, "num_agent_steps_sampled": 70000, "num_steps_trained": 552032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 552032, "last_target_update_ts": 70000, "num_target_updates": 139}, "done": false, "episodes_total": 233, "training_iteration": 70, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-53-06", "timestamp": 1652705586, "time_this_iter_s": 14.695182085037231, "time_total_s": 982.3178396224976, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 982.3178396224976, "timesteps_since_restore": 2240, "iterations_since_restore": 70, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.58095238095238, "ram_util_percent": 18.61904761904762}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 39088.44, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [32060.0, 41528.0, 34492.0, 33172.0, 37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2247430930349574, "mean_inference_ms": 1.944250297240621, "mean_action_processing_ms": 0.06465874735433425, "mean_env_wait_ms": 2.428144672676345, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 71000, "timesteps_this_iter": 32, "agent_timesteps_total": 71000, "timers": {"load_time_ms": 0.142, "load_throughput": 225500.215, "learn_time_ms": 22.859, "learn_throughput": 1399.883}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.002116413787007332, "mean_q": 10647.1357421875, "min_q": 6662.19775390625, "max_q": 11178.8642578125, "cur_lr": 0.6}, "td_error": [-4083.255859375, -118.0712890625, -84.0712890625, -24.0712890625, -56.0712890625, -128.0712890625, 10920.8642578125, -130.0712890625, 10936.8642578125, 55.9287109375, -4121.255859375, 113.9287109375, -30.0712890625, -90.0712890625, 121.9287109375, 101.9287109375, 10934.8642578125, 119.9287109375, -4460.73779296875, -112.0712890625, 45.9287109375, 59.9287109375, -94.0712890625, 121.9287109375, -28.0712890625, 39.9287109375, 121.9287109375, -4264.3603515625, -126.0712890625, 101.9287109375, 10880.8642578125, -12.0712890625], "mean_td_error": 834.8792724609375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 71000, "num_agent_steps_sampled": 71000, "num_steps_trained": 560032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 560032, "last_target_update_ts": 71000, "num_target_updates": 141}, "done": false, "episodes_total": 236, "training_iteration": 71, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-53-21", "timestamp": 1652705601, "time_this_iter_s": 14.897833108901978, "time_total_s": 997.2156727313995, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 997.2156727313995, "timesteps_since_restore": 2272, "iterations_since_restore": 71, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.463636363636365, "ram_util_percent": 18.654545454545453}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 39290.52, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37186.0, 34476.0, 43610.0, 33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22466868684051045, "mean_inference_ms": 1.9443254363773854, "mean_action_processing_ms": 0.06465489899508305, "mean_env_wait_ms": 2.409989266116978, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 72000, "timesteps_this_iter": 32, "agent_timesteps_total": 72000, "timers": {"load_time_ms": 0.223, "load_throughput": 143425.655, "learn_time_ms": 34.98, "learn_throughput": 914.818}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0026726368814706802, "mean_q": 10804.6630859375, "min_q": 6678.33984375, "max_q": 11210.4375, "cur_lr": 0.6}, "td_error": [32.970703125, -89.029296875, -91.029296875, 26.970703125, -19.029296875, -113.029296875, 10962.4375, -109.029296875, -127.029296875, 124.970703125, -119.029296875, 64.970703125, 10964.4375, 10956.4375, -21.029296875, 58.970703125, -4213.373046875, -129.029296875, 44.970703125, -129.029296875, -4099.373046875, -101.029296875, -93.029296875, -141.029296875, -71.029296875, -99.029296875, -77.029296875, -4601.126953125, 126.970703125, 108.970703125, -103.029296875, 124.970703125], "mean_td_error": 595.3961791992188, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 72000, "num_agent_steps_sampled": 72000, "num_steps_trained": 568032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 568032, "last_target_update_ts": 72000, "num_target_updates": 143}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12988162204041082, "mean_inference_ms": 1.5495724422890618, "mean_action_processing_ms": 0.057005039123975886, "mean_env_wait_ms": 1.1784442816149678, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 240, "training_iteration": 72, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-53-37", "timestamp": 1652705617, "time_this_iter_s": 15.634986877441406, "time_total_s": 1012.8506596088409, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1012.8506596088409, "timesteps_since_restore": 2304, "iterations_since_restore": 72, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.395454545454545, "ram_util_percent": 18.67272727272728}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 39373.12, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [33368.0, 47462.0, 39960.0, 40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.224607692954166, "mean_inference_ms": 1.9442836076448737, "mean_action_processing_ms": 0.06464562798365, "mean_env_wait_ms": 2.396837203295911, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 73000, "timesteps_this_iter": 32, "agent_timesteps_total": 73000, "timers": {"load_time_ms": 0.24, "load_throughput": 133576.561, "learn_time_ms": 35.73, "learn_throughput": 895.602}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.004408881068229675, "mean_q": 10827.896484375, "min_q": 6352.6396484375, "max_q": 11244.861328125, "cur_lr": 0.6}, "td_error": [10996.861328125, -22.3662109375, -4966.587890625, -92.3662109375, 61.6337890625, -78.3662109375, -68.3662109375, -122.3662109375, -72.3662109375, 81.6337890625, -102.3662109375, 127.6337890625, -82.3662109375, 47.6337890625, -108.3662109375, -110.3662109375, -4361.6796875, -114.3662109375, 23.6337890625, 10988.861328125, 123.6337890625, 89.6337890625, 10970.861328125, 127.6337890625, -108.3662109375, -98.3662109375, -168.3662109375, -4157.6796875, -58.3662109375, -122.3662109375, -120.3662109375, 67.6337890625], "mean_td_error": 580.3473510742188, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 73000, "num_agent_steps_sampled": 73000, "num_steps_trained": 576032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 576032, "last_target_update_ts": 73000, "num_target_updates": 145}, "done": false, "episodes_total": 243, "training_iteration": 73, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-53-51", "timestamp": 1652705631, "time_this_iter_s": 13.541301012039185, "time_total_s": 1026.3919606208801, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1026.3919606208801, "timesteps_since_restore": 2336, "iterations_since_restore": 73, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.726315789473684, "ram_util_percent": 18.69473684210526}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 39199.9, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [40984.0, 41712.0, 40876.0, 41440.0, 41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22454484888750909, "mean_inference_ms": 1.9442446399260422, "mean_action_processing_ms": 0.06463685189910624, "mean_env_wait_ms": 2.384101614156742, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 74000, "timesteps_this_iter": 32, "agent_timesteps_total": 74000, "timers": {"load_time_ms": 0.26, "load_throughput": 122966.311, "learn_time_ms": 40.519, "learn_throughput": 789.759}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0014718567254021764, "mean_q": 11013.998046875, "min_q": 6826.900390625, "max_q": 11286.6015625, "cur_lr": 0.6}, "td_error": [11012.6015625, -73.0673828125, -115.0673828125, -67.0673828125, -75.0673828125, 104.9326171875, 114.9326171875, 68.9326171875, 74.9326171875, -113.0673828125, -111.0673828125, 74.9326171875, 92.9326171875, -4382.7685546875, -11.0673828125, 11036.6015625, -61.0673828125, -4362.66943359375, 132.9326171875, -69.0673828125, 78.9326171875, 102.9326171875, 52.9326171875, -65.0673828125, 94.9326171875, -105.0673828125, 134.9326171875, 104.9326171875, -77.0673828125, 120.9326171875, -101.0673828125, -131.0673828125], "mean_td_error": 421.3712158203125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 74000, "num_agent_steps_sampled": 74000, "num_steps_trained": 584032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 584032, "last_target_update_ts": 74000, "num_target_updates": 147}, "done": false, "episodes_total": 246, "training_iteration": 74, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-54-05", "timestamp": 1652705645, "time_this_iter_s": 14.879101753234863, "time_total_s": 1041.271062374115, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1041.271062374115, "timesteps_since_restore": 2368, "iterations_since_restore": 74, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 21.95909090909091, "ram_util_percent": 18.627272727272732}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 39167.48, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [41396.0, 33676.0, 43696.0, 47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22453213461056643, "mean_inference_ms": 1.9448635273030492, "mean_action_processing_ms": 0.06465099294447738, "mean_env_wait_ms": 2.3680612016524747, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 75000, "timesteps_this_iter": 32, "agent_timesteps_total": 75000, "timers": {"load_time_ms": 0.236, "load_throughput": 135820.409, "learn_time_ms": 35.789, "learn_throughput": 894.133}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0009046377381309867, "mean_q": 10783.53125, "min_q": 6844.33203125, "max_q": 11330.873046875, "cur_lr": 0.6}, "td_error": [73.94140625, -56.05859375, -52.05859375, 119.94140625, 45.94140625, 139.94140625, 111.94140625, -36.05859375, 39.94140625, 11078.873046875, -98.05859375, 11080.873046875, -8.05859375, -86.05859375, -28.05859375, -72.05859375, 83.94140625, -4371.4521484375, 141.94140625, 39.94140625, -68.05859375, -100.05859375, -60.05859375, 6588.33203125, -4182.5146484375, -86.05859375, 139.94140625, 11022.873046875, -4.05859375, 141.94140625, -4486.599609375, -68.05859375], "mean_td_error": 843.3412475585938, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 75000, "num_agent_steps_sampled": 75000, "num_steps_trained": 592032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 592032, "last_target_update_ts": 75000, "num_target_updates": 149}, "done": false, "episodes_total": 250, "training_iteration": 75, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-54-22", "timestamp": 1652705662, "time_this_iter_s": 15.994048595428467, "time_total_s": 1057.2651109695435, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1057.2651109695435, "timesteps_since_restore": 2400, "iterations_since_restore": 75, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.5, "ram_util_percent": 18.65652173913043}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 39089.92, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [47146.0, 41278.0, 39188.0, 36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2245191138061959, "mean_inference_ms": 1.945320761050163, "mean_action_processing_ms": 0.06466150543022046, "mean_env_wait_ms": 2.356409743741137, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 76000, "timesteps_this_iter": 32, "agent_timesteps_total": 76000, "timers": {"load_time_ms": 0.236, "load_throughput": 135683.106, "learn_time_ms": 36.59, "learn_throughput": 874.567}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.006137483287602663, "mean_q": 10818.185546875, "min_q": 6751.74365234375, "max_q": 11370.4970703125, "cur_lr": 0.6}, "td_error": [102.0400390625, -4246.9345703125, -95.9599609375, -121.9599609375, -65.9599609375, 32.0400390625, -15.9599609375, -71.9599609375, -77.9599609375, -4552.71337890625, -113.9599609375, -45.9599609375, -117.9599609375, -91.9599609375, -135.9599609375, -99.9599609375, 70.0400390625, -117.9599609375, 11120.4970703125, -59.9599609375, 116.0400390625, -67.9599609375, -4573.9521484375, 102.0400390625, 132.0400390625, -101.9599609375, -15.9599609375, 108.0400390625, -61.9599609375, 130.0400390625, -4428.2041015625, -53.9599609375], "mean_td_error": -232.007080078125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 76000, "num_agent_steps_sampled": 76000, "num_steps_trained": 600032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 600032, "last_target_update_ts": 76000, "num_target_updates": 151}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13053461117318715, "mean_inference_ms": 1.5620570959824887, "mean_action_processing_ms": 0.05744984433893948, "mean_env_wait_ms": 1.1850394945022538, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 253, "training_iteration": 76, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-54-36", "timestamp": 1652705676, "time_this_iter_s": 14.869449138641357, "time_total_s": 1072.1345601081848, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1072.1345601081848, "timesteps_since_restore": 2432, "iterations_since_restore": 76, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.747619047619047, "ram_util_percent": 18.690476190476186}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 38884.76, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36468.0, 35078.0, 40384.0, 37932.0, 43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2245207739025236, "mean_inference_ms": 1.9458934542037227, "mean_action_processing_ms": 0.06467559259568212, "mean_env_wait_ms": 2.3452482325614503, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 77000, "timesteps_this_iter": 32, "agent_timesteps_total": 77000, "timers": {"load_time_ms": 0.231, "load_throughput": 138368.792, "learn_time_ms": 37.874, "learn_throughput": 844.9}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.002836326602846384, "mean_q": 10721.06640625, "min_q": 6435.56640625, "max_q": 11405.9755859375, "cur_lr": 0.6}, "td_error": [45.1357421875, -82.8642578125, -4140.7626953125, -3747.68994140625, -4591.17822265625, -146.8642578125, -104.8642578125, 51.1357421875, 65.1357421875, -70.8642578125, 127.1357421875, -18.8642578125, 125.1357421875, 127.1357421875, -116.8642578125, 27.1357421875, 93.1357421875, -108.8642578125, -5099.2734375, 57.1357421875, 113.1357421875, -74.8642578125, -92.8642578125, -68.8642578125, 127.1357421875, 127.1357421875, 25.1357421875, 101.1357421875, 39.1357421875, -4402.5244140625, -132.8642578125, -110.8642578125], "mean_td_error": -683.1488647460938, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 77000, "num_agent_steps_sampled": 77000, "num_steps_trained": 608032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 608032, "last_target_update_ts": 77000, "num_target_updates": 153}, "done": false, "episodes_total": 256, "training_iteration": 77, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-54-52", "timestamp": 1652705692, "time_this_iter_s": 15.402780294418335, "time_total_s": 1087.5373404026031, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1087.5373404026031, "timesteps_since_restore": 2464, "iterations_since_restore": 77, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.44545454545455, "ram_util_percent": 18.650000000000002}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 39020.54, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [43756.0, 41898.0, 29714.0, 46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2245067926982383, "mean_inference_ms": 1.9464937125849189, "mean_action_processing_ms": 0.0646872602586836, "mean_env_wait_ms": 2.3308569997910387, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 78000, "timesteps_this_iter": 32, "agent_timesteps_total": 78000, "timers": {"load_time_ms": 0.232, "load_throughput": 137758.111, "learn_time_ms": 35.139, "learn_throughput": 910.664}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.004177195951342583, "mean_q": 10905.494140625, "min_q": 6459.31494140625, "max_q": 11448.2265625, "cur_lr": 0.6}, "td_error": [-135.0791015625, 46.9208984375, 44.9208984375, 144.9208984375, 144.9208984375, 132.9208984375, 144.9208984375, -63.0791015625, -107.0791015625, 11206.2265625, -4240.06884765625, -3732.9560546875, -55.0791015625, -129.0791015625, 28.9208984375, 90.9208984375, 11192.2265625, -101.0791015625, 144.9208984375, -95.0791015625, 64.9208984375, -55.0791015625, 124.9208984375, -4945.99072265625, 114.9208984375, 11200.2265625, -4310.74072265625, -65.0791015625, 78.9208984375, 80.9208984375, 142.9208984375, -57.0791015625], "mean_td_error": 532.435791015625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 78000, "num_agent_steps_sampled": 78000, "num_steps_trained": 616032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 616032, "last_target_update_ts": 78000, "num_target_updates": 155}, "done": false, "episodes_total": 260, "training_iteration": 78, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-55-07", "timestamp": 1652705707, "time_this_iter_s": 14.75771689414978, "time_total_s": 1102.295057296753, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1102.295057296753, "timesteps_since_restore": 2496, "iterations_since_restore": 78, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.323809523809523, "ram_util_percent": 18.661904761904758}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 39034.6, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [46508.0, 42974.0, 35944.0, 44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2245243910814948, "mean_inference_ms": 1.947220861546645, "mean_action_processing_ms": 0.06470509155237654, "mean_env_wait_ms": 2.3206280631257745, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 79000, "timesteps_this_iter": 32, "agent_timesteps_total": 79000, "timers": {"load_time_ms": 0.171, "load_throughput": 187089.111, "learn_time_ms": 26.733, "learn_throughput": 1197.003}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0015713487518951297, "mean_q": 10804.1298828125, "min_q": 6484.1005859375, "max_q": 11494.5703125, "cur_lr": 0.6}, "td_error": [137.6376953125, -4768.2236328125, 137.6376953125, -72.3623046875, 11238.5703125, 11250.5703125, -4276.27099609375, 129.6376953125, -98.3623046875, -3903.099609375, 73.6376953125, -4255.49169921875, -4.3623046875, -90.3623046875, -60.3623046875, -110.3623046875, 11246.5703125, 129.6376953125, -4.3623046875, 137.6376953125, 137.6376953125, 71.6376953125, 11240.5703125, -96.3623046875, -5102.83203125, 111.6376953125, -58.3623046875, 11232.5703125, -112.3623046875, -96.3623046875, 69.6376953125, 11238.5703125], "mean_td_error": 1421.0592041015625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 79000, "num_agent_steps_sampled": 79000, "num_steps_trained": 624032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 624032, "last_target_update_ts": 79000, "num_target_updates": 157}, "done": false, "episodes_total": 263, "training_iteration": 79, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-55-22", "timestamp": 1652705722, "time_this_iter_s": 14.800184726715088, "time_total_s": 1117.095242023468, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1117.095242023468, "timesteps_since_restore": 2528, "iterations_since_restore": 79, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.82857142857143, "ram_util_percent": 18.671428571428574}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 39020.64, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [44174.0, 37688.0, 33198.0, 46978.0, 36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22452895241028578, "mean_inference_ms": 1.9478625795949682, "mean_action_processing_ms": 0.06471967984657699, "mean_env_wait_ms": 2.310631686445084, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 80000, "timesteps_this_iter": 32, "agent_timesteps_total": 80000, "timers": {"load_time_ms": 0.168, "load_throughput": 190244.831, "learn_time_ms": 24.906, "learn_throughput": 1284.834}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0011298978934064507, "mean_q": 11008.185546875, "min_q": 6977.0224609375, "max_q": 11544.6640625, "cur_lr": 0.6}, "td_error": [-90.4306640625, -88.4306640625, 121.5693359375, -154.4306640625, -8.4306640625, 43.5693359375, 119.5693359375, -4274.89990234375, 141.5693359375, 141.5693359375, 11264.6640625, -80.4306640625, -88.4306640625, -104.4306640625, 11.5693359375, -4322.90576171875, 141.5693359375, 137.5693359375, 123.5693359375, 141.5693359375, -4636.072265625, -58.4306640625, -120.4306640625, -3981.16796875, -96.4306640625, 115.5693359375, 129.5693359375, -22.4306640625, -68.4306640625, 71.5693359375, -66.4306640625, 139.5693359375], "mean_td_error": -169.31280517578125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 80000, "num_agent_steps_sampled": 80000, "num_steps_trained": 632032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 632032, "last_target_update_ts": 80000, "num_target_updates": 159}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.1298353048666897, "mean_inference_ms": 1.5543195689207236, "mean_action_processing_ms": 0.05713686746788947, "mean_env_wait_ms": 1.1832438355941532, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 266, "training_iteration": 80, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-55-36", "timestamp": 1652705736, "time_this_iter_s": 14.570823907852173, "time_total_s": 1131.6660659313202, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1131.6660659313202, "timesteps_since_restore": 2560, "iterations_since_restore": 80, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.71904761904762, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 38879.86, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36206.0, 39500.0, 35388.0, 36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22456559550452113, "mean_inference_ms": 1.948981111383872, "mean_action_processing_ms": 0.06474698714873112, "mean_env_wait_ms": 2.2978813475849402, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 81000, "timesteps_this_iter": 32, "agent_timesteps_total": 81000, "timers": {"load_time_ms": 0.226, "load_throughput": 141684.501, "learn_time_ms": 35.591, "learn_throughput": 899.098}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.00040699492092244327, "mean_q": 11335.5947265625, "min_q": 7251.3515625, "max_q": 11594.4521484375, "cur_lr": 0.6}, "td_error": [-107.73828125, -119.73828125, 142.26171875, -79.73828125, 68.26171875, -37.73828125, -105.73828125, -105.73828125, 42.26171875, -101.73828125, -4000.07666015625, 102.26171875, -101.73828125, -1.73828125, -57.73828125, -89.73828125, 38.26171875, -77.73828125, -109.73828125, -113.73828125, -4272.8388671875, 11316.4521484375, 142.26171875, 11348.4521484375, 48.26171875, 14.26171875, 142.26171875, -103.73828125, 138.26171875, 142.26171875, -93.73828125, 142.26171875], "mean_td_error": 442.1036376953125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 81000, "num_agent_steps_sampled": 81000, "num_steps_trained": 640032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 640032, "last_target_update_ts": 81000, "num_target_updates": 161}, "done": false, "episodes_total": 270, "training_iteration": 81, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-55-52", "timestamp": 1652705752, "time_this_iter_s": 15.631158590316772, "time_total_s": 1147.297224521637, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1147.297224521637, "timesteps_since_restore": 2592, "iterations_since_restore": 81, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.522727272727277, "ram_util_percent": 18.69545454545454}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 38872.3, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36556.0, 35940.0, 42192.0, 46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22459625325413213, "mean_inference_ms": 1.9498064299704925, "mean_action_processing_ms": 0.06476784779336561, "mean_env_wait_ms": 2.2886773263908227, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 82000, "timesteps_this_iter": 32, "agent_timesteps_total": 82000, "timers": {"load_time_ms": 0.14, "load_throughput": 229157.808, "learn_time_ms": 19.86, "learn_throughput": 1611.255}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0051984163001179695, "mean_q": 10783.96484375, "min_q": 6905.79345703125, "max_q": 11630.40234375, "cur_lr": 0.6}, "td_error": [77.58203125, -4.41796875, 145.58203125, 145.58203125, -60.41796875, -92.41796875, 73.58203125, 45.58203125, 143.58203125, -4811.02685546875, -84.41796875, -4665.02685546875, 127.58203125, -4336.62109375, -56.41796875, 143.58203125, -54.41796875, 143.58203125, 11382.40234375, 3.58203125, 107.58203125, 145.58203125, -4638.0322265625, 145.58203125, -4329.16455078125, -102.41796875, -104.41796875, 67.58203125, 145.58203125, -4290.66064453125, -100.41796875, 41.58203125], "mean_td_error": -457.6430969238281, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 82000, "num_agent_steps_sampled": 82000, "num_steps_trained": 648032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 648032, "last_target_update_ts": 82000, "num_target_updates": 163}, "done": false, "episodes_total": 273, "training_iteration": 82, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-56-06", "timestamp": 1652705766, "time_this_iter_s": 13.583313703536987, "time_total_s": 1160.880538225174, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1160.880538225174, "timesteps_since_restore": 2624, "iterations_since_restore": 82, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.645000000000003, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29520.0, "episode_reward_mean": 38900.06, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [46822.0, 39318.0, 37500.0, 34530.0, 38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22464110912103522, "mean_inference_ms": 1.9507662939630293, "mean_action_processing_ms": 0.06479267087087667, "mean_env_wait_ms": 2.279799680541331, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 83000, "timesteps_this_iter": 32, "agent_timesteps_total": 83000, "timers": {"load_time_ms": 0.188, "load_throughput": 169766.921, "learn_time_ms": 29.268, "learn_throughput": 1093.363}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.003067612648010254, "mean_q": 10452.3369140625, "min_q": 7070.85009765625, "max_q": 11681.064453125, "cur_lr": 0.6}, "td_error": [-59.119140625, 80.880859375, -4204.89892578125, -4402.6591796875, 4.880859375, -91.119140625, -4627.33349609375, 124.880859375, 11413.064453125, -4303.08740234375, -4238.89892578125, -4462.6591796875, -95.119140625, 8.880859375, -99.119140625, -135.119140625, -111.119140625, -69.119140625, 78.880859375, -53.119140625, 16.880859375, 2.880859375, -109.119140625, 11427.064453125, -3819.5576171875, -4529.33349609375, -111.119140625, -121.119140625, 144.880859375, 78.880859375, -4338.89892578125, -97.119140625], "mean_td_error": -521.7719116210938, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 83000, "num_agent_steps_sampled": 83000, "num_steps_trained": 656032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 656032, "last_target_update_ts": 83000, "num_target_updates": 165}, "done": false, "episodes_total": 276, "training_iteration": 83, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-56-20", "timestamp": 1652705780, "time_this_iter_s": 14.456508159637451, "time_total_s": 1175.3370463848114, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1175.3370463848114, "timesteps_since_restore": 2656, "iterations_since_restore": 83, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.095000000000002, "ram_util_percent": 18.695}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38720.64, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [38882.0, 43020.0, 40030.0, 36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22468744534964472, "mean_inference_ms": 1.9519137548523589, "mean_action_processing_ms": 0.06482134110883594, "mean_env_wait_ms": 2.268214324934048, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 84000, "timesteps_this_iter": 32, "agent_timesteps_total": 84000, "timers": {"load_time_ms": 0.226, "load_throughput": 141879.205, "learn_time_ms": 35.121, "learn_throughput": 911.142}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.005746497772634029, "mean_q": 11193.7861328125, "min_q": 7358.1640625, "max_q": 11726.662109375, "cur_lr": 0.6}, "td_error": [-79.2109375, -95.2109375, -11.2109375, -95.2109375, 11458.662109375, -97.2109375, -4439.708984375, -4230.23828125, -4085.2333984375, 50.7890625, 60.7890625, 116.7890625, -61.2109375, -115.2109375, 40.7890625, -117.2109375, 34.7890625, 134.7890625, -99.2109375, 64.7890625, 22.7890625, 4.7890625, 58.7890625, -115.2109375, 134.7890625, 74.7890625, 132.7890625, -4273.708984375, 134.7890625, 52.7890625, -23.2109375, 11474.662109375], "mean_td_error": 191.09219360351562, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 84000, "num_agent_steps_sampled": 84000, "num_steps_trained": 664032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 664032, "last_target_update_ts": 84000, "num_target_updates": 167}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13071679819237522, "mean_inference_ms": 1.5712650479484866, "mean_action_processing_ms": 0.05767765811011444, "mean_env_wait_ms": 1.1941373698239555, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 280, "training_iteration": 84, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-56-36", "timestamp": 1652705796, "time_this_iter_s": 15.932662010192871, "time_total_s": 1191.2697083950043, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1191.2697083950043, "timesteps_since_restore": 2688, "iterations_since_restore": 84, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.56086956521739, "ram_util_percent": 18.691304347826087}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38686.02, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36190.0, 33622.0, 41608.0, 36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2247182485054921, "mean_inference_ms": 1.9527453790630032, "mean_action_processing_ms": 0.06484204918204499, "mean_env_wait_ms": 2.259833861193255, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 85000, "timesteps_this_iter": 32, "agent_timesteps_total": 85000, "timers": {"load_time_ms": 0.126, "load_throughput": 253336.595, "learn_time_ms": 19.646, "learn_throughput": 1628.793}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0028910390101373196, "mean_q": 10924.2314453125, "min_q": 7014.625, "max_q": 11761.779296875, "cur_lr": 0.6}, "td_error": [-117.0546875, -4756.208984375, 120.9453125, 76.9453125, -13.0546875, -4346.9609375, -113.0546875, 134.9453125, 120.9453125, 134.9453125, -63.0546875, 68.9453125, 118.9453125, 11471.779296875, 132.9453125, -65.0546875, 130.9453125, -3836.6865234375, -129.0546875, -4852.208984375, -119.0546875, -11.0546875, 134.9453125, -95.0546875, 78.9453125, -73.0546875, -121.0546875, -4594.1416015625, -117.0546875, -4479.68017578125, -137.0546875, 70.9453125], "mean_td_error": -476.32733154296875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 85000, "num_agent_steps_sampled": 85000, "num_steps_trained": 672032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 672032, "last_target_update_ts": 85000, "num_target_updates": 169}, "done": false, "episodes_total": 283, "training_iteration": 85, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-56-49", "timestamp": 1652705809, "time_this_iter_s": 13.189650774002075, "time_total_s": 1204.4593591690063, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1204.4593591690063, "timesteps_since_restore": 2720, "iterations_since_restore": 85, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.810526315789474, "ram_util_percent": 18.7}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38699.72, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36278.0, 40968.0, 38426.0, 40064.0, 34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22473213272922637, "mean_inference_ms": 1.953398135668084, "mean_action_processing_ms": 0.06485675148685435, "mean_env_wait_ms": 2.251678917196308, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 86000, "timesteps_this_iter": 32, "agent_timesteps_total": 86000, "timers": {"load_time_ms": 0.131, "load_throughput": 245146.535, "learn_time_ms": 20.155, "learn_throughput": 1587.697}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.004798485431820154, "mean_q": 11083.15234375, "min_q": 6694.7177734375, "max_q": 11799.2421875, "cur_lr": 0.6}, "td_error": [75.435546875, 135.435546875, -0.564453125, -102.564453125, -110.564453125, -4234.7841796875, -38.564453125, -138.564453125, 71.435546875, 65.435546875, 65.435546875, -94.564453125, 67.435546875, -78.564453125, 29.435546875, 133.435546875, 37.435546875, 65.435546875, -114.564453125, 11543.2421875, 71.435546875, -70.564453125, -3979.94091796875, 57.435546875, 135.435546875, -5095.0888671875, 71.435546875, 135.435546875, -5207.0888671875, -108.564453125, 71.435546875, -4442.7841796875], "mean_td_error": -343.2850341796875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 86000, "num_agent_steps_sampled": 86000, "num_steps_trained": 680032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 680032, "last_target_update_ts": 86000, "num_target_updates": 171}, "done": false, "episodes_total": 286, "training_iteration": 86, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-56-59", "timestamp": 1652705819, "time_this_iter_s": 9.720551252365112, "time_total_s": 1214.1799104213715, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1214.1799104213715, "timesteps_since_restore": 2752, "iterations_since_restore": 86, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.857142857142854, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38851.18, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [34094.0, 37988.0, 43782.0, 38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2247210257907939, "mean_inference_ms": 1.953967788554002, "mean_action_processing_ms": 0.06486670750805089, "mean_env_wait_ms": 2.2410092135154036, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 87000, "timesteps_this_iter": 32, "agent_timesteps_total": 87000, "timers": {"load_time_ms": 0.128, "load_throughput": 250266.135, "learn_time_ms": 20.113, "learn_throughput": 1590.993}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.004228077828884125, "mean_q": 11439.587890625, "min_q": 7477.7041015625, "max_q": 11848.01953125, "cur_lr": 0.6}, "td_error": [-63.5947265625, 34.4052734375, -89.5947265625, 112.4052734375, 144.4052734375, -89.5947265625, 11592.01953125, -79.5947265625, -3.5947265625, -61.5947265625, -67.5947265625, -69.5947265625, -95.5947265625, -4441.91015625, 124.4052734375, 144.4052734375, 78.4052734375, -57.5947265625, 96.4052734375, -85.5947265625, 50.4052734375, 142.4052734375, -4408.80810546875, -75.5947265625, 126.4052734375, 144.4052734375, -85.5947265625, -85.5947265625, -87.5947265625, -4355.91015625, 144.4052734375, -23.5947265625], "mean_td_error": -43.53941345214844, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 87000, "num_agent_steps_sampled": 87000, "num_steps_trained": 688032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 688032, "last_target_update_ts": 87000, "num_target_updates": 173}, "done": false, "episodes_total": 290, "training_iteration": 87, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-57-09", "timestamp": 1652705829, "time_this_iter_s": 9.569579601287842, "time_total_s": 1223.7494900226593, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1223.7494900226593, "timesteps_since_restore": 2784, "iterations_since_restore": 87, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.128571428571426, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38893.26, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [38896.0, 36482.0, 41392.0, 44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22472307107245582, "mean_inference_ms": 1.954488510962662, "mean_action_processing_ms": 0.06487769317994116, "mean_env_wait_ms": 2.2332890427305507, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 88000, "timesteps_this_iter": 32, "agent_timesteps_total": 88000, "timers": {"load_time_ms": 0.269, "load_throughput": 118860.9, "learn_time_ms": 42.549, "learn_throughput": 752.073}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.002685191109776497, "mean_q": 11600.5693359375, "min_q": 6776.04443359375, "max_q": 11884.529296875, "cur_lr": 0.6}, "td_error": [136.1201171875, -4072.14501953125, -115.8798828125, 136.1201171875, 66.1201171875, -113.8798828125, 136.1201171875, 78.1201171875, 11606.529296875, -13.8798828125, 136.1201171875, 11604.529296875, -115.8798828125, 98.1201171875, 48.1201171875, -105.8798828125, 128.1201171875, -5074.36474609375, 118.1201171875, -13.8798828125, 98.1201171875, 130.1201171875, 136.1201171875, 11640.529296875, -111.8798828125, -99.8798828125, 134.1201171875, -141.8798828125, -83.8798828125, 76.1201171875, 52.1201171875, -103.8798828125], "mean_td_error": 824.760009765625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 88000, "num_agent_steps_sampled": 88000, "num_steps_trained": 696032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 696032, "last_target_update_ts": 88000, "num_target_updates": 175}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13308856653780707, "mean_inference_ms": 1.603675889094803, "mean_action_processing_ms": 0.058967332878973856, "mean_env_wait_ms": 1.2126044782070189, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 293, "training_iteration": 88, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-57-21", "timestamp": 1652705841, "time_this_iter_s": 12.644419431686401, "time_total_s": 1236.3939094543457, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1236.3939094543457, "timesteps_since_restore": 2816, "iterations_since_restore": 88, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 24.172222222222224, "ram_util_percent": 18.7}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29474.0, "episode_reward_mean": 39007.4, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [44052.0, 42624.0, 35542.0, 39756.0, 37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22473046842678418, "mean_inference_ms": 1.9550364578893202, "mean_action_processing_ms": 0.0648886476197646, "mean_env_wait_ms": 2.225753323547916, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 89000, "timesteps_this_iter": 32, "agent_timesteps_total": 89000, "timers": {"load_time_ms": 0.26, "load_throughput": 123033.943, "learn_time_ms": 37.327, "learn_throughput": 857.283}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.001092504127882421, "mean_q": 11354.5, "min_q": 7164.09716796875, "max_q": 11924.4560546875, "cur_lr": 0.6}, "td_error": [-4713.78955078125, 70.5693359375, -69.4306640625, 138.5693359375, -4753.78955078125, 122.5693359375, 80.5693359375, -57.4306640625, -117.4306640625, 140.5693359375, -139.4306640625, -51.4306640625, -111.4306640625, -107.4306640625, -61.4306640625, -87.4306640625, 140.5693359375, 138.5693359375, -4371.58154296875, 11668.4560546875, -137.4306640625, 16.5693359375, 138.5693359375, -105.4306640625, 98.5693359375, -81.4306640625, -4273.123046875, 11668.4560546875, 0.5693359375, -63.4306640625, 11676.4560546875, 80.5693359375], "mean_td_error": 527.4161376953125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 89000, "num_agent_steps_sampled": 89000, "num_steps_trained": 704032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 704032, "last_target_update_ts": 89000, "num_target_updates": 177}, "done": false, "episodes_total": 296, "training_iteration": 89, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-57-37", "timestamp": 1652705857, "time_this_iter_s": 15.582026243209839, "time_total_s": 1251.9759356975555, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1251.9759356975555, "timesteps_since_restore": 2848, "iterations_since_restore": 89, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.086363636363632, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38788.48, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37564.0, 29520.0, 39008.0, 37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2246982878747054, "mean_inference_ms": 1.9554184538952746, "mean_action_processing_ms": 0.0648922739618796, "mean_env_wait_ms": 2.215639149746731, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 90000, "timesteps_this_iter": 32, "agent_timesteps_total": 90000, "timers": {"load_time_ms": 0.131, "load_throughput": 244610.403, "learn_time_ms": 20.297, "learn_throughput": 1576.586}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0038025910034775734, "mean_q": 11004.2412109375, "min_q": 6851.919921875, "max_q": 11968.11328125, "cur_lr": 0.6}, "td_error": [84.86328125, -4991.330078125, -3839.369140625, -113.13671875, 134.86328125, -63.13671875, -119.13671875, -83.13671875, 142.86328125, -4361.3974609375, -4451.6640625, -77.13671875, -109.13671875, -67.13671875, -121.13671875, -4207.3974609375, -11.13671875, 82.86328125, 82.86328125, -4219.0439453125, 42.86328125, 94.86328125, -4471.6640625, -109.13671875, 11710.11328125, 142.86328125, 118.86328125, 134.86328125, 11716.11328125, -75.13671875, -107.13671875, -63.13671875], "mean_td_error": -224.11825561523438, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 90000, "num_agent_steps_sampled": 90000, "num_steps_trained": 712032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 712032, "last_target_update_ts": 90000, "num_target_updates": 179}, "done": false, "episodes_total": 300, "training_iteration": 90, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-57-50", "timestamp": 1652705870, "time_this_iter_s": 12.419390678405762, "time_total_s": 1264.3953263759613, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1264.3953263759613, "timesteps_since_restore": 2880, "iterations_since_restore": 90, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.372222222222224, "ram_util_percent": 18.7}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38974.7, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37444.0, 32222.0, 37312.0, 41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22468253114452424, "mean_inference_ms": 1.9557559051028703, "mean_action_processing_ms": 0.06489667425592245, "mean_env_wait_ms": 2.20837112220272, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 91000, "timesteps_this_iter": 32, "agent_timesteps_total": 91000, "timers": {"load_time_ms": 0.231, "load_throughput": 138798.064, "learn_time_ms": 34.917, "learn_throughput": 916.47}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0012129417154937983, "mean_q": 11297.4228515625, "min_q": 6894.4873046875, "max_q": 12001.6328125, "cur_lr": 0.6}, "td_error": [-59.1796875, 76.8203125, -71.1796875, -3982.1162109375, -127.1796875, -73.1796875, -4221.7509765625, -85.1796875, -123.1796875, 70.8203125, 68.8203125, -49.1796875, -3910.1162109375, 96.8203125, -73.1796875, -4996.3251953125, 11745.6328125, 136.8203125, 112.8203125, 11745.6328125, -71.1796875, 11731.6328125, 11751.6328125, 60.8203125, 136.8203125, 136.8203125, -5026.3251953125, 84.8203125, 136.8203125, -111.1796875, 11745.6328125, 11757.6328125], "mean_td_error": 1519.26220703125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 91000, "num_agent_steps_sampled": 91000, "num_steps_trained": 720032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 720032, "last_target_update_ts": 91000, "num_target_updates": 181}, "done": false, "episodes_total": 303, "training_iteration": 91, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-58-04", "timestamp": 1652705884, "time_this_iter_s": 14.640376567840576, "time_total_s": 1279.0357029438019, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1279.0357029438019, "timesteps_since_restore": 2912, "iterations_since_restore": 91, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.204761904761906, "ram_util_percent": 18.69047619047619}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29474.0, "episode_reward_mean": 39029.96, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [41396.0, 38042.0, 35782.0, 32738.0, 37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22466335204608062, "mean_inference_ms": 1.9560446286834017, "mean_action_processing_ms": 0.06489926633943674, "mean_env_wait_ms": 2.20136406818736, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 92000, "timesteps_this_iter": 32, "agent_timesteps_total": 92000, "timers": {"load_time_ms": 0.23, "load_throughput": 139360.116, "learn_time_ms": 34.582, "learn_throughput": 925.338}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0015910518122836947, "mean_q": 11189.5810546875, "min_q": 7287.650390625, "max_q": 12023.552734375, "cur_lr": 0.6}, "td_error": [63.1162109375, 11747.552734375, 77.1162109375, 137.1162109375, -4656.7861328125, 137.1162109375, -4191.828125, -38.8837890625, -108.8837890625, 113.1162109375, 117.1162109375, -4365.828125, -2.8837890625, -116.8837890625, -4277.333984375, -104.8837890625, -96.8837890625, -74.8837890625, 11765.552734375, 125.1162109375, -84.8837890625, -38.8837890625, 33.1162109375, -30.8837890625, -4425.828125, -112.8837890625, -70.8837890625, -98.8837890625, 73.1162109375, -4642.7861328125, 135.1162109375, 67.1162109375], "mean_td_error": -92.20297241210938, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 92000, "num_agent_steps_sampled": 92000, "num_steps_trained": 728032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 728032, "last_target_update_ts": 92000, "num_target_updates": 183}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13384423796948514, "mean_inference_ms": 1.6179700775986703, "mean_action_processing_ms": 0.059386886145615644, "mean_env_wait_ms": 1.223138235147717, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 306, "training_iteration": 92, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-58-21", "timestamp": 1652705901, "time_this_iter_s": 16.322700023651123, "time_total_s": 1295.358402967453, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1295.358402967453, "timesteps_since_restore": 2944, "iterations_since_restore": 92, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.82608695652174, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29474.0, "episode_reward_mean": 39026.12, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37882.0, 36526.0, 40256.0, 45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2246704333776198, "mean_inference_ms": 1.9567124544456949, "mean_action_processing_ms": 0.06491196392085205, "mean_env_wait_ms": 2.1924979374203373, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 93000, "timesteps_this_iter": 32, "agent_timesteps_total": 93000, "timers": {"load_time_ms": 0.236, "load_throughput": 135532.392, "learn_time_ms": 34.852, "learn_throughput": 918.169}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.003904742421582341, "mean_q": 11472.345703125, "min_q": 6978.689453125, "max_q": 12057.587890625, "cur_lr": 0.6}, "td_error": [131.9951171875, 39.9951171875, 99.9951171875, 133.9951171875, 11795.587890625, 77.9951171875, 59.9951171875, -72.0048828125, 65.9951171875, -82.0048828125, -102.0048828125, -116.0048828125, 11809.587890625, 39.9951171875, -56.0048828125, -4170.009765625, -128.0048828125, -4231.951171875, 11783.587890625, -72.0048828125, -4996.9033203125, -58.0048828125, 133.9951171875, 133.9951171875, 79.9951171875, -10.0048828125, 49.9951171875, 125.9951171875, -62.0048828125, 37.9951171875, -78.0048828125, -4954.9033203125], "mean_td_error": 544.08984375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 93000, "num_agent_steps_sampled": 93000, "num_steps_trained": 736032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 736032, "last_target_update_ts": 93000, "num_target_updates": 185}, "done": false, "episodes_total": 310, "training_iteration": 93, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-58-37", "timestamp": 1652705917, "time_this_iter_s": 16.04323434829712, "time_total_s": 1311.4016373157501, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1311.4016373157501, "timesteps_since_restore": 2976, "iterations_since_restore": 93, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.221739130434777, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38990.56, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [45032.0, 34120.0, 35942.0, 38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22470191238960296, "mean_inference_ms": 1.9574438509313867, "mean_action_processing_ms": 0.06492959788516271, "mean_env_wait_ms": 2.1861630883614915, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 94000, "timesteps_this_iter": 32, "agent_timesteps_total": 94000, "timers": {"load_time_ms": 0.228, "load_throughput": 140292.388, "learn_time_ms": 36.569, "learn_throughput": 875.051}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.001568320789374411, "mean_q": 11270.2080078125, "min_q": 7367.6455078125, "max_q": 12100.3994140625, "cur_lr": 0.6}, "td_error": [144.203125, 86.203125, -4411.87158203125, -97.796875, -83.796875, -107.796875, -109.796875, -115.796875, 144.203125, -4734.369140625, 0.203125, -11.796875, 78.203125, -45.796875, -107.796875, 130.203125, 11850.3994140625, 144.203125, 74.203125, 144.203125, -4654.55078125, -99.796875, -4344.93603515625, -107.796875, -83.796875, -4323.87158203125, 142.203125, -57.796875, 54.203125, 86.203125, 84.203125, -4327.29833984375], "mean_td_error": -458.23187255859375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 94000, "num_agent_steps_sampled": 94000, "num_steps_trained": 744032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 744032, "last_target_update_ts": 94000, "num_target_updates": 187}, "done": false, "episodes_total": 313, "training_iteration": 94, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-58-52", "timestamp": 1652705932, "time_this_iter_s": 15.217610120773315, "time_total_s": 1326.6192474365234, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1326.6192474365234, "timesteps_since_restore": 3008, "iterations_since_restore": 94, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.581818181818186, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 49596.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38902.42, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [38798.0, 49596.0, 36506.0, 33544.0, 35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22474558113096396, "mean_inference_ms": 1.958290252728696, "mean_action_processing_ms": 0.06495155173998209, "mean_env_wait_ms": 2.18006226207791, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 95000, "timesteps_this_iter": 32, "agent_timesteps_total": 95000, "timers": {"load_time_ms": 0.226, "load_throughput": 141744.353, "learn_time_ms": 35.159, "learn_throughput": 910.146}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.004121819976717234, "mean_q": 11299.314453125, "min_q": 7055.35302734375, "max_q": 12133.6318359375, "cur_lr": 0.6}, "td_error": [-31.1376953125, -113.1376953125, 134.8623046875, 92.8623046875, -17.1376953125, 60.8623046875, 132.8623046875, -21.1376953125, 50.8623046875, -4176.36767578125, -1.1376953125, 24.8623046875, 134.8623046875, -4308.23828125, 68.8623046875, -101.1376953125, 134.8623046875, -5013.41650390625, -133.1376953125, -4404.23828125, -91.1376953125, 32.8623046875, 66.8623046875, 134.8623046875, -107.1376953125, 102.8623046875, 24.8623046875, 132.8623046875, -4408.36767578125, -75.1376953125, -57.1376953125, -4424.36767578125], "mean_td_error": -817.26806640625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 95000, "num_agent_steps_sampled": 95000, "num_steps_trained": 752032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 752032, "last_target_update_ts": 95000, "num_target_updates": 189}, "done": false, "episodes_total": 316, "training_iteration": 95, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-59-07", "timestamp": 1652705947, "time_this_iter_s": 14.786122798919678, "time_total_s": 1341.4053702354431, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1341.4053702354431, "timesteps_since_restore": 3040, "iterations_since_restore": 95, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.099999999999998, "ram_util_percent": 18.695238095238093}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38835.9, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [35768.0, 40974.0, 45016.0, 42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22479253293551935, "mean_inference_ms": 1.9592810847824205, "mean_action_processing_ms": 0.06497531945197492, "mean_env_wait_ms": 2.172095239016772, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 96000, "timesteps_this_iter": 32, "agent_timesteps_total": 96000, "timers": {"load_time_ms": 0.26, "load_throughput": 123271.242, "learn_time_ms": 36.567, "learn_throughput": 875.114}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0013971894513815641, "mean_q": 11395.048828125, "min_q": 7096.0, "max_q": 12159.6748046875, "cur_lr": 0.6}, "td_error": [-126.9443359375, 115.0556640625, -90.9443359375, 105.0556640625, 11893.6748046875, 129.0556640625, -118.9443359375, 131.0556640625, 97.0556640625, 67.0556640625, -5122.619140625, -88.9443359375, -5068.619140625, -88.9443359375, -116.9443359375, 65.0556640625, 67.0556640625, 11881.6748046875, -16.9443359375, 67.0556640625, -68.9443359375, 119.0556640625, 115.0556640625, -102.9443359375, -4278.27587890625, 109.0556640625, -5006.619140625, -14.9443359375, -74.9443359375, -104.9443359375, -58.9443359375, -5166.619140625], "mean_td_error": -23.594100952148438, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 96000, "num_agent_steps_sampled": 96000, "num_steps_trained": 760032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 760032, "last_target_update_ts": 96000, "num_target_updates": 191}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.134245022653358, "mean_inference_ms": 1.6263574216948864, "mean_action_processing_ms": 0.0597566486216141, "mean_env_wait_ms": 1.2286749006625497, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 320, "training_iteration": 96, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-59-22", "timestamp": 1652705962, "time_this_iter_s": 15.435134172439575, "time_total_s": 1356.8405044078827, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1356.8405044078827, "timesteps_since_restore": 3072, "iterations_since_restore": 96, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.20909090909091, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38600.56, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [42944.0, 40456.0, 40150.0, 33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22481789274844033, "mean_inference_ms": 1.95992475643716, "mean_action_processing_ms": 0.06498997160687846, "mean_env_wait_ms": 2.1661700915018858, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 97000, "timesteps_this_iter": 32, "agent_timesteps_total": 97000, "timers": {"load_time_ms": 0.134, "load_throughput": 238906.6, "learn_time_ms": 21.188, "learn_throughput": 1510.299}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.005210141185671091, "mean_q": 11652.783203125, "min_q": 7473.64794921875, "max_q": 12172.15625, "cur_lr": 0.6}, "td_error": [2.6181640625, 100.6181640625, 114.6181640625, 11924.15625, 130.6181640625, 102.6181640625, 64.6181640625, 56.6181640625, -11.3818359375, -109.3818359375, 132.6181640625, 56.6181640625, 134.6181640625, -3878.849609375, 126.6181640625, 134.6181640625, -3736.849609375, -7.3818359375, -4181.876953125, -4745.89013671875, 34.6181640625, -139.3818359375, -89.3818359375, 26.6181640625, 62.6181640625, -129.3818359375, 11908.15625, 104.6181640625, 128.6181640625, -73.3818359375, 134.6181640625, -93.3818359375], "mean_td_error": 258.9037170410156, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 97000, "num_agent_steps_sampled": 97000, "num_steps_trained": 768032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 768032, "last_target_update_ts": 97000, "num_target_updates": 193}, "done": false, "episodes_total": 323, "training_iteration": 97, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-59-37", "timestamp": 1652705977, "time_this_iter_s": 14.64976167678833, "time_total_s": 1371.490266084671, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1371.490266084671, "timesteps_since_restore": 3104, "iterations_since_restore": 97, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.847619047619045, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38587.82, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [33628.0, 35166.0, 38268.0, 41714.0, 47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2248501471105456, "mean_inference_ms": 1.960610267420729, "mean_action_processing_ms": 0.06500603996069776, "mean_env_wait_ms": 2.160370426322264, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 98000, "timesteps_this_iter": 32, "agent_timesteps_total": 98000, "timers": {"load_time_ms": 0.235, "load_throughput": 136455.6, "learn_time_ms": 34.89, "learn_throughput": 917.157}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.01107944454997778, "mean_q": 11804.64453125, "min_q": 7514.23681640625, "max_q": 12213.501953125, "cur_lr": 0.6}, "td_error": [-4211.91552734375, 83.1923828125, 111.1923828125, 121.1923828125, -72.8076171875, 121.1923828125, 85.1923828125, 111.1923828125, 11953.501953125, 93.1923828125, 141.1923828125, -108.8076171875, 141.1923828125, -4255.84765625, -88.8076171875, 13.1923828125, 139.1923828125, 79.1923828125, -4698.07275390625, 141.1923828125, 139.1923828125, -84.8076171875, 141.1923828125, -68.8076171875, -66.8076171875, 141.1923828125, 127.1923828125, -62.8076171875, 139.1923828125, -64.8076171875, 123.1923828125, 65.1923828125], "mean_td_error": 13.345382690429688, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 98000, "num_agent_steps_sampled": 98000, "num_steps_trained": 776032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 776032, "last_target_update_ts": 98000, "num_target_updates": 195}, "done": false, "episodes_total": 326, "training_iteration": 98, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-59-47", "timestamp": 1652705987, "time_this_iter_s": 9.707414150238037, "time_total_s": 1381.197680234909, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1381.197680234909, "timesteps_since_restore": 3136, "iterations_since_restore": 98, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.30714285714286, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38718.38, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [47916.0, 33554.0, 44136.0, 43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22491706333619932, "mean_inference_ms": 1.9617174128510084, "mean_action_processing_ms": 0.06503425964677635, "mean_env_wait_ms": 2.1529593708903887, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 99000, "timesteps_this_iter": 32, "agent_timesteps_total": 99000, "timers": {"load_time_ms": 0.2, "load_throughput": 160259.974, "learn_time_ms": 27.671, "learn_throughput": 1156.436}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0022680694237351418, "mean_q": 11844.86328125, "min_q": 8008.76513671875, "max_q": 12240.4912109375, "cur_lr": 0.6}, "td_error": [76.912109375, -111.087890625, 36.912109375, -91.087890625, -27.087890625, -27.087890625, -4318.81396484375, -59.087890625, 110.912109375, -4191.7412109375, 52.912109375, 140.912109375, 86.912109375, 108.912109375, -107.087890625, 136.912109375, 82.912109375, 122.912109375, -4092.81396484375, 26.912109375, -101.087890625, 140.912109375, 120.912109375, -1.087890625, -53.087890625, -79.087890625, 12.912109375, 140.912109375, -79.087890625, 126.912109375, 140.912109375, -111.087890625], "mean_td_error": -368.2161865234375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 99000, "num_agent_steps_sampled": 99000, "num_steps_trained": 784032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 784032, "last_target_update_ts": 99000, "num_target_updates": 197}, "done": false, "episodes_total": 330, "training_iteration": 99, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_12-59-57", "timestamp": 1652705997, "time_this_iter_s": 10.156639575958252, "time_total_s": 1391.3543198108673, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1391.3543198108673, "timesteps_since_restore": 3168, "iterations_since_restore": 99, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.08, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38546.04, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [43282.0, 45812.0, 41782.0, 37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22493599814273846, "mean_inference_ms": 1.9622390797120328, "mean_action_processing_ms": 0.06504589995670351, "mean_env_wait_ms": 2.1473233342465408, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 100000, "timesteps_this_iter": 32, "agent_timesteps_total": 100000, "timers": {"load_time_ms": 0.138, "load_throughput": 231969.803, "learn_time_ms": 21.131, "learn_throughput": 1514.355}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.008077815175056458, "mean_q": 11547.9130859375, "min_q": 7249.146484375, "max_q": 12258.2216796875, "cur_lr": 0.6}, "td_error": [-9.0625, 36.9375, 72.9375, -67.0625, -109.0625, 136.9375, -109.0625, 11984.2216796875, -4872.1376953125, 72.9375, 12016.2216796875, -4725.6494140625, 66.9375, 136.9375, 11984.2216796875, -87.0625, 18.9375, -4657.90234375, -73.0625, -91.0625, 12000.2216796875, 132.9375, -3981.822265625, 80.9375, -4549.6494140625, -105.0625, 58.9375, 62.9375, 106.9375, 38.9375, 136.9375, -95.0625], "mean_td_error": 800.415283203125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 100000, "num_agent_steps_sampled": 100000, "num_steps_trained": 792032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 792032, "last_target_update_ts": 100000, "num_target_updates": 199}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.1321243232225612, "mean_inference_ms": 1.5988742458138558, "mean_action_processing_ms": 0.05870230308200118, "mean_env_wait_ms": 1.206111501112189, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 333, "training_iteration": 100, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-00-07", "timestamp": 1652706007, "time_this_iter_s": 9.458111047744751, "time_total_s": 1400.812430858612, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1400.812430858612, "timesteps_since_restore": 3200, "iterations_since_restore": 100, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 24.923076923076927, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38596.48, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37938.0, 37770.0, 45656.0, 40096.0, 41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22494754867613798, "mean_inference_ms": 1.9626781957970811, "mean_action_processing_ms": 0.06505452411701187, "mean_env_wait_ms": 2.1417269791431037, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 101000, "timesteps_this_iter": 32, "agent_timesteps_total": 101000, "timers": {"load_time_ms": 0.199, "load_throughput": 160528.32, "learn_time_ms": 31.43, "learn_throughput": 1018.13}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.02562037482857704, "mean_q": 11868.7705078125, "min_q": 7643.06201171875, "max_q": 12272.791015625, "cur_lr": 0.6}, "td_error": [12020.791015625, 113.716796875, -98.283203125, -76.283203125, 11970.791015625, 113.716796875, 129.716796875, -100.283203125, 109.716796875, -4409.146484375, -100.283203125, 129.716796875, -140.283203125, -164.283203125, -46.283203125, -70.283203125, -142.283203125, -130.283203125, -62.283203125, 95.716796875, -114.283203125, -106.283203125, -4634.01220703125, 25.716796875, 125.716796875, -46.283203125, -3676.3369140625, -16.283203125, -110.283203125, 127.716796875, -80.283203125, 61.716796875], "mean_td_error": 334.38873291015625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 101000, "num_agent_steps_sampled": 101000, "num_steps_trained": 800032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 800032, "last_target_update_ts": 101000, "num_target_updates": 201}, "done": false, "episodes_total": 336, "training_iteration": 101, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-00-20", "timestamp": 1652706020, "time_this_iter_s": 13.516031980514526, "time_total_s": 1414.3284628391266, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1414.3284628391266, "timesteps_since_restore": 3232, "iterations_since_restore": 101, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.250000000000004, "ram_util_percent": 18.695}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38528.6, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [41878.0, 43678.0, 37976.0, 36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22494266936050317, "mean_inference_ms": 1.9630559215546493, "mean_action_processing_ms": 0.0650593114542263, "mean_env_wait_ms": 2.1342634147198836, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 102000, "timesteps_this_iter": 32, "agent_timesteps_total": 102000, "timers": {"load_time_ms": 0.252, "load_throughput": 127124.198, "learn_time_ms": 35.956, "learn_throughput": 889.987}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.04364078491926193, "mean_q": 11125.544921875, "min_q": 7393.4296875, "max_q": 12273.7919921875, "cur_lr": 0.6}, "td_error": [56.6025390625, -3873.8330078125, 94.6025390625, -125.3974609375, -69.3974609375, 102.6025390625, 110.6025390625, -4831.759765625, -4787.759765625, -79.3974609375, 26.6025390625, 12.6025390625, 84.6025390625, -85.3974609375, 72.6025390625, 116.6025390625, -4963.759765625, 86.6025390625, 11995.7919921875, -4991.759765625, 118.6025390625, 11995.7919921875, -4883.759765625, -4570.64208984375, 78.6025390625, 88.6025390625, -147.3974609375, -153.3974609375, -3957.8330078125, 84.6025390625, 118.6025390625, -93.3974609375], "mean_td_error": -386.5708312988281, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 102000, "num_agent_steps_sampled": 102000, "num_steps_trained": 808032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 808032, "last_target_update_ts": 102000, "num_target_updates": 203}, "done": false, "episodes_total": 340, "training_iteration": 102, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-00-32", "timestamp": 1652706032, "time_this_iter_s": 11.736794710159302, "time_total_s": 1426.065257549286, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1426.065257549286, "timesteps_since_restore": 3264, "iterations_since_restore": 102, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 27.33125, "ram_util_percent": 18.7}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38555.8, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36978.0, 29610.0, 36880.0, 36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22494428132622155, "mean_inference_ms": 1.963388048906898, "mean_action_processing_ms": 0.06506508649144965, "mean_env_wait_ms": 2.128863537291883, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 103000, "timesteps_this_iter": 32, "agent_timesteps_total": 103000, "timers": {"load_time_ms": 0.234, "load_throughput": 136998.804, "learn_time_ms": 37.611, "learn_throughput": 850.816}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.01404931116849184, "mean_q": 11144.0771484375, "min_q": 7436.091796875, "max_q": 12261.8427734375, "cur_lr": 0.6}, "td_error": [-4790.2294921875, 12009.8427734375, -4902.2294921875, 95.521484375, 121.521484375, 12031.8427734375, 12017.8427734375, -84.478515625, -92.478515625, -54.478515625, 11981.8427734375, -4908.2294921875, 107.521484375, 123.521484375, -108.478515625, -4097.5625, -4706.2294921875, 12017.8427734375, -72.478515625, 12013.8427734375, -122.478515625, -70.478515625, -102.478515625, -4316.017578125, -66.478515625, 125.521484375, 107.521484375, -4126.27392578125, -3913.5625, -54.478515625, -26.478515625, 95.521484375], "mean_td_error": 1132.31591796875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 103000, "num_agent_steps_sampled": 103000, "num_steps_trained": 816032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 816032, "last_target_update_ts": 103000, "num_target_updates": 205}, "done": false, "episodes_total": 343, "training_iteration": 103, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-00-47", "timestamp": 1652706047, "time_this_iter_s": 14.93074107170105, "time_total_s": 1440.995998620987, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1440.995998620987, "timesteps_since_restore": 3296, "iterations_since_restore": 103, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.590909090909086, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38583.54, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36828.0, 37090.0, 44858.0, 42994.0, 32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22493267832338112, "mean_inference_ms": 1.9635724268546364, "mean_action_processing_ms": 0.06506697525797245, "mean_env_wait_ms": 2.1234913562775146, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 104000, "timesteps_this_iter": 32, "agent_timesteps_total": 104000, "timers": {"load_time_ms": 0.128, "load_throughput": 249707.401, "learn_time_ms": 19.75, "learn_throughput": 1620.279}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03773614019155502, "mean_q": 11505.0146484375, "min_q": 7485.00732421875, "max_q": 12280.966796875, "cur_lr": 0.6}, "td_error": [131.98046875, 137.98046875, -3947.326171875, -72.01953125, -120.01953125, 135.98046875, 133.98046875, -3788.552734375, 5.98046875, -130.01953125, -110.01953125, -4373.3115234375, 81.98046875, -4657.97900390625, 73.98046875, 12050.966796875, 137.98046875, -70.01953125, 99.98046875, -2.01953125, 137.98046875, -3758.552734375, 51.98046875, -3676.8779296875, 77.98046875, 93.98046875, -70.01953125, 115.98046875, -108.01953125, 67.98046875, 137.98046875, 71.98046875], "mean_td_error": -348.0662841796875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 104000, "num_agent_steps_sampled": 104000, "num_steps_trained": 824032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 824032, "last_target_update_ts": 104000, "num_target_updates": 207}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13018828021488257, "mean_inference_ms": 1.5731971549034243, "mean_action_processing_ms": 0.057723243761301016, "mean_env_wait_ms": 1.188300386665265, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 346, "training_iteration": 104, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-01-00", "timestamp": 1652706060, "time_this_iter_s": 12.981969594955444, "time_total_s": 1453.9779682159424, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1453.9779682159424, "timesteps_since_restore": 3328, "iterations_since_restore": 104, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.744444444444447, "ram_util_percent": 18.7}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38467.94, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [32122.0, 39422.0, 39468.0, 35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2248935083114627, "mean_inference_ms": 1.9635647951641417, "mean_action_processing_ms": 0.06505678142187449, "mean_env_wait_ms": 2.1162917000053696, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 105000, "timesteps_this_iter": 32, "agent_timesteps_total": 105000, "timers": {"load_time_ms": 0.254, "load_throughput": 126049.707, "learn_time_ms": 36.811, "learn_throughput": 869.297}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0204693041741848, "mean_q": 11410.9326171875, "min_q": 7852.73828125, "max_q": 12276.9033203125, "cur_lr": 0.6}, "td_error": [-56.9384765625, -98.9384765625, 127.0615234375, 103.0615234375, -70.9384765625, 129.0615234375, 67.0615234375, -3624.6396484375, -140.9384765625, -3877.39453125, -3528.6396484375, 129.0615234375, -4431.103515625, -4435.103515625, -72.9384765625, 63.0615234375, 25.0615234375, -4418.11279296875, -124.9384765625, -124.9384765625, 59.0615234375, -110.9384765625, -64.9384765625, 12032.9033203125, 127.0615234375, 12002.9033203125, -88.9384765625, -136.9384765625, -72.9384765625, -6.9384765625, 129.0615234375, -3508.6396484375], "mean_td_error": -125.04409790039062, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 105000, "num_agent_steps_sampled": 105000, "num_steps_trained": 832032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 832032, "last_target_update_ts": 105000, "num_target_updates": 209}, "done": false, "episodes_total": 350, "training_iteration": 105, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-01-13", "timestamp": 1652706073, "time_this_iter_s": 12.848385095596313, "time_total_s": 1466.8263533115387, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1466.8263533115387, "timesteps_since_restore": 3360, "iterations_since_restore": 105, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.763157894736846, "ram_util_percent": 18.7}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38416.32, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [35090.0, 37670.0, 34336.0, 42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2248642737857551, "mean_inference_ms": 1.9635295729866693, "mean_action_processing_ms": 0.0650478887219611, "mean_env_wait_ms": 2.111031743101254, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 106000, "timesteps_this_iter": 32, "agent_timesteps_total": 106000, "timers": {"load_time_ms": 0.238, "load_throughput": 134486.701, "learn_time_ms": 36.945, "learn_throughput": 866.162}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0197647362947464, "mean_q": 11819.8349609375, "min_q": 8284.4072265625, "max_q": 12290.736328125, "cur_lr": 0.6}, "td_error": [126.0732421875, 32.0732421875, -131.9267578125, 124.0732421875, -65.9267578125, -85.9267578125, -17.9267578125, -23.9267578125, 124.0732421875, 126.0732421875, -111.9267578125, 122.0732421875, 12034.736328125, -125.9267578125, -47.9267578125, 12010.736328125, -77.9267578125, -3666.609375, -77.9267578125, 122.0732421875, -135.9267578125, -3399.5048828125, 48.0732421875, -49.9267578125, 32.0732421875, -3900.1630859375, -4076.255859375, 58.0732421875, -23.9267578125, 11988.736328125, 34.0732421875, -97.9267578125], "mean_td_error": 652.047119140625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 106000, "num_agent_steps_sampled": 106000, "num_steps_trained": 840032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 840032, "last_target_update_ts": 106000, "num_target_updates": 211}, "done": false, "episodes_total": 353, "training_iteration": 106, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-01-27", "timestamp": 1652706087, "time_this_iter_s": 13.64683198928833, "time_total_s": 1480.473185300827, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1480.473185300827, "timesteps_since_restore": 3392, "iterations_since_restore": 106, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.710526315789476, "ram_util_percent": 18.7}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38456.34, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [42106.0, 43088.0, 38350.0, 39896.0, 36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22482810764198594, "mean_inference_ms": 1.9634173862033462, "mean_action_processing_ms": 0.06503670762845472, "mean_env_wait_ms": 2.1059004311746397, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 107000, "timesteps_this_iter": 32, "agent_timesteps_total": 107000, "timers": {"load_time_ms": 0.245, "load_throughput": 130727.309, "learn_time_ms": 38.083, "learn_throughput": 840.26}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.009327739477157593, "mean_q": 11637.80078125, "min_q": 7605.86328125, "max_q": 12304.142578125, "cur_lr": 0.6}, "td_error": [-132.9453125, 125.0546875, 12044.142578125, -78.9453125, 12038.142578125, -80.9453125, 53.0546875, -114.9453125, 123.0546875, 33.0546875, -98.9453125, 125.0546875, 123.0546875, 12030.142578125, -4371.6357421875, -4317.6357421875, -4753.224609375, -80.9453125, -126.9453125, 113.0546875, 125.0546875, -108.9453125, -108.9453125, -96.9453125, -3776.3076171875, 119.0546875, 111.0546875, -4252.8486328125, -78.9453125, 95.0546875, 87.0546875, 125.0546875], "mean_td_error": 465.3152160644531, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 107000, "num_agent_steps_sampled": 107000, "num_steps_trained": 848032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 848032, "last_target_update_ts": 107000, "num_target_updates": 213}, "done": false, "episodes_total": 356, "training_iteration": 107, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-01-41", "timestamp": 1652706101, "time_this_iter_s": 14.589659929275513, "time_total_s": 1495.0628452301025, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1495.0628452301025, "timesteps_since_restore": 3424, "iterations_since_restore": 107, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.876190476190477, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38332.76, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36438.0, 35594.0, 44742.0, 42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22478567604709937, "mean_inference_ms": 1.963310082349818, "mean_action_processing_ms": 0.06502293409339917, "mean_env_wait_ms": 2.0991882208715356, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 108000, "timesteps_this_iter": 32, "agent_timesteps_total": 108000, "timers": {"load_time_ms": 0.227, "load_throughput": 140955.396, "learn_time_ms": 34.925, "learn_throughput": 916.248}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.006627731956541538, "mean_q": 11720.6171875, "min_q": 7998.3408203125, "max_q": 12330.0947265625, "cur_lr": 0.6}, "td_error": [48.91015625, 12040.0947265625, -109.08984375, 12074.0947265625, 12082.0947265625, 120.91015625, -81.08984375, -113.08984375, 126.91015625, 102.91015625, -3544.23046875, -3829.3603515625, 42.91015625, 20.91015625, 126.91015625, -3763.3603515625, -23.08984375, 128.91015625, -105.08984375, -23.08984375, -67.08984375, -19.08984375, -99.08984375, 12078.0947265625, -4044.9306640625, 126.91015625, 40.91015625, 114.91015625, -4288.84375, 126.91015625, -117.08984375, -73.08984375], "mean_td_error": 909.455810546875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 108000, "num_agent_steps_sampled": 108000, "num_steps_trained": 856032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 856032, "last_target_update_ts": 108000, "num_target_updates": 215}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13057182871431586, "mean_inference_ms": 1.5822891070068419, "mean_action_processing_ms": 0.058059941955649286, "mean_env_wait_ms": 1.1955612516008827, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 360, "training_iteration": 108, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-01-58", "timestamp": 1652706118, "time_this_iter_s": 16.829449892044067, "time_total_s": 1511.8922951221466, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1511.8922951221466, "timesteps_since_restore": 3456, "iterations_since_restore": 108, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.25, "ram_util_percent": 18.7}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38258.46, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [42378.0, 40366.0, 41286.0, 39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22474983134409773, "mean_inference_ms": 1.9631588074467485, "mean_action_processing_ms": 0.065010420531827, "mean_env_wait_ms": 2.0941910202292386, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 109000, "timesteps_this_iter": 32, "agent_timesteps_total": 109000, "timers": {"load_time_ms": 0.229, "load_throughput": 139592.021, "learn_time_ms": 38.971, "learn_throughput": 821.132}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.01125709805637598, "mean_q": 11857.5849609375, "min_q": 8402.7216796875, "max_q": 12321.357421875, "cur_lr": 0.6}, "td_error": [109.8876953125, 57.8876953125, 123.8876953125, 83.8876953125, -128.1123046875, 121.8876953125, -132.1123046875, 57.8876953125, 27.8876953125, 19.8876953125, -3764.154296875, 103.8876953125, -162.1123046875, -126.1123046875, -120.1123046875, -44.1123046875, 55.8876953125, -108.1123046875, -108.1123046875, -86.1123046875, -68.1123046875, -72.1123046875, -52.1123046875, 12059.357421875, -20.1123046875, -3728.5849609375, -104.1123046875, -128.1123046875, -3413.7041015625, 53.8876953125, -3918.748046875, -142.1123046875], "mean_td_error": -110.96456909179688, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 109000, "num_agent_steps_sampled": 109000, "num_steps_trained": 864032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 864032, "last_target_update_ts": 109000, "num_target_updates": 217}, "done": false, "episodes_total": 363, "training_iteration": 109, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-02-12", "timestamp": 1652706132, "time_this_iter_s": 13.749062299728394, "time_total_s": 1525.641357421875, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1525.641357421875, "timesteps_since_restore": 3488, "iterations_since_restore": 109, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.725000000000005, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38012.02, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [39310.0, 35896.0, 32226.0, 40528.0, 40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2247176862642629, "mean_inference_ms": 1.9630167877545919, "mean_action_processing_ms": 0.06499825353161073, "mean_env_wait_ms": 2.089285936791194, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 110000, "timesteps_this_iter": 32, "agent_timesteps_total": 110000, "timers": {"load_time_ms": 0.237, "load_throughput": 135041.481, "learn_time_ms": 37.767, "learn_throughput": 847.31}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03870689496397972, "mean_q": 12221.369140625, "min_q": 8615.2392578125, "max_q": 12337.6962890625, "cur_lr": 0.6}, "td_error": [133.50390625, 133.50390625, -64.49609375, 12089.6962890625, 133.50390625, 29.50390625, -74.49609375, -122.49609375, 75.50390625, 97.50390625, -100.49609375, 127.50390625, 133.50390625, 129.50390625, 85.50390625, 133.50390625, -122.49609375, -68.49609375, -66.49609375, 69.50390625, -64.49609375, 125.50390625, 133.50390625, 12087.6962890625, 12071.6962890625, 133.50390625, -3816.953125, -62.49609375, -150.49609375, -122.49609375, -6.49609375, 27.50390625], "mean_td_error": 1034.6326904296875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 110000, "num_agent_steps_sampled": 110000, "num_steps_trained": 872032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 872032, "last_target_update_ts": 110000, "num_target_updates": 219}, "done": false, "episodes_total": 366, "training_iteration": 110, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-02-27", "timestamp": 1652706147, "time_this_iter_s": 14.63709545135498, "time_total_s": 1540.27845287323, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1540.27845287323, "timesteps_since_restore": 3520, "iterations_since_restore": 110, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.174999999999997, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38188.78, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [40008.0, 38462.0, 31868.0, 45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22464911997499917, "mean_inference_ms": 1.9625952165381653, "mean_action_processing_ms": 0.06497500368961008, "mean_env_wait_ms": 2.082798645415332, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 111000, "timesteps_this_iter": 32, "agent_timesteps_total": 111000, "timers": {"load_time_ms": 0.13, "load_throughput": 245730.004, "learn_time_ms": 20.612, "learn_throughput": 1552.504}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0036082258448004723, "mean_q": 11851.365234375, "min_q": 7778.0224609375, "max_q": 12364.232421875, "cur_lr": 0.6}, "td_error": [-4637.494140625, -93.2841796875, 12074.232421875, 12106.232421875, 74.7158203125, -81.2841796875, -75.2841796875, 144.7158203125, 136.7158203125, 12120.232421875, 104.7158203125, 12108.232421875, 80.7158203125, -101.2841796875, 12114.232421875, 106.7158203125, 112.7158203125, 106.7158203125, -3648.10546875, -3975.2177734375, -9.2841796875, -4124.107421875, -89.2841796875, 4.7158203125, -57.2841796875, 80.7158203125, -89.2841796875, -85.2841796875, 76.7158203125, 140.7158203125, -57.2841796875, -93.2841796875], "mean_td_error": 1389.8968505859375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 111000, "num_agent_steps_sampled": 111000, "num_steps_trained": 880032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 880032, "last_target_update_ts": 111000, "num_target_updates": 221}, "done": false, "episodes_total": 370, "training_iteration": 111, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-02-39", "timestamp": 1652706159, "time_this_iter_s": 12.509796857833862, "time_total_s": 1552.7882497310638, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1552.7882497310638, "timesteps_since_restore": 3552, "iterations_since_restore": 111, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.52777777777778, "ram_util_percent": 18.7}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38290.04, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [45668.0, 36352.0, 35444.0, 35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22459439669528647, "mean_inference_ms": 1.9622155439356113, "mean_action_processing_ms": 0.0649556832368941, "mean_env_wait_ms": 2.0780491030368298, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 112000, "timesteps_this_iter": 32, "agent_timesteps_total": 112000, "timers": {"load_time_ms": 0.271, "load_throughput": 118055.878, "learn_time_ms": 43.714, "learn_throughput": 732.032}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.00564179290086031, "mean_q": 11887.04296875, "min_q": 8163.76220703125, "max_q": 12368.373046875, "cur_lr": 0.6}, "td_error": [12122.373046875, 130.705078125, -115.294921875, 26.705078125, -119.294921875, -123.294921875, -113.294921875, -123.294921875, -109.294921875, -63.294921875, -109.294921875, 130.705078125, 96.705078125, 122.705078125, -89.294921875, -129.294921875, -4113.90576171875, 12120.373046875, -69.294921875, 12106.373046875, -59.294921875, -125.294921875, 28.705078125, 64.705078125, -3669.501953125, -3658.0048828125, 82.705078125, 52.705078125, 124.705078125, 78.705078125, -15.294921875, -3770.3349609375], "mean_td_error": 647.28125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 112000, "num_agent_steps_sampled": 112000, "num_steps_trained": 888032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 888032, "last_target_update_ts": 112000, "num_target_updates": 223}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13091595112433138, "mean_inference_ms": 1.587379859921592, "mean_action_processing_ms": 0.05825771176946999, "mean_env_wait_ms": 1.201117014603989, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 373, "training_iteration": 112, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-02-53", "timestamp": 1652706173, "time_this_iter_s": 13.28463339805603, "time_total_s": 1566.0728831291199, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1566.0728831291199, "timesteps_since_restore": 3584, "iterations_since_restore": 112, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.7421052631579, "ram_util_percent": 18.7}}
{"episode_reward_max": 48094.0, "episode_reward_min": 29474.0, "episode_reward_mean": 38252.2, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [35784.0, 36472.0, 38498.0, 29474.0, 40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22453714410844616, "mean_inference_ms": 1.9618031885354725, "mean_action_processing_ms": 0.0649359946305481, "mean_env_wait_ms": 2.0733779459243733, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 113000, "timesteps_this_iter": 32, "agent_timesteps_total": 113000, "timers": {"load_time_ms": 0.129, "load_throughput": 247223.666, "learn_time_ms": 20.6, "learn_throughput": 1553.428}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.022304924204945564, "mean_q": 11998.40625, "min_q": 7862.7529296875, "max_q": 12385.466796875, "cur_lr": 0.6}, "td_error": [122.376953125, -4618.3369140625, -57.623046875, 12139.466796875, -4378.3369140625, -3332.1591796875, 12137.466796875, 10.376953125, 146.376953125, 32.376953125, 12133.466796875, 146.376953125, 146.376953125, 144.376953125, 18.376953125, -61.623046875, -5.623046875, 12107.466796875, -93.623046875, 146.376953125, -97.623046875, 146.376953125, -151.623046875, -101.623046875, -83.623046875, -101.623046875, 112.376953125, 0.376953125, -93.623046875, -69.623046875, 42.376953125, -121.623046875], "mean_td_error": 1136.3892822265625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 113000, "num_agent_steps_sampled": 113000, "num_steps_trained": 896032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 896032, "last_target_update_ts": 113000, "num_target_updates": 225}, "done": false, "episodes_total": 376, "training_iteration": 113, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-03-07", "timestamp": 1652706187, "time_this_iter_s": 14.132859706878662, "time_total_s": 1580.2057428359985, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1580.2057428359985, "timesteps_since_restore": 3616, "iterations_since_restore": 113, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.794999999999998, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 48094.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38348.56, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [40170.0, 39302.0, 38998.0, 42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22446309947361034, "mean_inference_ms": 1.9612464381899701, "mean_action_processing_ms": 0.0649095160780926, "mean_env_wait_ms": 2.0673347260503534, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 114000, "timesteps_this_iter": 32, "agent_timesteps_total": 114000, "timers": {"load_time_ms": 0.223, "load_throughput": 143226.687, "learn_time_ms": 33.696, "learn_throughput": 949.655}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03486480563879013, "mean_q": 11931.275390625, "min_q": 7910.859375, "max_q": 12396.736328125, "cur_lr": 0.6}, "td_error": [90.3583984375, -41.6416015625, 108.3583984375, -69.6416015625, 12152.736328125, 110.3583984375, -95.6416015625, 110.3583984375, 12132.736328125, 12140.736328125, -133.6416015625, 66.3583984375, 48.3583984375, -4567.5185546875, 108.3583984375, -3426.5859375, 8.3583984375, -93.6416015625, 46.3583984375, 12118.736328125, -91.6416015625, -117.6416015625, 110.3583984375, 110.3583984375, -3642.5302734375, 4.3583984375, 80.3583984375, 98.3583984375, -117.6416015625, 110.3583984375, 90.3583984375, -3202.6318359375], "mean_td_error": 1070.1964111328125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 114000, "num_agent_steps_sampled": 114000, "num_steps_trained": 904032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 904032, "last_target_update_ts": 114000, "num_target_updates": 227}, "done": false, "episodes_total": 380, "training_iteration": 114, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-03-22", "timestamp": 1652706202, "time_this_iter_s": 14.722000122070312, "time_total_s": 1594.9277429580688, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1594.9277429580688, "timesteps_since_restore": 3648, "iterations_since_restore": 114, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.586363636363636, "ram_util_percent": 18.60000000000001}}
{"episode_reward_max": 48094.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38332.08, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [42198.0, 40320.0, 30272.0, 48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22439925088687754, "mean_inference_ms": 1.9607323213344179, "mean_action_processing_ms": 0.06488646805248971, "mean_env_wait_ms": 2.0628289410536094, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 115000, "timesteps_this_iter": 32, "agent_timesteps_total": 115000, "timers": {"load_time_ms": 0.133, "load_throughput": 240017.396, "learn_time_ms": 20.714, "learn_throughput": 1544.828}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.01180141419172287, "mean_q": 12062.36328125, "min_q": 7965.3798828125, "max_q": 12427.87890625, "cur_lr": 0.6}, "td_error": [144.3759765625, 12179.87890625, -27.6240234375, -4510.123046875, -101.6240234375, -3729.857421875, 144.3759765625, 2.3759765625, 122.3759765625, -99.6240234375, 142.3759765625, -63.6240234375, -63.6240234375, -83.6240234375, 88.3759765625, -83.6240234375, -45.6240234375, 34.3759765625, 128.3759765625, -101.6240234375, 12143.87890625, 110.3759765625, 142.3759765625, 48.3759765625, 112.3759765625, -95.6240234375, 62.3759765625, 142.3759765625, -91.6240234375, -3709.3505859375, 12149.87890625, -65.6240234375], "mean_td_error": 782.0650634765625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 115000, "num_agent_steps_sampled": 115000, "num_steps_trained": 912032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 912032, "last_target_update_ts": 115000, "num_target_updates": 229}, "done": false, "episodes_total": 383, "training_iteration": 115, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-03-32", "timestamp": 1652706212, "time_this_iter_s": 10.642125129699707, "time_total_s": 1605.5698680877686, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1605.5698680877686, "timesteps_since_restore": 3680, "iterations_since_restore": 115, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.360000000000003, "ram_util_percent": 18.6}}
{"episode_reward_max": 48094.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38447.5, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [48094.0, 35984.0, 44092.0, 42712.0, 43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22436384157084796, "mean_inference_ms": 1.9604814384580107, "mean_action_processing_ms": 0.06487221792579097, "mean_env_wait_ms": 2.058626604887982, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 116000, "timesteps_this_iter": 32, "agent_timesteps_total": 116000, "timers": {"load_time_ms": 0.193, "load_throughput": 166193.323, "learn_time_ms": 29.24, "learn_throughput": 1094.388}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.02374107576906681, "mean_q": 12112.98046875, "min_q": 8853.986328125, "max_q": 12433.8154296875, "cur_lr": 0.6}, "td_error": [76.2353515625, -3311.67578125, 134.2353515625, -119.7646484375, -119.7646484375, -89.7646484375, 132.2353515625, -135.7646484375, -3449.59375, -113.7646484375, 130.2353515625, 126.2353515625, 134.2353515625, 134.2353515625, 112.2353515625, 90.2353515625, 134.2353515625, 64.2353515625, -89.7646484375, 132.2353515625, -69.7646484375, 134.2353515625, -129.7646484375, -105.7646484375, 12155.8154296875, 134.2353515625, -11.7646484375, -3326.720703125, -117.7646484375, -109.7646484375, 106.2353515625, 46.2353515625], "mean_td_error": 83.63796997070312, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 116000, "num_agent_steps_sampled": 116000, "num_steps_trained": 920032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 920032, "last_target_update_ts": 116000, "num_target_updates": 231}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12962824448047458, "mean_inference_ms": 1.5675848947833453, "mean_action_processing_ms": 0.05751351572538077, "mean_env_wait_ms": 1.1886313734020588, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 386, "training_iteration": 116, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-03-48", "timestamp": 1652706228, "time_this_iter_s": 15.61889123916626, "time_total_s": 1621.1887593269348, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1621.1887593269348, "timesteps_since_restore": 3712, "iterations_since_restore": 116, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.090909090909093, "ram_util_percent": 18.60000000000001}}
{"episode_reward_max": 48072.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38160.18, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [43814.0, 42032.0, 34226.0, 42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22435345769186804, "mean_inference_ms": 1.9604895390366692, "mean_action_processing_ms": 0.0648642847290669, "mean_env_wait_ms": 2.0532628998272635, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 117000, "timesteps_this_iter": 32, "agent_timesteps_total": 117000, "timers": {"load_time_ms": 0.134, "load_throughput": 238397.385, "learn_time_ms": 20.28, "learn_throughput": 1577.926}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0016451120609417558, "mean_q": 12345.4970703125, "min_q": 9204.763671875, "max_q": 12446.810546875, "cur_lr": 0.6}, "td_error": [-122.828125, -94.828125, 125.171875, -62.828125, 12172.810546875, -106.828125, 133.171875, -60.828125, -18.828125, 125.171875, 73.171875, 133.171875, -86.828125, -94.828125, -150.828125, 77.171875, 133.171875, 27.171875, -40.828125, 41.171875, -72.828125, -3108.875, -68.828125, 133.171875, 71.171875, -118.828125, -98.828125, -110.828125, 75.171875, -118.828125, 43.171875, -70.828125], "mean_td_error": 273.59661865234375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 117000, "num_agent_steps_sampled": 117000, "num_steps_trained": 928032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 928032, "last_target_update_ts": 117000, "num_target_updates": 233}, "done": false, "episodes_total": 390, "training_iteration": 117, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-04-02", "timestamp": 1652706242, "time_this_iter_s": 14.403721809387207, "time_total_s": 1635.592481136322, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1635.592481136322, "timesteps_since_restore": 3744, "iterations_since_restore": 117, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.576190476190476, "ram_util_percent": 18.647619047619052}}
{"episode_reward_max": 48072.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38089.24, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [42074.0, 46058.0, 40052.0, 37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22435887888737827, "mean_inference_ms": 1.960624032853758, "mean_action_processing_ms": 0.06486246952325199, "mean_env_wait_ms": 2.0494088293468526, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 118000, "timesteps_this_iter": 32, "agent_timesteps_total": 118000, "timers": {"load_time_ms": 0.216, "load_throughput": 147979.854, "learn_time_ms": 33.664, "learn_throughput": 950.571}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.02866433933377266, "mean_q": 12092.716796875, "min_q": 8107.337890625, "max_q": 12457.658203125, "cur_lr": 0.6}, "td_error": [-69.982421875, 140.017578125, 122.017578125, -95.982421875, -89.982421875, -3342.3447265625, -67.982421875, 12179.658203125, -77.982421875, 70.017578125, -61.982421875, -99.982421875, -53.982421875, -95.982421875, -101.982421875, -131.982421875, 112.017578125, 66.017578125, 138.017578125, -83.982421875, 74.017578125, 98.017578125, -4418.302734375, -39.982421875, 122.017578125, 82.017578125, 140.017578125, -3991.447265625, -131.982421875, 12199.658203125, -65.982421875, -59.982421875], "mean_td_error": 389.4280090332031, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 118000, "num_agent_steps_sampled": 118000, "num_steps_trained": 936032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 936032, "last_target_update_ts": 118000, "num_target_updates": 235}, "done": false, "episodes_total": 393, "training_iteration": 118, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-04-17", "timestamp": 1652706257, "time_this_iter_s": 14.098766088485718, "time_total_s": 1649.6912472248077, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1649.6912472248077, "timesteps_since_restore": 3776, "iterations_since_restore": 118, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 21.725, "ram_util_percent": 18.674999999999997}}
{"episode_reward_max": 48072.0, "episode_reward_min": 30052.0, "episode_reward_mean": 37915.68, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37394.0, 35944.0, 35728.0, 31016.0, 47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22435426332167738, "mean_inference_ms": 1.9606739911868167, "mean_action_processing_ms": 0.06485772780772829, "mean_env_wait_ms": 2.045559114843785, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 119000, "timesteps_this_iter": 32, "agent_timesteps_total": 119000, "timers": {"load_time_ms": 0.228, "load_throughput": 140160.535, "learn_time_ms": 34.643, "learn_throughput": 923.698}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.037415966391563416, "mean_q": 12231.478515625, "min_q": 8519.11328125, "max_q": 12470.703125, "cur_lr": 0.6}, "td_error": [-105.998046875, 132.001953125, 26.001953125, 62.001953125, 92.001953125, 110.001953125, 12222.703125, -65.998046875, 28.001953125, 22.001953125, -67.998046875, -99.998046875, -91.998046875, 72.001953125, 120.001953125, -9.998046875, -97.998046875, -127.998046875, -3965.587890625, -93.998046875, -115.998046875, 68.001953125, -3827.60546875, -103.998046875, 132.001953125, -107.998046875, 114.001953125, -119.998046875, -127.998046875, -63.998046875, -113.998046875, -75.998046875], "mean_td_error": 119.2364501953125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 119000, "num_agent_steps_sampled": 119000, "num_steps_trained": 944032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 944032, "last_target_update_ts": 119000, "num_target_updates": 237}, "done": false, "episodes_total": 396, "training_iteration": 119, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-04-31", "timestamp": 1652706271, "time_this_iter_s": 14.190556764602661, "time_total_s": 1663.8818039894104, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1663.8818039894104, "timesteps_since_restore": 3808, "iterations_since_restore": 119, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.665, "ram_util_percent": 18.695}}
{"episode_reward_max": 48072.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38035.28, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [47260.0, 42438.0, 35016.0, 32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22434520683680387, "mean_inference_ms": 1.9606934467054409, "mean_action_processing_ms": 0.06484937554617748, "mean_env_wait_ms": 2.040564391344186, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 120000, "timesteps_this_iter": 32, "agent_timesteps_total": 120000, "timers": {"load_time_ms": 0.132, "load_throughput": 242620.622, "learn_time_ms": 20.303, "learn_throughput": 1576.086}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.02465924806892872, "mean_q": 11801.599609375, "min_q": 8203.4501953125, "max_q": 12487.103515625, "cur_lr": 0.6}, "td_error": [94.029296875, 60.029296875, 112.029296875, 124.029296875, 62.029296875, 94.029296875, 12237.103515625, 104.029296875, -4255.6240234375, -3483.392578125, 124.029296875, -109.970703125, -97.970703125, -121.970703125, -123.970703125, -115.970703125, -3586.9443359375, -3558.9443359375, -69.970703125, 124.029296875, -111.970703125, -69.970703125, -79.970703125, 122.029296875, 122.029296875, 40.029296875, -3635.392578125, 124.029296875, 110.029296875, -3293.642578125, 20.029296875, 124.029296875], "mean_td_error": -278.6907958984375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 120000, "num_agent_steps_sampled": 120000, "num_steps_trained": 952032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 952032, "last_target_update_ts": 120000, "num_target_updates": 239}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12810584399822275, "mean_inference_ms": 1.546615442293589, "mean_action_processing_ms": 0.05674089356112939, "mean_env_wait_ms": 1.1768389007751445, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 400, "training_iteration": 120, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-04-43", "timestamp": 1652706283, "time_this_iter_s": 11.925268173217773, "time_total_s": 1675.8070721626282, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1675.8070721626282, "timesteps_since_restore": 3840, "iterations_since_restore": 120, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 21.841176470588238, "ram_util_percent": 18.7}}
{"episode_reward_max": 48072.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38049.68, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [32818.0, 37796.0, 41890.0, 36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2243314267398817, "mean_inference_ms": 1.9606596356152841, "mean_action_processing_ms": 0.0648417722136975, "mean_env_wait_ms": 2.036878829279466, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 121000, "timesteps_this_iter": 32, "agent_timesteps_total": 121000, "timers": {"load_time_ms": 0.242, "load_throughput": 132312.429, "learn_time_ms": 35.783, "learn_throughput": 894.284}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.02621830441057682, "mean_q": 11912.755859375, "min_q": 8469.3740234375, "max_q": 12488.07421875, "cur_lr": 0.6}, "td_error": [-3433.3681640625, 23.80078125, -94.19921875, -4096.8994140625, 12244.07421875, -3295.1044921875, 111.80078125, 101.80078125, -140.19921875, -3772.91015625, 113.80078125, -114.19921875, -76.19921875, -88.19921875, 55.80078125, 105.80078125, 41.80078125, -136.19921875, 113.80078125, -142.19921875, -92.19921875, -3868.91015625, -154.19921875, 79.80078125, -86.19921875, -134.19921875, -102.19921875, -126.19921875, -134.19921875, -130.19921875, 73.80078125, -164.19921875], "mean_td_error": -228.63430786132812, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 121000, "num_agent_steps_sampled": 121000, "num_steps_trained": 960032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 960032, "last_target_update_ts": 121000, "num_target_updates": 241}, "done": false, "episodes_total": 403, "training_iteration": 121, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-04-57", "timestamp": 1652706297, "time_this_iter_s": 13.838149547576904, "time_total_s": 1689.645221710205, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1689.645221710205, "timesteps_since_restore": 3872, "iterations_since_restore": 121, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 25.505, "ram_util_percent": 18.620000000000005}}
{"episode_reward_max": 48072.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38097.7, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36810.0, 39220.0, 35876.0, 35668.0, 37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22432014071334194, "mean_inference_ms": 1.9606198218916533, "mean_action_processing_ms": 0.0648344236739887, "mean_env_wait_ms": 2.0332336128554527, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 122000, "timesteps_this_iter": 32, "agent_timesteps_total": 122000, "timers": {"load_time_ms": 0.271, "load_throughput": 118243.087, "learn_time_ms": 33.738, "learn_throughput": 948.496}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.04827268049120903, "mean_q": 11460.0390625, "min_q": 8288.3349609375, "max_q": 12495.517578125, "cur_lr": 0.6}, "td_error": [-3630.9599609375, 119.2412109375, -28.7587890625, -114.7587890625, -92.7587890625, -66.7587890625, -126.7587890625, 55.2412109375, -3449.0361328125, 9.2412109375, -80.7587890625, 12201.517578125, -3720.9599609375, -114.7587890625, -4279.94140625, 17.2412109375, -3549.0361328125, -80.7587890625, -140.7587890625, 12239.517578125, -3906.0576171875, 49.2412109375, -80.7587890625, -3443.0361328125, -4004.0576171875, -116.7587890625, -98.7587890625, -46.7587890625, -3311.0361328125, 31.2412109375, -132.7587890625, -80.7587890625], "mean_td_error": -311.7193298339844, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 122000, "num_agent_steps_sampled": 122000, "num_steps_trained": 968032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 968032, "last_target_update_ts": 122000, "num_target_updates": 243}, "done": false, "episodes_total": 406, "training_iteration": 122, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-05-13", "timestamp": 1652706313, "time_this_iter_s": 15.835158824920654, "time_total_s": 1705.4803805351257, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1705.4803805351257, "timesteps_since_restore": 3904, "iterations_since_restore": 122, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.730434782608693, "ram_util_percent": 18.60869565217392}}
{"episode_reward_max": 48476.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38179.04, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37998.0, 33232.0, 39878.0, 30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22428391822605698, "mean_inference_ms": 1.96036910770085, "mean_action_processing_ms": 0.06481870065244548, "mean_env_wait_ms": 2.028351136552169, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 123000, "timesteps_this_iter": 32, "agent_timesteps_total": 123000, "timers": {"load_time_ms": 0.257, "load_throughput": 124540.9, "learn_time_ms": 39.445, "learn_throughput": 811.259}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.005187452770769596, "mean_q": 12056.3896484375, "min_q": 8329.0634765625, "max_q": 12506.2998046875, "cur_lr": 0.6}, "td_error": [64.712890625, -3040.35546875, 60.712890625, 60.712890625, -123.287109375, 14.712890625, 130.712890625, -121.287109375, 130.712890625, -111.287109375, 122.712890625, -3983.0078125, -89.287109375, -145.287109375, 126.712890625, -117.287109375, -77.287109375, 74.712890625, -123.287109375, 74.712890625, -15.287109375, 60.712890625, -3090.35546875, -71.287109375, -155.287109375, 128.712890625, -119.287109375, -19.287109375, -4294.5234375, -73.287109375, 122.712890625, -107.287109375], "mean_td_error": -459.5087890625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 123000, "num_agent_steps_sampled": 123000, "num_steps_trained": 976032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 976032, "last_target_update_ts": 123000, "num_target_updates": 245}, "done": false, "episodes_total": 410, "training_iteration": 123, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-05-26", "timestamp": 1652706326, "time_this_iter_s": 13.28060531616211, "time_total_s": 1718.7609858512878, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1718.7609858512878, "timesteps_since_restore": 3936, "iterations_since_restore": 123, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.21052631578947, "ram_util_percent": 18.636842105263156}}
{"episode_reward_max": 48476.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38173.02, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [30762.0, 36250.0, 39268.0, 31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22425124282908232, "mean_inference_ms": 1.9601314736987687, "mean_action_processing_ms": 0.06480609774651339, "mean_env_wait_ms": 2.024794766586884, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 124000, "timesteps_this_iter": 32, "agent_timesteps_total": 124000, "timers": {"load_time_ms": 0.243, "load_throughput": 131779.802, "learn_time_ms": 35.964, "learn_throughput": 889.782}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.031238790601491928, "mean_q": 11722.849609375, "min_q": 8365.3974609375, "max_q": 12506.1943359375, "cur_lr": 0.6}, "td_error": [-13.857421875, 128.142578125, -119.857421875, 12254.1943359375, -3914.0400390625, -81.857421875, -81.857421875, 66.142578125, -3830.0400390625, -79.857421875, 68.142578125, -3251.517578125, -103.857421875, -119.857421875, -69.857421875, 90.142578125, -3495.4306640625, 128.142578125, -3387.0146484375, 126.142578125, -4108.654296875, 128.142578125, -3.857421875, -109.857421875, -3203.3388671875, -69.857421875, 126.142578125, 22.142578125, 70.142578125, -135.857421875, 98.142578125, -21.857421875], "mean_td_error": -403.01312255859375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 124000, "num_agent_steps_sampled": 124000, "num_steps_trained": 984032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 984032, "last_target_update_ts": 124000, "num_target_updates": 247}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12844222269446834, "mean_inference_ms": 1.5526142649439865, "mean_action_processing_ms": 0.0569932679439947, "mean_env_wait_ms": 1.1830379880637125, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 413, "training_iteration": 124, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-05-41", "timestamp": 1652706341, "time_this_iter_s": 15.540391683578491, "time_total_s": 1734.3013775348663, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1734.3013775348663, "timesteps_since_restore": 3968, "iterations_since_restore": 124, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.290909090909096, "ram_util_percent": 18.622727272727275}}
{"episode_reward_max": 48476.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38213.08, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [31980.0, 34124.0, 42928.0, 42760.0, 33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22421538887480363, "mean_inference_ms": 1.9598678408379875, "mean_action_processing_ms": 0.06479253902138718, "mean_env_wait_ms": 2.0213140121258393, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 125000, "timesteps_this_iter": 32, "agent_timesteps_total": 125000, "timers": {"load_time_ms": 0.148, "load_throughput": 216654.928, "learn_time_ms": 22.802, "learn_throughput": 1403.401}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.01725991629064083, "mean_q": 12175.921875, "min_q": 8590.0126953125, "max_q": 12504.3583984375, "cur_lr": 0.6}, "td_error": [-123.23828125, 128.76171875, 128.76171875, 126.76171875, 128.76171875, 126.76171875, 100.76171875, -87.23828125, -79.23828125, 126.76171875, 62.76171875, -119.23828125, -69.23828125, -109.23828125, 124.76171875, -121.23828125, 108.76171875, -3194.857421875, 128.76171875, 128.76171875, 108.76171875, -75.23828125, -59.23828125, -91.23828125, -3485.2578125, -81.23828125, -81.23828125, 110.76171875, 126.76171875, -71.23828125, -109.23828125, -3997.583984375], "mean_td_error": -318.36279296875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 125000, "num_agent_steps_sampled": 125000, "num_steps_trained": 992032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 992032, "last_target_update_ts": 125000, "num_target_updates": 249}, "done": false, "episodes_total": 416, "training_iteration": 125, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-05-56", "timestamp": 1652706356, "time_this_iter_s": 14.480791091918945, "time_total_s": 1748.7821686267853, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1748.7821686267853, "timesteps_since_restore": 4000, "iterations_since_restore": 125, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.814285714285713, "ram_util_percent": 18.685714285714283}}
{"episode_reward_max": 48476.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38136.56, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [33628.0, 33630.0, 30966.0, 39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22416263397224667, "mean_inference_ms": 1.959465162001389, "mean_action_processing_ms": 0.06477292643768182, "mean_env_wait_ms": 2.0166849459799048, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 126000, "timesteps_this_iter": 32, "agent_timesteps_total": 126000, "timers": {"load_time_ms": 0.266, "load_throughput": 120525.977, "learn_time_ms": 41.892, "learn_throughput": 763.868}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03445533290505409, "mean_q": 12076.3564453125, "min_q": 8613.888671875, "max_q": 12497.33984375, "cur_lr": 0.6}, "td_error": [91.0634765625, -88.9365234375, -3047.01171875, -120.9365234375, -138.9365234375, -126.9365234375, -40.9365234375, 43.0634765625, 39.0634765625, 75.0634765625, 15.0634765625, 107.0634765625, -0.9365234375, 107.0634765625, 43.0634765625, -114.9365234375, -126.9365234375, 47.0634765625, 101.0634765625, -204.9365234375, -3056.1328125, -96.9365234375, -134.9365234375, -3812.3876953125, 49.0634765625, 41.0634765625, 71.0634765625, 109.0634765625, -3415.6875, 97.0634765625, -78.9365234375, 69.0634765625], "mean_td_error": -421.9200744628906, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 126000, "num_agent_steps_sampled": 126000, "num_steps_trained": 1000032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1000032, "last_target_update_ts": 126000, "num_target_updates": 251}, "done": false, "episodes_total": 420, "training_iteration": 126, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-06-10", "timestamp": 1652706370, "time_this_iter_s": 13.542244672775269, "time_total_s": 1762.3244132995605, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1762.3244132995605, "timesteps_since_restore": 4032, "iterations_since_restore": 126, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.810526315789474, "ram_util_percent": 18.7}}
{"episode_reward_max": 48476.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38254.02, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [39484.0, 39048.0, 43744.0, 39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22411152287480213, "mean_inference_ms": 1.9590536595193115, "mean_action_processing_ms": 0.06475498142475027, "mean_env_wait_ms": 2.0132701182279096, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 127000, "timesteps_this_iter": 32, "agent_timesteps_total": 127000, "timers": {"load_time_ms": 0.128, "load_throughput": 250359.5, "learn_time_ms": 19.623, "learn_throughput": 1630.718}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.023606300354003906, "mean_q": 12190.8251953125, "min_q": 8471.380859375, "max_q": 12520.462890625, "cur_lr": 0.6}, "td_error": [-100.62890625, -100.62890625, 71.37109375, -3365.666015625, 47.37109375, 129.37109375, 139.37109375, 12268.462890625, -4167.7109375, 141.37109375, 61.37109375, -90.62890625, -62.62890625, 12282.462890625, -66.62890625, 35.37109375, 119.37109375, -112.62890625, 95.37109375, -106.62890625, -94.62890625, 141.37109375, -2902.9052734375, 41.37109375, 141.37109375, 12276.462890625, 133.37109375, 141.37109375, -102.62890625, -114.62890625, 1.37109375, -4.62890625], "mean_td_error": 839.8360595703125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 127000, "num_agent_steps_sampled": 127000, "num_steps_trained": 1008032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1008032, "last_target_update_ts": 127000, "num_target_updates": 253}, "done": false, "episodes_total": 423, "training_iteration": 127, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-06-22", "timestamp": 1652706382, "time_this_iter_s": 12.2549409866333, "time_total_s": 1774.5793542861938, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1774.5793542861938, "timesteps_since_restore": 4064, "iterations_since_restore": 127, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.716666666666665, "ram_util_percent": 18.7}}
{"episode_reward_max": 48476.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38228.54, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [39638.0, 37070.0, 44626.0, 40498.0, 37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22407852953667245, "mean_inference_ms": 1.9589406847422404, "mean_action_processing_ms": 0.06474280348168636, "mean_env_wait_ms": 2.010059692324556, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 128000, "timesteps_this_iter": 32, "agent_timesteps_total": 128000, "timers": {"load_time_ms": 0.228, "load_throughput": 140645.214, "learn_time_ms": 35.317, "learn_throughput": 906.076}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.04205857589840889, "mean_q": 11734.1171875, "min_q": 8510.6083984375, "max_q": 12543.4970703125, "cur_lr": 0.6}, "td_error": [-3536.03515625, -66.36328125, 133.63671875, 103.63671875, -152.36328125, -3566.03515625, -60.36328125, -118.36328125, -3800.654296875, -8.36328125, 12277.4970703125, 137.63671875, -64.36328125, 33.63671875, 71.63671875, -108.36328125, -106.36328125, -3770.03515625, -12.36328125, -4081.251953125, -3907.251953125, 91.63671875, -118.36328125, -2949.443359375, 135.63671875, 12277.4970703125, 12271.4970703125, 33.63671875, -104.36328125, -110.36328125, 75.63671875, -104.36328125], "mean_td_error": 340.55596923828125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 128000, "num_agent_steps_sampled": 128000, "num_steps_trained": 1016032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1016032, "last_target_update_ts": 128000, "num_target_updates": 255}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12909813531176917, "mean_inference_ms": 1.5610803255077799, "mean_action_processing_ms": 0.05728792938114814, "mean_env_wait_ms": 1.1891686982157925, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 426, "training_iteration": 128, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-06-37", "timestamp": 1652706397, "time_this_iter_s": 14.84714674949646, "time_total_s": 1789.4265010356903, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1789.4265010356903, "timesteps_since_restore": 4096, "iterations_since_restore": 128, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.87, "ram_util_percent": 18.66}}
{"episode_reward_max": 48476.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38095.52, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37902.0, 35446.0, 35024.0, 45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2240678445918001, "mean_inference_ms": 1.9590961336607566, "mean_action_processing_ms": 0.06473695959628797, "mean_env_wait_ms": 2.0060577172276637, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 129000, "timesteps_this_iter": 32, "agent_timesteps_total": 129000, "timers": {"load_time_ms": 0.24, "load_throughput": 133178.932, "learn_time_ms": 36.273, "learn_throughput": 882.199}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03653939440846443, "mean_q": 12015.603515625, "min_q": 8688.080078125, "max_q": 12548.783203125, "cur_lr": 0.6}, "td_error": [-63.8583984375, -13.8583984375, -55.8583984375, 130.1416015625, 54.1416015625, 100.1416015625, -131.8583984375, -3463.2900390625, 12284.783203125, -3872.5615234375, -71.8583984375, -3357.8466796875, 12270.783203125, -15.8583984375, -13.8583984375, 130.1416015625, -101.8583984375, -3369.5048828125, -25.8583984375, 74.1416015625, 70.1416015625, 68.1416015625, 70.1416015625, -93.8583984375, -69.8583984375, -51.8583984375, 132.1416015625, -75.8583984375, 72.1416015625, 12296.783203125, -3185.8466796875, -113.8583984375], "mean_td_error": 612.6467895507812, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 129000, "num_agent_steps_sampled": 129000, "num_steps_trained": 1024032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1024032, "last_target_update_ts": 129000, "num_target_updates": 257}, "done": false, "episodes_total": 430, "training_iteration": 129, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-06-53", "timestamp": 1652706413, "time_this_iter_s": 15.544117212295532, "time_total_s": 1804.9706182479858, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1804.9706182479858, "timesteps_since_restore": 4128, "iterations_since_restore": 129, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.508695652173916, "ram_util_percent": 18.639130434782615}}
{"episode_reward_max": 48476.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38279.02, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [45322.0, 43098.0, 47500.0, 35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2240902479950237, "mean_inference_ms": 1.9594941331616718, "mean_action_processing_ms": 0.06474207649411928, "mean_env_wait_ms": 2.0033486287017617, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 130000, "timesteps_this_iter": 32, "agent_timesteps_total": 130000, "timers": {"load_time_ms": 0.225, "load_throughput": 142421.188, "learn_time_ms": 35.271, "learn_throughput": 907.272}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.028671037405729294, "mean_q": 12187.62890625, "min_q": 8572.86328125, "max_q": 12536.6923828125, "cur_lr": 0.6}, "td_error": [94.564453125, -79.435546875, 14.564453125, -143.435546875, -3938.6240234375, 22.564453125, -4075.2646484375, -95.435546875, 120.564453125, 122.564453125, -139.435546875, 46.564453125, -67.435546875, -21.435546875, 124.564453125, -111.435546875, -65.435546875, -19.435546875, 124.564453125, -105.435546875, 64.564453125, -3466.4482421875, -75.435546875, 124.564453125, -103.435546875, 12.564453125, -127.435546875, 124.564453125, 56.564453125, 58.564453125, -81.435546875, 124.564453125], "mean_td_error": -358.7489929199219, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 130000, "num_agent_steps_sampled": 130000, "num_steps_trained": 1032032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1032032, "last_target_update_ts": 130000, "num_target_updates": 259}, "done": false, "episodes_total": 433, "training_iteration": 130, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-07-08", "timestamp": 1652706428, "time_this_iter_s": 15.698676824569702, "time_total_s": 1820.6692950725555, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1820.6692950725555, "timesteps_since_restore": 4160, "iterations_since_restore": 130, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.490909090909092, "ram_util_percent": 18.60000000000001}}
{"episode_reward_max": 48476.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38005.92, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [35510.0, 40740.0, 37706.0, 40716.0, 40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22411974738486923, "mean_inference_ms": 1.9599531549510987, "mean_action_processing_ms": 0.06474904773286781, "mean_env_wait_ms": 2.000741261439134, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 131000, "timesteps_this_iter": 32, "agent_timesteps_total": 131000, "timers": {"load_time_ms": 0.175, "load_throughput": 183032.494, "learn_time_ms": 26.845, "learn_throughput": 1192.016}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0222785621881485, "mean_q": 12332.095703125, "min_q": 8727.6904296875, "max_q": 12556.6396484375, "cur_lr": 0.6}, "td_error": [69.0537109375, -88.9462890625, -62.9462890625, 103.0537109375, 137.0537109375, -46.9462890625, -62.9462890625, -56.9462890625, 12298.6396484375, 139.0537109375, 55.0537109375, 137.0537109375, -66.9462890625, 73.0537109375, 67.0537109375, -3693.8955078125, -66.9462890625, -3375.3916015625, -4.9462890625, -68.9462890625, -106.9462890625, 135.0537109375, -116.9462890625, 101.0537109375, -70.9462890625, 43.0537109375, -66.9462890625, 75.0537109375, 137.0537109375, -116.9462890625, 117.0537109375, 139.0537109375], "mean_td_error": 179.7471923828125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 131000, "num_agent_steps_sampled": 131000, "num_steps_trained": 1040032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1040032, "last_target_update_ts": 131000, "num_target_updates": 261}, "done": false, "episodes_total": 436, "training_iteration": 131, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-07-23", "timestamp": 1652706443, "time_this_iter_s": 15.15802550315857, "time_total_s": 1835.827320575714, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1835.827320575714, "timesteps_since_restore": 4192, "iterations_since_restore": 131, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.949999999999996, "ram_util_percent": 18.604545454545462}}
{"episode_reward_max": 48476.0, "episode_reward_min": 30052.0, "episode_reward_mean": 37865.34, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [40224.0, 48072.0, 37956.0, 35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22414864623511882, "mean_inference_ms": 1.9604622491464345, "mean_action_processing_ms": 0.06475513412928123, "mean_env_wait_ms": 1.9972512865020355, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 132000, "timesteps_this_iter": 32, "agent_timesteps_total": 132000, "timers": {"load_time_ms": 0.129, "load_throughput": 248000.237, "learn_time_ms": 19.811, "learn_throughput": 1615.242}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.041473932564258575, "mean_q": 11933.2294921875, "min_q": 8652.1162109375, "max_q": 12569.3505859375, "cur_lr": 0.6}, "td_error": [129.056640625, 12283.3505859375, 93.056640625, -142.943359375, 129.056640625, -70.943359375, -76.943359375, -104.943359375, -56.943359375, 123.056640625, -84.943359375, 13.056640625, -2824.3369140625, -36.943359375, -72.943359375, -0.943359375, 131.056640625, -60.943359375, 129.056640625, -3200.7744140625, -3994.177734375, -76.943359375, -3786.177734375, -92.943359375, 103.056640625, -100.943359375, -3071.2841796875, 41.056640625, 12329.3505859375, -3426.7744140625, -4.943359375, 119.056640625], "mean_td_error": 135.4542236328125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 132000, "num_agent_steps_sampled": 132000, "num_steps_trained": 1048032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1048032, "last_target_update_ts": 132000, "num_target_updates": 263}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12880936377867944, "mean_inference_ms": 1.5594039874996228, "mean_action_processing_ms": 0.05721691918727088, "mean_env_wait_ms": 1.190338095274011, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 440, "training_iteration": 132, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-07-33", "timestamp": 1652706453, "time_this_iter_s": 9.92595648765564, "time_total_s": 1845.7532770633698, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1845.7532770633698, "timesteps_since_restore": 4224, "iterations_since_restore": 132, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.321428571428577, "ram_util_percent": 18.68571428571428}}
{"episode_reward_max": 48476.0, "episode_reward_min": 30052.0, "episode_reward_mean": 37841.96, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [35786.0, 33788.0, 36668.0, 35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22416902345825876, "mean_inference_ms": 1.9608276054082139, "mean_action_processing_ms": 0.06475931101091568, "mean_env_wait_ms": 1.9946568335150063, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 133000, "timesteps_this_iter": 32, "agent_timesteps_total": 133000, "timers": {"load_time_ms": 0.249, "load_throughput": 128339.767, "learn_time_ms": 35.593, "learn_throughput": 899.063}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03623359277844429, "mean_q": 11831.15234375, "min_q": 8695.65625, "max_q": 12575.20703125, "cur_lr": 0.6}, "td_error": [12323.20703125, -3264.1328125, 128.4912109375, 56.4912109375, 52.4912109375, 130.4912109375, -3987.0595703125, -3586.587890625, 118.4912109375, -121.5087890625, -121.5087890625, 130.4912109375, -139.5087890625, 130.4912109375, -75.5087890625, 128.4912109375, -3264.1328125, 62.4912109375, -3761.3759765625, -3482.587890625, -75.5087890625, 12297.20703125, -115.5087890625, 128.4912109375, -71.5087890625, -2812.447265625, 128.4912109375, -127.5087890625, 130.4912109375, -69.5087890625, 70.4912109375, 70.4912109375], "mean_td_error": 31.605865478515625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 133000, "num_agent_steps_sampled": 133000, "num_steps_trained": 1056032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1056032, "last_target_update_ts": 133000, "num_target_updates": 265}, "done": false, "episodes_total": 443, "training_iteration": 133, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-07-48", "timestamp": 1652706468, "time_this_iter_s": 14.966957330703735, "time_total_s": 1860.7202343940735, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1860.7202343940735, "timesteps_since_restore": 4256, "iterations_since_restore": 133, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.247619047619054, "ram_util_percent": 18.685714285714283}}
{"episode_reward_max": 48476.0, "episode_reward_min": 30052.0, "episode_reward_mean": 37934.56, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [35152.0, 40836.0, 37110.0, 37112.0, 32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22420108873641034, "mean_inference_ms": 1.9613025450595085, "mean_action_processing_ms": 0.06476611327428934, "mean_env_wait_ms": 1.9922220063124754, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 134000, "timesteps_this_iter": 32, "agent_timesteps_total": 134000, "timers": {"load_time_ms": 0.246, "load_throughput": 129942.616, "learn_time_ms": 36.986, "learn_throughput": 865.2}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03291208669543266, "mean_q": 12194.2900390625, "min_q": 9149.2626953125, "max_q": 12591.3857421875, "cur_lr": 0.6}, "td_error": [-60.984375, -3351.65625, 121.015625, 133.015625, 12351.3857421875, 141.015625, -72.984375, -88.984375, 119.015625, -3333.107421875, -126.984375, 133.015625, -44.984375, 47.015625, 12305.3857421875, -100.984375, -3221.458984375, -124.984375, 55.015625, -60.984375, 12343.3857421875, 127.015625, -82.984375, -100.984375, -2962.75390625, -68.984375, -100.984375, 81.015625, 111.015625, 141.015625, -54.984375, 117.015625], "mean_td_error": 761.455322265625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 134000, "num_agent_steps_sampled": 134000, "num_steps_trained": 1064032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1064032, "last_target_update_ts": 134000, "num_target_updates": 267}, "done": false, "episodes_total": 446, "training_iteration": 134, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-08-04", "timestamp": 1652706484, "time_this_iter_s": 15.32689881324768, "time_total_s": 1876.0471332073212, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1876.0471332073212, "timesteps_since_restore": 4288, "iterations_since_restore": 134, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.772727272727277, "ram_util_percent": 18.69545454545454}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38208.62, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [32636.0, 32844.0, 40370.0, 36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2242501673767002, "mean_inference_ms": 1.9620121081198798, "mean_action_processing_ms": 0.06477723694392043, "mean_env_wait_ms": 1.9891454169295235, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 135000, "timesteps_this_iter": 32, "agent_timesteps_total": 135000, "timers": {"load_time_ms": 0.254, "load_throughput": 126085.231, "learn_time_ms": 41.512, "learn_throughput": 770.857}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.013399406336247921, "mean_q": 12029.3505859375, "min_q": 8847.5107421875, "max_q": 12617.998046875, "cur_lr": 0.6}, "td_error": [112.751953125, 74.751953125, 28.751953125, -103.248046875, -3010.587890625, 134.751953125, 44.751953125, -111.248046875, -95.248046875, 114.751953125, 114.751953125, 76.751953125, 132.751953125, 134.751953125, -71.248046875, -3779.7353515625, 130.751953125, -2896.3193359375, -103.248046875, 84.751953125, -2900.3193359375, 134.751953125, 134.751953125, -101.248046875, -3164.6044921875, -61.248046875, 12353.998046875, -7.248046875, -13.248046875, 132.751953125, -3256.6044921875, -59.248046875], "mean_td_error": -181.04293823242188, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 135000, "num_agent_steps_sampled": 135000, "num_steps_trained": 1072032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1072032, "last_target_update_ts": 135000, "num_target_updates": 269}, "done": false, "episodes_total": 450, "training_iteration": 135, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-08-18", "timestamp": 1652706498, "time_this_iter_s": 14.474316835403442, "time_total_s": 1890.5214500427246, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1890.5214500427246, "timesteps_since_restore": 4320, "iterations_since_restore": 135, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.785714285714285, "ram_util_percent": 18.680952380952377}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38395.96, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36272.0, 37980.0, 36846.0, 39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22428790196237558, "mean_inference_ms": 1.962558400150416, "mean_action_processing_ms": 0.06478573326498684, "mean_env_wait_ms": 1.9869695204024282, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 136000, "timesteps_this_iter": 32, "agent_timesteps_total": 136000, "timers": {"load_time_ms": 0.234, "load_throughput": 136566.675, "learn_time_ms": 35.937, "learn_throughput": 890.444}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.06887833774089813, "mean_q": 12096.763671875, "min_q": 8838.5322265625, "max_q": 12613.2060546875, "cur_lr": 0.6}, "td_error": [-2995.8369140625, -119.5849609375, -51.5849609375, 132.4150390625, 132.4150390625, 130.4150390625, -59.5849609375, -107.5849609375, -113.5849609375, 106.4150390625, 134.4150390625, -15.5849609375, -69.5849609375, 74.4150390625, 66.4150390625, 12361.2060546875, -1.5849609375, -3187.8369140625, -63.5849609375, -75.5849609375, -109.5849609375, -3782.2587890625, -2637.9033203125, -3774.2587890625, 12363.2060546875, -67.5849609375, 132.4150390625, -49.5849609375, -99.5849609375, -5.5849609375, -7.5849609375, 134.4150390625], "mean_td_error": 261.64666748046875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 136000, "num_agent_steps_sampled": 136000, "num_steps_trained": 1080032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1080032, "last_target_update_ts": 136000, "num_target_updates": 271}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.1294697257727107, "mean_inference_ms": 1.5680191856005088, "mean_action_processing_ms": 0.057503728583307545, "mean_env_wait_ms": 1.1941610440449695, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 453, "training_iteration": 136, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-08-34", "timestamp": 1652706514, "time_this_iter_s": 15.351855516433716, "time_total_s": 1905.8733055591583, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1905.8733055591583, "timesteps_since_restore": 4352, "iterations_since_restore": 136, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.281818181818178, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38470.74, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [39898.0, 37452.0, 32104.0, 41628.0, 33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22432300878898012, "mean_inference_ms": 1.9630795772508172, "mean_action_processing_ms": 0.0647939220331076, "mean_env_wait_ms": 1.9848076246794668, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 137000, "timesteps_this_iter": 32, "agent_timesteps_total": 137000, "timers": {"load_time_ms": 0.243, "load_throughput": 131702.216, "learn_time_ms": 35.413, "learn_throughput": 903.633}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.033739905804395676, "mean_q": 12131.7470703125, "min_q": 8875.796875, "max_q": 12620.41015625, "cur_lr": 0.6}, "td_error": [-67.4580078125, 128.5419921875, 126.5419921875, 126.5419921875, -71.4580078125, -3172.8955078125, 12342.41015625, -3001.275390625, -111.4580078125, 12370.41015625, -55.4580078125, -123.4580078125, -45.4580078125, -19.4580078125, -109.4580078125, 122.5419921875, -73.4580078125, 66.5419921875, 126.5419921875, 12326.41015625, 128.5419921875, -83.4580078125, 12374.41015625, -75.4580078125, 70.5419921875, -107.4580078125, -2985.146484375, -75.4580078125, -2873.146484375, 12366.41015625, 12342.41015625, -3712.0712890625], "mean_td_error": 1820.4783935546875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 137000, "num_agent_steps_sampled": 137000, "num_steps_trained": 1088032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1088032, "last_target_update_ts": 137000, "num_target_updates": 273}, "done": false, "episodes_total": 456, "training_iteration": 137, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-08-48", "timestamp": 1652706528, "time_this_iter_s": 14.380225419998169, "time_total_s": 1920.2535309791565, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1920.2535309791565, "timesteps_since_restore": 4384, "iterations_since_restore": 137, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.81904761904762, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38442.02, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [33438.0, 37952.0, 37954.0, 33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22435081041112226, "mean_inference_ms": 1.9635973305438807, "mean_action_processing_ms": 0.06479945145816482, "mean_env_wait_ms": 1.9818613340962394, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 138000, "timesteps_this_iter": 32, "agent_timesteps_total": 138000, "timers": {"load_time_ms": 0.241, "load_throughput": 132704.892, "learn_time_ms": 36.278, "learn_throughput": 882.082}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.007854800671339035, "mean_q": 12256.93359375, "min_q": 9449.6630859375, "max_q": 12632.7890625, "cur_lr": 0.6}, "td_error": [132.01953125, -51.98046875, 9203.6630859375, 140.01953125, -2752.8505859375, 114.01953125, -63.98046875, 138.01953125, 140.01953125, -2959.8134765625, -25.98046875, 94.01953125, -103.98046875, 64.01953125, 122.01953125, 140.01953125, 120.01953125, 12392.7890625, -109.98046875, 138.01953125, -93.98046875, 138.01953125, 140.01953125, -59.98046875, -7.98046875, 122.01953125, -3025.51171875, -103.98046875, -51.98046875, -101.98046875, 70.01953125, -109.98046875], "mean_td_error": 430.7751159667969, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 138000, "num_agent_steps_sampled": 138000, "num_steps_trained": 1096032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1096032, "last_target_update_ts": 138000, "num_target_updates": 275}, "done": false, "episodes_total": 460, "training_iteration": 138, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-09-01", "timestamp": 1652706541, "time_this_iter_s": 12.860327243804932, "time_total_s": 1933.1138582229614, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1933.1138582229614, "timesteps_since_restore": 4416, "iterations_since_restore": 138, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.15, "ram_util_percent": 18.7}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30052.0, "episode_reward_mean": 38498.52, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [33050.0, 30052.0, 36284.0, 36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22437791847156777, "mean_inference_ms": 1.9640389119266108, "mean_action_processing_ms": 0.06480555341822641, "mean_env_wait_ms": 1.9797673152430877, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 139000, "timesteps_this_iter": 32, "agent_timesteps_total": 139000, "timers": {"load_time_ms": 0.242, "load_throughput": 132182.123, "learn_time_ms": 36.761, "learn_throughput": 870.485}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.07020525634288788, "mean_q": 12243.005859375, "min_q": 8946.431640625, "max_q": 12638.884765625, "cur_lr": 0.6}, "td_error": [12382.884765625, 129.4443359375, 63.4443359375, -3563.0087890625, 12402.884765625, 119.4443359375, -104.5556640625, -3026.3349609375, -2600.638671875, 129.4443359375, 97.4443359375, 12368.884765625, 12392.884765625, -72.5556640625, -20.5556640625, 81.4443359375, -122.5556640625, 129.4443359375, -106.5556640625, 53.4443359375, 129.4443359375, 25.4443359375, -112.5556640625, -3130.3349609375, -74.5556640625, 12368.884765625, 12396.884765625, 115.4443359375, 71.4443359375, 95.4443359375, 21.4443359375, 39.4443359375], "mean_td_error": 1958.7744140625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 139000, "num_agent_steps_sampled": 139000, "num_steps_trained": 1104032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1104032, "last_target_update_ts": 139000, "num_target_updates": 277}, "done": false, "episodes_total": 463, "training_iteration": 139, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-09-17", "timestamp": 1652706557, "time_this_iter_s": 15.310679912567139, "time_total_s": 1948.4245381355286, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1948.4245381355286, "timesteps_since_restore": 4448, "iterations_since_restore": 139, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.36363636363636, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 38674.96, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36540.0, 42270.0, 38902.0, 47924.0, 40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2244027755191276, "mean_inference_ms": 1.9644492052713816, "mean_action_processing_ms": 0.06481072849690464, "mean_env_wait_ms": 1.9777319150175063, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 140000, "timesteps_this_iter": 32, "agent_timesteps_total": 140000, "timers": {"load_time_ms": 0.237, "load_throughput": 135231.968, "learn_time_ms": 36.568, "learn_throughput": 875.092}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.034940578043460846, "mean_q": 11990.3984375, "min_q": 8969.5439453125, "max_q": 12626.25, "cur_lr": 0.6}, "td_error": [70.36328125, 62.36328125, 122.36328125, -3526.3427734375, 124.36328125, -119.63671875, 110.36328125, 122.36328125, 112.36328125, -3746.9013671875, -2516.623046875, 78.36328125, -95.63671875, -77.63671875, 110.36328125, 12376.25, 110.36328125, -15.63671875, -69.63671875, -161.63671875, 12376.25, -3700.9013671875, 84.36328125, 110.36328125, -3041.953125, -63.63671875, 110.36328125, 130.36328125, 128.36328125, 130.36328125, -103.63671875, -3668.3427734375], "mean_td_error": 173.81732177734375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 140000, "num_agent_steps_sampled": 140000, "num_steps_trained": 1112032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1112032, "last_target_update_ts": 140000, "num_target_updates": 279}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12971993662404874, "mean_inference_ms": 1.5730398766779465, "mean_action_processing_ms": 0.05765474815503062, "mean_env_wait_ms": 1.197695947808205, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 466, "training_iteration": 140, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-09-31", "timestamp": 1652706571, "time_this_iter_s": 14.484015941619873, "time_total_s": 1962.9085540771484, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1962.9085540771484, "timesteps_since_restore": 4480, "iterations_since_restore": 140, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 24.74, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 38603.24, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [40150.0, 39088.0, 41226.0, 37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22444628165643274, "mean_inference_ms": 1.9650716540168593, "mean_action_processing_ms": 0.06482013103867472, "mean_env_wait_ms": 1.975119576142725, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 141000, "timesteps_this_iter": 32, "agent_timesteps_total": 141000, "timers": {"load_time_ms": 0.228, "load_throughput": 140380.429, "learn_time_ms": 36.329, "learn_throughput": 880.85}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.006880582310259342, "mean_q": 12228.73828125, "min_q": 9419.8037109375, "max_q": 12609.7880859375, "cur_lr": 0.6}, "td_error": [53.1494140625, -128.8505859375, -134.8505859375, -3057.2109375, -138.8505859375, 111.1494140625, 109.1494140625, 113.1494140625, 43.1494140625, 53.1494140625, 12367.7880859375, 83.1494140625, 43.1494140625, -96.8505859375, 55.1494140625, -82.8505859375, -88.8505859375, 113.1494140625, -102.8505859375, -88.8505859375, -128.8505859375, -3080.8203125, -3226.8349609375, -88.8505859375, 51.1494140625, 12353.7880859375, -136.8505859375, -148.8505859375, 25.1494140625, -3100.1376953125, -128.8505859375, 11.1494140625], "mean_td_error": 363.3268127441406, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 141000, "num_agent_steps_sampled": 141000, "num_steps_trained": 1120032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1120032, "last_target_update_ts": 141000, "num_target_updates": 281}, "done": false, "episodes_total": 470, "training_iteration": 141, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-09-45", "timestamp": 1652706585, "time_this_iter_s": 14.113336086273193, "time_total_s": 1977.0218901634216, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1977.0218901634216, "timesteps_since_restore": 4512, "iterations_since_restore": 141, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.461904761904762, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 38455.78, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37354.0, 35890.0, 40436.0, 33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22448894822884224, "mean_inference_ms": 1.9656343089313695, "mean_action_processing_ms": 0.06483026162308342, "mean_env_wait_ms": 1.9732025546375436, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 142000, "timesteps_this_iter": 32, "agent_timesteps_total": 142000, "timers": {"load_time_ms": 0.234, "load_throughput": 136566.675, "learn_time_ms": 35.603, "learn_throughput": 898.796}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.07426567375659943, "mean_q": 12223.205078125, "min_q": 9036.7265625, "max_q": 12602.740234375, "cur_lr": 0.6}, "td_error": [21.2421875, 12346.740234375, 121.2421875, 67.2421875, -2815.0791015625, -110.7578125, -150.7578125, 67.2421875, 27.2421875, 125.2421875, -90.7578125, 123.2421875, 125.2421875, 11.2421875, 23.2421875, -56.7578125, 123.2421875, 25.2421875, 39.2421875, -3272.7158203125, 19.2421875, -3536.771484375, -2495.5869140625, 59.2421875, 101.2421875, 57.2421875, -120.7578125, -84.7578125, 12338.740234375, 63.2421875, 125.2421875, 12302.740234375], "mean_td_error": 799.3475341796875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 142000, "num_agent_steps_sampled": 142000, "num_steps_trained": 1128032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1128032, "last_target_update_ts": 142000, "num_target_updates": 283}, "done": false, "episodes_total": 473, "training_iteration": 142, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-10-00", "timestamp": 1652706600, "time_this_iter_s": 14.652597665786743, "time_total_s": 1991.6744878292084, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 1991.6744878292084, "timesteps_since_restore": 4544, "iterations_since_restore": 142, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.504761904761907, "ram_util_percent": 18.699999999999996}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 38484.96, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [33806.0, 35402.0, 42638.0, 38018.0, 32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22453052251176928, "mean_inference_ms": 1.966194141878267, "mean_action_processing_ms": 0.0648403143427776, "mean_env_wait_ms": 1.971333125584257, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 143000, "timesteps_this_iter": 32, "agent_timesteps_total": 143000, "timers": {"load_time_ms": 0.175, "load_throughput": 183132.389, "learn_time_ms": 27.911, "learn_throughput": 1146.483}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.012251805514097214, "mean_q": 12545.943359375, "min_q": 9908.6328125, "max_q": 12631.0185546875, "cur_lr": 0.6}, "td_error": [76.265625, 112.265625, -13.734375, 144.265625, 12375.0185546875, -101.734375, -81.734375, 144.265625, -39.734375, -9.734375, 134.265625, 82.265625, 86.265625, -45.734375, -93.734375, -109.734375, -43.734375, -59.734375, 116.265625, -2808.1201171875, 144.265625, -101.734375, -71.734375, 12363.0185546875, 48.265625, -7.734375, 50.265625, 86.265625, -109.734375, -71.734375, 146.265625, 66.265625], "mean_td_error": 700.1756591796875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 143000, "num_agent_steps_sampled": 143000, "num_steps_trained": 1136032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1136032, "last_target_update_ts": 143000, "num_target_updates": 285}, "done": false, "episodes_total": 476, "training_iteration": 143, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-10-14", "timestamp": 1652706614, "time_this_iter_s": 14.321732521057129, "time_total_s": 2005.9962203502655, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2005.9962203502655, "timesteps_since_restore": 4576, "iterations_since_restore": 143, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.795, "ram_util_percent": 18.635}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 38606.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [32422.0, 40832.0, 43568.0, 40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22458644195933253, "mean_inference_ms": 1.966950136656893, "mean_action_processing_ms": 0.06485402968138927, "mean_env_wait_ms": 1.9689231594145116, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 144000, "timesteps_this_iter": 32, "agent_timesteps_total": 144000, "timers": {"load_time_ms": 0.251, "load_throughput": 127583.392, "learn_time_ms": 41.459, "learn_throughput": 771.84}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.017628464847803116, "mean_q": 12442.572265625, "min_q": 9517.8388671875, "max_q": 12623.728515625, "cur_lr": 0.6}, "td_error": [-114.1728515625, 12355.728515625, 83.8271484375, 119.8271484375, 27.8271484375, -2984.0625, -22.1728515625, 119.8271484375, 87.8271484375, -16.1728515625, -120.1728515625, 121.8271484375, 79.8271484375, 109.8271484375, -130.1728515625, 12369.728515625, 117.8271484375, 57.8271484375, -80.1728515625, 19.8271484375, -42.1728515625, -116.1728515625, 99.8271484375, -2709.267578125, -50.1728515625, 121.8271484375, 61.8271484375, 119.8271484375, 59.8271484375, 119.8271484375, -124.1728515625, 121.8271484375], "mean_td_error": 620.8527221679688, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 144000, "num_agent_steps_sampled": 144000, "num_steps_trained": 1144032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1144032, "last_target_update_ts": 144000, "num_target_updates": 287}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.1314739509485748, "mean_inference_ms": 1.587054760585658, "mean_action_processing_ms": 0.058251104468758946, "mean_env_wait_ms": 1.2077953448815208, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 480, "training_iteration": 144, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-10-31", "timestamp": 1652706631, "time_this_iter_s": 16.536112070083618, "time_total_s": 2022.5323324203491, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2022.5323324203491, "timesteps_since_restore": 4608, "iterations_since_restore": 144, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.920833333333334, "ram_util_percent": 18.625000000000004}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 38695.9, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [40944.0, 46216.0, 37172.0, 30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22464147647977162, "mean_inference_ms": 1.9676556725156233, "mean_action_processing_ms": 0.0648691804316063, "mean_env_wait_ms": 1.9672561779315478, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 145000, "timesteps_this_iter": 32, "agent_timesteps_total": 145000, "timers": {"load_time_ms": 0.261, "load_throughput": 122763.86, "learn_time_ms": 35.975, "learn_throughput": 889.506}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.009866375476121902, "mean_q": 12024.0517578125, "min_q": 9096.1669921875, "max_q": 12604.908203125, "cur_lr": 0.6}, "td_error": [57.599609375, 123.599609375, -66.400390625, 13.599609375, 12368.908203125, -110.400390625, 121.599609375, -2805.1611328125, -94.400390625, 12308.908203125, 103.599609375, 65.599609375, 89.599609375, -3387.1416015625, 123.599609375, -2905.1611328125, -106.400390625, -3439.1416015625, -16.400390625, -24.400390625, -74.400390625, -98.400390625, -116.400390625, -2767.5087890625, 35.599609375, 41.599609375, -120.400390625, -22.400390625, 123.599609375, -124.400390625, -3165.701171875, 12340.908203125], "mean_td_error": 577.3031005859375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 145000, "num_agent_steps_sampled": 145000, "num_steps_trained": 1152032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1152032, "last_target_update_ts": 145000, "num_target_updates": 289}, "done": false, "episodes_total": 483, "training_iteration": 145, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-10-46", "timestamp": 1652706646, "time_this_iter_s": 14.414155721664429, "time_total_s": 2036.9464881420135, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2036.9464881420135, "timesteps_since_restore": 4640, "iterations_since_restore": 145, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.65714285714286, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 38693.6, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [30452.0, 34860.0, 38266.0, 38572.0, 39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22469396100167427, "mean_inference_ms": 1.9683435649428307, "mean_action_processing_ms": 0.06488359058857568, "mean_env_wait_ms": 1.965561945635601, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 146000, "timesteps_this_iter": 32, "agent_timesteps_total": 146000, "timers": {"load_time_ms": 0.276, "load_throughput": 116145.49, "learn_time_ms": 39.154, "learn_throughput": 817.29}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.02069203369319439, "mean_q": 12152.44921875, "min_q": 9123.107421875, "max_q": 12610.607421875, "cur_lr": 0.6}, "td_error": [-2673.5498046875, 62.59765625, 136.59765625, -3005.591796875, -65.40234375, -85.40234375, -2945.228515625, 12346.607421875, -85.40234375, -99.40234375, -69.40234375, -109.40234375, 134.59765625, 136.59765625, 70.59765625, 136.59765625, 68.59765625, 40.59765625, 70.59765625, -91.40234375, -113.40234375, -111.40234375, -65.40234375, -3350.90234375, -125.40234375, 136.59765625, -91.40234375, -71.40234375, 68.59765625, 128.59765625, -2498.822265625, -65.40234375], "mean_td_error": -68.31088256835938, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 146000, "num_agent_steps_sampled": 146000, "num_steps_trained": 1160032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1160032, "last_target_update_ts": 146000, "num_target_updates": 291}, "done": false, "episodes_total": 486, "training_iteration": 146, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-11-00", "timestamp": 1652706660, "time_this_iter_s": 14.673386812210083, "time_total_s": 2051.6198749542236, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2051.6198749542236, "timesteps_since_restore": 4672, "iterations_since_restore": 146, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.945, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 38880.76, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [39526.0, 33948.0, 39504.0, 39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22475685668084963, "mean_inference_ms": 1.9692038485979928, "mean_action_processing_ms": 0.06490225099393399, "mean_env_wait_ms": 1.9633836669606137, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 147000, "timesteps_this_iter": 32, "agent_timesteps_total": 147000, "timers": {"load_time_ms": 0.238, "load_throughput": 134594.593, "learn_time_ms": 36.671, "learn_throughput": 872.624}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.023821499198675156, "mean_q": 12236.3076171875, "min_q": 9153.8056640625, "max_q": 12609.63671875, "cur_lr": 0.6}, "td_error": [54.44140625, 118.44140625, 88.44140625, 118.44140625, 82.44140625, 118.44140625, 88.44140625, -87.55859375, 12307.63671875, -3079.8291015625, 18.44140625, 20.44140625, -2853.78125, 56.44140625, 92.44140625, 118.44140625, -2731.78125, -109.55859375, -85.55859375, -83.55859375, -55.55859375, 48.44140625, -75.55859375, -3401.3896484375, -21.55859375, -79.55859375, -13.55859375, 26.44140625, -109.55859375, -87.55859375, 118.44140625, 12365.63671875], "mean_td_error": 405.1865234375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 147000, "num_agent_steps_sampled": 147000, "num_steps_trained": 1168032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1168032, "last_target_update_ts": 147000, "num_target_updates": 293}, "done": false, "episodes_total": 490, "training_iteration": 147, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-11-14", "timestamp": 1652706674, "time_this_iter_s": 13.568514347076416, "time_total_s": 2065.1883893013, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2065.1883893013, "timesteps_since_restore": 4704, "iterations_since_restore": 147, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.705000000000002, "ram_util_percent": 18.605000000000008}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 39010.74, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [39854.0, 33146.0, 37828.0, 39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22480840033393443, "mean_inference_ms": 1.9698906264500127, "mean_action_processing_ms": 0.06491755242127054, "mean_env_wait_ms": 1.9618384444003976, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 148000, "timesteps_this_iter": 32, "agent_timesteps_total": 148000, "timers": {"load_time_ms": 0.417, "load_throughput": 76673.938, "learn_time_ms": 36.584, "learn_throughput": 874.711}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03236616402864456, "mean_q": 12227.4462890625, "min_q": 9211.095703125, "max_q": 12614.0546875, "cur_lr": 0.6}, "td_error": [129.5029296875, -42.4970703125, 12348.0546875, 129.5029296875, -62.4970703125, -114.4970703125, 57.5029296875, -74.4970703125, -2503.705078125, -92.4970703125, -3367.4560546875, -106.4970703125, -42.4970703125, -30.4970703125, 27.5029296875, 55.5029296875, -120.4970703125, 129.5029296875, 99.5029296875, 127.5029296875, 107.5029296875, -3477.4560546875, -2932.83203125, -76.4970703125, 115.5029296875, -132.4970703125, -86.4970703125, -84.4970703125, -124.4970703125, 12360.0546875, 12316.0546875, 67.5029296875], "mean_td_error": 768.696533203125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 148000, "num_agent_steps_sampled": 148000, "num_steps_trained": 1176032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1176032, "last_target_update_ts": 148000, "num_target_updates": 295}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13181587863898409, "mean_inference_ms": 1.5923863713263005, "mean_action_processing_ms": 0.058503185733031954, "mean_env_wait_ms": 1.2104239144654716, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 493, "training_iteration": 148, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-11-31", "timestamp": 1652706691, "time_this_iter_s": 16.717401266098022, "time_total_s": 2081.905790567398, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2081.905790567398, "timesteps_since_restore": 4736, "iterations_since_restore": 148, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.366666666666664, "ram_util_percent": 18.604166666666668}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 39068.32, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [39470.0, 37096.0, 38128.0, 37348.0, 45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22485557652883578, "mean_inference_ms": 1.970540280504837, "mean_action_processing_ms": 0.06493159758251714, "mean_env_wait_ms": 1.9602849620908367, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 149000, "timesteps_this_iter": 32, "agent_timesteps_total": 149000, "timers": {"load_time_ms": 0.223, "load_throughput": 143196.125, "learn_time_ms": 36.009, "learn_throughput": 888.664}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.01837589219212532, "mean_q": 12618.0166015625, "min_q": 12618.017578125, "max_q": 12618.017578125, "cur_lr": 0.6}, "td_error": [123.1064453125, 125.1064453125, 12366.017578125, -132.8935546875, 63.1064453125, -148.8935546875, 123.1064453125, -98.8935546875, -122.8935546875, -104.8935546875, 127.1064453125, 127.1064453125, 93.1064453125, 127.1064453125, -118.8935546875, 119.1064453125, -108.8935546875, 35.1064453125, 127.1064453125, -118.8935546875, -16.8935546875, 12358.017578125, -20.8935546875, -112.8935546875, 55.1064453125, -90.8935546875, -94.8935546875, -104.8935546875, -90.8935546875, 105.1064453125, 127.1064453125, -100.8935546875], "mean_td_error": 769.1633911132812, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 149000, "num_agent_steps_sampled": 149000, "num_steps_trained": 1184032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1184032, "last_target_update_ts": 149000, "num_target_updates": 297}, "done": false, "episodes_total": 496, "training_iteration": 149, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-11-44", "timestamp": 1652706704, "time_this_iter_s": 13.109623908996582, "time_total_s": 2095.0154144763947, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2095.0154144763947, "timesteps_since_restore": 4768, "iterations_since_restore": 149, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.957894736842103, "ram_util_percent": 18.61052631578948}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 38942.62, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [45338.0, 34882.0, 45934.0, 39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22493187965073638, "mean_inference_ms": 1.9715366415298907, "mean_action_processing_ms": 0.06495457883743629, "mean_env_wait_ms": 1.9583216563910413, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 150000, "timesteps_this_iter": 32, "agent_timesteps_total": 150000, "timers": {"load_time_ms": 0.237, "load_throughput": 134878.633, "learn_time_ms": 35.569, "learn_throughput": 899.658}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0053789024241268635, "mean_q": 12250.7294921875, "min_q": 9239.0302734375, "max_q": 12630.552734375, "cur_lr": 0.6}, "td_error": [-2905.2373046875, 37.4443359375, 131.4443359375, -36.5556640625, -62.5556640625, 73.4443359375, 75.4443359375, 133.4443359375, 113.4443359375, -56.5556640625, -120.5556640625, 131.4443359375, -3358.078125, 77.4443359375, -112.5556640625, -114.5556640625, -3362.078125, -12.5556640625, -70.5556640625, 12374.552734375, 121.4443359375, 133.4443359375, -78.5556640625, -98.5556640625, -2497.212890625, -104.5556640625, 12374.552734375, -14.5556640625, 39.4443359375, 69.4443359375, 19.4443359375, 59.4443359375], "mean_td_error": 405.0016174316406, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 150000, "num_agent_steps_sampled": 150000, "num_steps_trained": 1192032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1192032, "last_target_update_ts": 150000, "num_target_updates": 299}, "done": false, "episodes_total": 500, "training_iteration": 150, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-11-58", "timestamp": 1652706718, "time_this_iter_s": 13.96930718421936, "time_total_s": 2108.984721660614, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2108.984721660614, "timesteps_since_restore": 4800, "iterations_since_restore": 150, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.134999999999998, "ram_util_percent": 18.610000000000007}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 38769.68, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [39816.0, 45724.0, 31766.0, 39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.224991793751113, "mean_inference_ms": 1.9722989186811677, "mean_action_processing_ms": 0.06497242680593662, "mean_env_wait_ms": 1.9568666875795016, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 151000, "timesteps_this_iter": 32, "agent_timesteps_total": 151000, "timers": {"load_time_ms": 0.157, "load_throughput": 203483.517, "learn_time_ms": 23.953, "learn_throughput": 1335.945}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.06806517392396927, "mean_q": 12524.826171875, "min_q": 9999.158203125, "max_q": 12606.2998046875, "cur_lr": 0.6}, "td_error": [-103.486328125, 50.513671875, -87.486328125, -97.486328125, -79.486328125, -143.486328125, 20.513671875, 76.513671875, -113.486328125, -29.486328125, -75.486328125, -151.486328125, 36.513671875, 50.513671875, -2716.6279296875, -111.486328125, -125.486328125, 0.513671875, -83.486328125, -87.486328125, 94.513671875, 82.513671875, -129.486328125, 114.513671875, -123.486328125, 98.513671875, 116.513671875, -33.486328125, 12336.2998046875, -127.486328125, -23.486328125, -137.486328125], "mean_td_error": 265.5338134765625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 151000, "num_agent_steps_sampled": 151000, "num_steps_trained": 1200032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1200032, "last_target_update_ts": 151000, "num_target_updates": 301}, "done": false, "episodes_total": 503, "training_iteration": 151, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-12-12", "timestamp": 1652706732, "time_this_iter_s": 14.494789600372314, "time_total_s": 2123.4795112609863, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2123.4795112609863, "timesteps_since_restore": 4832, "iterations_since_restore": 151, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.560000000000002, "ram_util_percent": 18.63}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 38761.1, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [39046.0, 32382.0, 35804.0, 48476.0, 39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22503806957722877, "mean_inference_ms": 1.9729504736500323, "mean_action_processing_ms": 0.06498633331615221, "mean_env_wait_ms": 1.9553594472440818, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 152000, "timesteps_this_iter": 32, "agent_timesteps_total": 152000, "timers": {"load_time_ms": 0.129, "load_throughput": 247725.596, "learn_time_ms": 20.004, "learn_throughput": 1599.696}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.022766204550862312, "mean_q": 12526.6513671875, "min_q": 9766.0595703125, "max_q": 12615.701171875, "cur_lr": 0.6}, "td_error": [-113.0087890625, 12351.701171875, 58.9912109375, -123.0087890625, 120.9912109375, -73.0087890625, 52.9912109375, 12371.701171875, 68.9912109375, -123.0087890625, 12349.701171875, -15.0087890625, 120.9912109375, -107.0087890625, 126.9912109375, -127.0087890625, -2988.650390625, -13.0087890625, -107.0087890625, 126.9912109375, -71.0087890625, 128.9912109375, -113.0087890625, 26.9912109375, -23.0087890625, -127.0087890625, 94.9912109375, 62.9912109375, -69.0087890625, 126.9912109375, -43.0087890625, 12355.701171875], "mean_td_error": 1447.216064453125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 152000, "num_agent_steps_sampled": 152000, "num_steps_trained": 1208032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1208032, "last_target_update_ts": 152000, "num_target_updates": 303}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13053671287534613, "mean_inference_ms": 1.5749273859778055, "mean_action_processing_ms": 0.05785133449479829, "mean_env_wait_ms": 1.2006913765555212, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 506, "training_iteration": 152, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-12-26", "timestamp": 1652706746, "time_this_iter_s": 13.046060562133789, "time_total_s": 2136.52557182312, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2136.52557182312, "timesteps_since_restore": 4864, "iterations_since_restore": 152, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.84210526315789, "ram_util_percent": 18.657894736842106}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 38807.56, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [39434.0, 35976.0, 35096.0, 33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22508587435997204, "mean_inference_ms": 1.973682915539328, "mean_action_processing_ms": 0.06500081381663404, "mean_env_wait_ms": 1.9533027709904325, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 153000, "timesteps_this_iter": 32, "agent_timesteps_total": 153000, "timers": {"load_time_ms": 0.127, "load_throughput": 251579.621, "learn_time_ms": 19.503, "learn_throughput": 1640.737}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.016766119748353958, "mean_q": 12028.841796875, "min_q": 9344.576171875, "max_q": 12613.755859375, "cur_lr": 0.6}, "td_error": [118.3193359375, -2461.4130859375, 12355.755859375, -117.6806640625, 96.3193359375, 36.3193359375, -2177.88671875, -137.6806640625, -115.6806640625, -177.6806640625, 58.3193359375, -2539.05859375, -135.6806640625, -7.6806640625, 104.3193359375, -153.6806640625, -141.6806640625, 116.3193359375, 118.3193359375, -99.6806640625, -2894.0576171875, 44.3193359375, 18.3193359375, 118.3193359375, -123.6806640625, 52.3193359375, -3180.8603515625, -77.6806640625, 116.3193359375, -2511.05859375, -15.6806640625, -2726.705078125], "mean_td_error": -201.30062866210938, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 153000, "num_agent_steps_sampled": 153000, "num_steps_trained": 1216032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1216032, "last_target_update_ts": 153000, "num_target_updates": 305}, "done": false, "episodes_total": 510, "training_iteration": 153, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-12-36", "timestamp": 1652706756, "time_this_iter_s": 9.948815822601318, "time_total_s": 2146.4743876457214, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2146.4743876457214, "timesteps_since_restore": 4896, "iterations_since_restore": 153, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.64285714285714, "ram_util_percent": 18.62142857142857}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 38931.34, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [33148.0, 41586.0, 35552.0, 33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22511875474905138, "mean_inference_ms": 1.9741898735067036, "mean_action_processing_ms": 0.06500959835062221, "mean_env_wait_ms": 1.9517452661843004, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 154000, "timesteps_this_iter": 32, "agent_timesteps_total": 154000, "timers": {"load_time_ms": 0.232, "load_throughput": 137927.991, "learn_time_ms": 36.64, "learn_throughput": 873.362}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.007422588299959898, "mean_q": 12283.560546875, "min_q": 9369.6142578125, "max_q": 12609.2353515625, "cur_lr": 0.6}, "td_error": [126.716796875, 36.716796875, -103.283203125, -2622.26171875, 16.716796875, -17.283203125, -165.283203125, 126.716796875, -93.283203125, -73.283203125, 52.716796875, -17.283203125, 68.716796875, 70.716796875, -2604.26171875, 126.716796875, -53.283203125, -2063.2900390625, 128.716796875, 124.716796875, 110.716796875, 36.716796875, -111.283203125, -101.283203125, -15.283203125, -91.283203125, -91.283203125, -3170.904296875, 120.716796875, -67.283203125, 32.716796875, 128.716796875], "mean_td_error": -317.2702331542969, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 154000, "num_agent_steps_sampled": 154000, "num_steps_trained": 1224032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1224032, "last_target_update_ts": 154000, "num_target_updates": 307}, "done": false, "episodes_total": 513, "training_iteration": 154, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-12-49", "timestamp": 1652706769, "time_this_iter_s": 13.923329591751099, "time_total_s": 2160.3977172374725, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2160.3977172374725, "timesteps_since_restore": 4928, "iterations_since_restore": 154, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.305000000000003, "ram_util_percent": 18.610000000000007}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30108.0, "episode_reward_mean": 38954.84, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [33814.0, 30108.0, 40750.0, 39468.0, 35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2251541692365409, "mean_inference_ms": 1.9747181094101416, "mean_action_processing_ms": 0.06501897495244782, "mean_env_wait_ms": 1.9502441691398358, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 155000, "timesteps_this_iter": 32, "agent_timesteps_total": 155000, "timers": {"load_time_ms": 0.231, "load_throughput": 138383.058, "learn_time_ms": 36.224, "learn_throughput": 883.401}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.00967054907232523, "mean_q": 12205.212890625, "min_q": 9378.55859375, "max_q": 12616.732421875, "cur_lr": 0.6}, "td_error": [73.34375, -1998.6181640625, 113.34375, -8.65625, -104.65625, 135.34375, -8.65625, 69.34375, -102.65625, 27.34375, 123.34375, 133.34375, 95.34375, 129.34375, -12.65625, 131.34375, -114.65625, -92.65625, -2637.7470703125, -2632.0732421875, -104.65625, -34.65625, -90.65625, 101.34375, -62.65625, -98.65625, 75.34375, -112.65625, -2366.6650390625, -3330.830078125, 127.34375, 12368.732421875], "mean_td_error": -6.57073974609375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 155000, "num_agent_steps_sampled": 155000, "num_steps_trained": 1232032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1232032, "last_target_update_ts": 155000, "num_target_updates": 309}, "done": false, "episodes_total": 516, "training_iteration": 155, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-13-05", "timestamp": 1652706785, "time_this_iter_s": 15.696361780166626, "time_total_s": 2176.094079017639, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2176.094079017639, "timesteps_since_restore": 4960, "iterations_since_restore": 155, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.236363636363635, "ram_util_percent": 18.640909090909087}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39146.82, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [35358.0, 36050.0, 38562.0, 36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22520915690340576, "mean_inference_ms": 1.9754894984175817, "mean_action_processing_ms": 0.06503359744990286, "mean_env_wait_ms": 1.9483226342060112, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 156000, "timesteps_this_iter": 32, "agent_timesteps_total": 156000, "timers": {"load_time_ms": 0.257, "load_throughput": 124529.345, "learn_time_ms": 37.323, "learn_throughput": 857.376}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03402550518512726, "mean_q": 12569.3603515625, "min_q": 10535.67578125, "max_q": 12634.9638671875, "cur_lr": 0.6}, "td_error": [132.970703125, -75.029296875, -105.029296875, 62.970703125, 74.970703125, -115.029296875, 132.970703125, 132.970703125, 130.970703125, 128.970703125, 14.970703125, 126.970703125, -87.029296875, -129.029296875, 36.970703125, 132.970703125, 72.970703125, 132.970703125, -131.029296875, 36.970703125, 132.970703125, 132.970703125, -107.029296875, 132.970703125, 70.970703125, -17.029296875, 132.970703125, -19.029296875, 120.970703125, -71.029296875, -2034.3173828125, -65.029296875], "mean_td_error": -27.507049560546875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 156000, "num_agent_steps_sampled": 156000, "num_steps_trained": 1240032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1240032, "last_target_update_ts": 156000, "num_target_updates": 311}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.1308205046008646, "mean_inference_ms": 1.57925064019103, "mean_action_processing_ms": 0.05797734271562076, "mean_env_wait_ms": 1.2045985928418015, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 520, "training_iteration": 156, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-13-22", "timestamp": 1652706802, "time_this_iter_s": 16.29414653778076, "time_total_s": 2192.38822555542, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2192.38822555542, "timesteps_since_restore": 4992, "iterations_since_restore": 156, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.395652173913046, "ram_util_percent": 18.65217391304348}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39249.16, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36174.0, 41546.0, 42008.0, 34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22526110361348814, "mean_inference_ms": 1.9761628245101384, "mean_action_processing_ms": 0.06504764948007949, "mean_env_wait_ms": 1.9469601033385369, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 157000, "timesteps_this_iter": 32, "agent_timesteps_total": 157000, "timers": {"load_time_ms": 0.235, "load_throughput": 136220.164, "learn_time_ms": 34.486, "learn_throughput": 927.919}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.040448617190122604, "mean_q": 12467.712890625, "min_q": 9443.66796875, "max_q": 12631.91015625, "cur_lr": 0.6}, "td_error": [-88.5048828125, 119.4951171875, -84.5048828125, -136.5048828125, -3176.7470703125, -76.5048828125, -2004.607421875, -148.5048828125, -120.5048828125, -118.5048828125, -108.5048828125, 53.4951171875, -126.5048828125, -84.5048828125, 111.4951171875, 12377.91015625, 12359.91015625, -152.5048828125, -8.5048828125, 115.4951171875, -126.5048828125, 119.4951171875, -130.5048828125, 119.4951171875, 12361.91015625, 45.4951171875, -164.5048828125, 119.4951171875, -128.5048828125, -118.5048828125, -86.5048828125, -116.5048828125], "mean_td_error": 956.148193359375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 157000, "num_agent_steps_sampled": 157000, "num_steps_trained": 1248032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1248032, "last_target_update_ts": 157000, "num_target_updates": 313}, "done": false, "episodes_total": 523, "training_iteration": 157, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-13-37", "timestamp": 1652706817, "time_this_iter_s": 15.144768953323364, "time_total_s": 2207.5329945087433, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2207.5329945087433, "timesteps_since_restore": 5024, "iterations_since_restore": 157, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.831818181818186, "ram_util_percent": 18.64545454545455}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39137.4, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [34944.0, 38734.0, 40000.0, 34852.0, 45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.225323550190927, "mean_inference_ms": 1.9768023462975506, "mean_action_processing_ms": 0.06506526061777695, "mean_env_wait_ms": 1.9456383249385636, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 158000, "timesteps_this_iter": 32, "agent_timesteps_total": 158000, "timers": {"load_time_ms": 0.261, "load_throughput": 122562.075, "learn_time_ms": 38.459, "learn_throughput": 832.065}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.008717663586139679, "mean_q": 12291.11328125, "min_q": 9469.9365234375, "max_q": 12628.1455078125, "cur_lr": 0.6}, "td_error": [-87.4521484375, -137.4521484375, 12380.1455078125, 10.5478515625, -83.4521484375, -117.4521484375, -35.4521484375, -151.4521484375, -2598.609375, 116.5478515625, 12380.1455078125, 116.5478515625, 42.5478515625, -87.4521484375, -83.4521484375, -167.4521484375, 116.5478515625, -25.4521484375, 12364.1455078125, -131.4521484375, -101.4521484375, 48.5478515625, -3075.6611328125, -119.4521484375, 118.5478515625, 48.5478515625, 118.5478515625, -2406.939453125, 118.5478515625, -81.4521484375, 58.5478515625, -2663.6240234375], "mean_td_error": 808.8530883789062, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 158000, "num_agent_steps_sampled": 158000, "num_steps_trained": 1256032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1256032, "last_target_update_ts": 158000, "num_target_updates": 315}, "done": false, "episodes_total": 526, "training_iteration": 158, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-13-53", "timestamp": 1652706833, "time_this_iter_s": 16.321290016174316, "time_total_s": 2223.8542845249176, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2223.8542845249176, "timesteps_since_restore": 5056, "iterations_since_restore": 158, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.178260869565214, "ram_util_percent": 18.60000000000001}}
{"episode_reward_max": 49564.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39193.32, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [45396.0, 41394.0, 39932.0, 37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22540501472851504, "mean_inference_ms": 1.9776251969549088, "mean_action_processing_ms": 0.0650878510759143, "mean_env_wait_ms": 1.9439131716075189, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 159000, "timesteps_this_iter": 32, "agent_timesteps_total": 159000, "timers": {"load_time_ms": 0.13, "load_throughput": 246316.256, "learn_time_ms": 20.217, "learn_throughput": 1582.803}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.043056562542915344, "mean_q": 12334.6064453125, "min_q": 10015.74609375, "max_q": 12641.16796875, "cur_lr": 0.6}, "td_error": [-22.447265625, 103.552734375, -66.447265625, 117.552734375, 12393.16796875, -70.447265625, -122.447265625, 35.552734375, 17.552734375, 12381.16796875, -76.447265625, -2647.869140625, 93.552734375, -120.447265625, -78.447265625, -2338.44921875, -126.447265625, 45.552734375, -28.447265625, -2356.978515625, -76.447265625, -116.447265625, -116.447265625, 93.552734375, -78.447265625, -68.447265625, 12351.16796875, -2476.44921875, -76.447265625, -116.447265625, 123.552734375, -120.447265625], "mean_td_error": 826.7055053710938, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 159000, "num_agent_steps_sampled": 159000, "num_steps_trained": 1264032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1264032, "last_target_update_ts": 159000, "num_target_updates": 317}, "done": false, "episodes_total": 530, "training_iteration": 159, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-14-08", "timestamp": 1652706848, "time_this_iter_s": 15.178444385528564, "time_total_s": 2239.032728910446, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2239.032728910446, "timesteps_since_restore": 5088, "iterations_since_restore": 159, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.33636363636364, "ram_util_percent": 18.60000000000001}}
{"episode_reward_max": 55942.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39315.6, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37994.0, 31696.0, 38920.0, 33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22545786685348054, "mean_inference_ms": 1.9781655886424099, "mean_action_processing_ms": 0.06510231037792134, "mean_env_wait_ms": 1.9426232277241313, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 160000, "timesteps_this_iter": 32, "agent_timesteps_total": 160000, "timers": {"load_time_ms": 0.262, "load_throughput": 122138.255, "learn_time_ms": 39.678, "learn_throughput": 806.484}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.060116831213235855, "mean_q": 12558.349609375, "min_q": 10046.322265625, "max_q": 12639.3828125, "cur_lr": 0.6}, "td_error": [-2666.103515625, -23.04296875, 12395.3828125, 30.95703125, -77.04296875, -135.04296875, -125.04296875, -145.04296875, -21.04296875, -77.04296875, -105.04296875, 122.95703125, 114.95703125, -73.04296875, 18.95703125, 122.95703125, 12385.3828125, -77.04296875, -159.04296875, -119.04296875, -77.04296875, -119.04296875, 48.95703125, -139.04296875, -19.04296875, -151.04296875, 122.95703125, -55.04296875, 120.95703125, 58.95703125, 76.95703125, -125.04296875], "mean_td_error": 660.3880004882812, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 160000, "num_agent_steps_sampled": 160000, "num_steps_trained": 1272032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1272032, "last_target_update_ts": 160000, "num_target_updates": 319}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13135079691463902, "mean_inference_ms": 1.5891851877015923, "mean_action_processing_ms": 0.05832112597362288, "mean_env_wait_ms": 1.2100856211391868, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 533, "training_iteration": 160, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-14-23", "timestamp": 1652706863, "time_this_iter_s": 14.922326564788818, "time_total_s": 2253.955055475235, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2253.955055475235, "timesteps_since_restore": 5120, "iterations_since_restore": 160, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.938095238095237, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39608.44, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [33560.0, 41648.0, 33368.0, 32038.0, 42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22550968697863322, "mean_inference_ms": 1.9787059329111738, "mean_action_processing_ms": 0.06511693889315659, "mean_env_wait_ms": 1.9413810179982844, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 161000, "timesteps_this_iter": 32, "agent_timesteps_total": 161000, "timers": {"load_time_ms": 0.23, "load_throughput": 139201.128, "learn_time_ms": 36.032, "learn_throughput": 888.109}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.060983773320913315, "mean_q": 12355.935546875, "min_q": 10134.16796875, "max_q": 12657.5068359375, "cur_lr": 0.6}, "td_error": [-2606.4501953125, 136.888671875, 136.888671875, 130.888671875, -91.111328125, 136.888671875, -103.111328125, -67.111328125, 104.888671875, 138.888671875, 72.888671875, -2415.443359375, 66.888671875, 12381.5068359375, 72.888671875, -69.111328125, 138.888671875, -59.111328125, -89.111328125, 96.888671875, 12365.5068359375, -135.111328125, -89.111328125, 12367.5068359375, 138.888671875, -2265.421875, -89.111328125, 44.888671875, -107.111328125, -2299.421875, 28.888671875, 138.888671875], "mean_td_error": 881.6875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 161000, "num_agent_steps_sampled": 161000, "num_steps_trained": 1280032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1280032, "last_target_update_ts": 161000, "num_target_updates": 321}, "done": false, "episodes_total": 536, "training_iteration": 161, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-14-40", "timestamp": 1652706880, "time_this_iter_s": 16.065044164657593, "time_total_s": 2270.0200996398926, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2270.0200996398926, "timesteps_since_restore": 5152, "iterations_since_restore": 161, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.8, "ram_util_percent": 18.60000000000001}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39727.22, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [42618.0, 41550.0, 39746.0, 38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2256122180476502, "mean_inference_ms": 1.9797373373106035, "mean_action_processing_ms": 0.06514653802618939, "mean_env_wait_ms": 1.9399909163785145, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 162000, "timesteps_this_iter": 32, "agent_timesteps_total": 162000, "timers": {"load_time_ms": 0.262, "load_throughput": 122193.853, "learn_time_ms": 42.245, "learn_throughput": 757.486}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.005537639372050762, "mean_q": 12335.392578125, "min_q": 9616.96875, "max_q": 12653.74609375, "cur_lr": 0.6}, "td_error": [-41.294921875, 128.705078125, -59.294921875, 114.705078125, 128.705078125, -143.294921875, -2218.0146484375, -109.294921875, 90.705078125, 102.705078125, -19.294921875, -2415.6640625, 12381.74609375, 130.705078125, -15.294921875, 12397.74609375, 14.705078125, -101.294921875, 70.705078125, -2422.7705078125, -115.294921875, -75.294921875, 132.705078125, -69.294921875, -3068.072265625, -91.294921875, 114.705078125, 132.705078125, 80.705078125, -149.294921875, -71.294921875, 12375.74609375], "mean_td_error": 850.3857421875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 162000, "num_agent_steps_sampled": 162000, "num_steps_trained": 1288032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1288032, "last_target_update_ts": 162000, "num_target_updates": 323}, "done": false, "episodes_total": 540, "training_iteration": 162, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-14-56", "timestamp": 1652706896, "time_this_iter_s": 16.260295152664185, "time_total_s": 2286.2803947925568, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2286.2803947925568, "timesteps_since_restore": 5184, "iterations_since_restore": 162, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.5125, "ram_util_percent": 18.6}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39678.42, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [38484.0, 38434.0, 38584.0, 42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22568720148446733, "mean_inference_ms": 1.9805047595922378, "mean_action_processing_ms": 0.06516813552481017, "mean_env_wait_ms": 1.938963740010951, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 163000, "timesteps_this_iter": 32, "agent_timesteps_total": 163000, "timers": {"load_time_ms": 0.132, "load_throughput": 242445.318, "learn_time_ms": 20.902, "learn_throughput": 1530.922}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03421338275074959, "mean_q": 12407.748046875, "min_q": 9647.521484375, "max_q": 12649.1982421875, "cur_lr": 0.6}, "td_error": [-78.4833984375, -102.4833984375, 33.5166015625, 133.5166015625, -76.4833984375, 135.5166015625, 135.5166015625, -108.4833984375, -70.4833984375, -104.4833984375, -2.4833984375, -100.4833984375, 135.5166015625, -2365.19140625, -114.4833984375, 69.5166015625, 31.5166015625, -102.4833984375, -78.4833984375, 95.5166015625, 12401.1982421875, -136.4833984375, -2524.5400390625, 43.5166015625, -108.4833984375, -98.4833984375, -90.4833984375, -124.4833984375, 49.5166015625, 135.5166015625, -134.4833984375, -2896.16015625], "mean_td_error": 124.43035888671875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 163000, "num_agent_steps_sampled": 163000, "num_steps_trained": 1296032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1296032, "last_target_update_ts": 163000, "num_target_updates": 325}, "done": false, "episodes_total": 543, "training_iteration": 163, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-15-11", "timestamp": 1652706911, "time_this_iter_s": 15.162817239761353, "time_total_s": 2301.443212032318, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2301.443212032318, "timesteps_since_restore": 5216, "iterations_since_restore": 163, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.123809523809523, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39758.24, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [42112.0, 45546.0, 40394.0, 49564.0, 42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22575461819058776, "mean_inference_ms": 1.9811869163526148, "mean_action_processing_ms": 0.06518703759448838, "mean_env_wait_ms": 1.937918603358359, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 164000, "timesteps_this_iter": 32, "agent_timesteps_total": 164000, "timers": {"load_time_ms": 0.135, "load_throughput": 236715.57, "learn_time_ms": 20.536, "learn_throughput": 1558.243}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.01782127469778061, "mean_q": 12324.015625, "min_q": 9688.529296875, "max_q": 12659.220703125, "cur_lr": 0.6}, "td_error": [-110.9853515625, 97.0146484375, -2288.5400390625, -108.9853515625, -68.9853515625, 65.0146484375, 27.0146484375, -2408.583984375, 12361.220703125, -2939.6767578125, 111.0146484375, -80.9853515625, -76.9853515625, 12409.220703125, -100.9853515625, 127.0146484375, 129.0146484375, -114.9853515625, -20.9853515625, 121.0146484375, -84.9853515625, -130.9853515625, -2935.6767578125, -24.9853515625, -78.9853515625, -146.9853515625, 99.0146484375, 129.0146484375, 49.0146484375, 81.0146484375, 12385.220703125, -62.9853515625], "mean_td_error": 825.1422119140625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 164000, "num_agent_steps_sampled": 164000, "num_steps_trained": 1304032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1304032, "last_target_update_ts": 164000, "num_target_updates": 327}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12999014702863534, "mean_inference_ms": 1.573122029342881, "mean_action_processing_ms": 0.05772470235921495, "mean_env_wait_ms": 1.1988514739801686, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 546, "training_iteration": 164, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-15-25", "timestamp": 1652706925, "time_this_iter_s": 13.657276391983032, "time_total_s": 2315.100488424301, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2315.100488424301, "timesteps_since_restore": 5248, "iterations_since_restore": 164, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.165, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39532.56, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [42124.0, 46326.0, 36134.0, 35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22584455013532434, "mean_inference_ms": 1.9820737713198655, "mean_action_processing_ms": 0.06521192957456493, "mean_env_wait_ms": 1.9364881750906724, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 165000, "timesteps_this_iter": 32, "agent_timesteps_total": 165000, "timers": {"load_time_ms": 0.225, "load_throughput": 142164.737, "learn_time_ms": 35.566, "learn_throughput": 899.734}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.039336394518613815, "mean_q": 12406.2666015625, "min_q": 9691.1796875, "max_q": 12660.2431640625, "cur_lr": 0.6}, "td_error": [12396.2431640625, -110.79296875, 12388.2431640625, -3119.8564453125, -76.79296875, -12.79296875, 129.20703125, 127.20703125, 23.20703125, 125.20703125, -2129.669921875, -12.79296875, -114.79296875, 57.20703125, -82.79296875, 89.20703125, 12390.2431640625, -84.79296875, -3036.0751953125, -88.79296875, 12384.2431640625, -124.79296875, 111.20703125, -106.79296875, 53.20703125, 123.20703125, -122.79296875, 91.20703125, -118.79296875, -96.79296875, -112.79296875, -68.79296875], "mean_td_error": 1277.11083984375, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 165000, "num_agent_steps_sampled": 165000, "num_steps_trained": 1312032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1312032, "last_target_update_ts": 165000, "num_target_updates": 329}, "done": false, "episodes_total": 550, "training_iteration": 165, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-15-39", "timestamp": 1652706939, "time_this_iter_s": 14.169258832931519, "time_total_s": 2329.2697472572327, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2329.2697472572327, "timesteps_since_restore": 5280, "iterations_since_restore": 165, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.445, "ram_util_percent": 18.610000000000007}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39549.14, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [35578.0, 42806.0, 40192.0, 37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22590690821454956, "mean_inference_ms": 1.9826901256447633, "mean_action_processing_ms": 0.06523014860530711, "mean_env_wait_ms": 1.9353848066988097, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 166000, "timesteps_this_iter": 32, "agent_timesteps_total": 166000, "timers": {"load_time_ms": 0.126, "load_throughput": 253911.706, "learn_time_ms": 19.823, "learn_throughput": 1614.302}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.005393545608967543, "mean_q": 12509.08984375, "min_q": 9754.3232421875, "max_q": 12676.1171875, "cur_lr": 0.6}, "td_error": [72.439453125, 136.439453125, 140.439453125, -59.560546875, -2945.3544921875, -113.560546875, -61.560546875, 62.439453125, -2530.625, -69.560546875, 74.439453125, 136.439453125, -5.560546875, -63.560546875, 82.439453125, 114.439453125, -87.560546875, -61.560546875, -99.560546875, -99.560546875, -59.560546875, -107.560546875, 140.439453125, 140.439453125, 78.439453125, 140.439453125, -67.560546875, -43.560546875, 106.439453125, 140.439453125, -89.560546875, 74.439453125], "mean_td_error": -153.89987182617188, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 166000, "num_agent_steps_sampled": 166000, "num_steps_trained": 1320032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1320032, "last_target_update_ts": 166000, "num_target_updates": 331}, "done": false, "episodes_total": 553, "training_iteration": 166, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-15-52", "timestamp": 1652706952, "time_this_iter_s": 12.934303522109985, "time_total_s": 2342.2040507793427, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2342.2040507793427, "timesteps_since_restore": 5312, "iterations_since_restore": 166, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.452631578947365, "ram_util_percent": 18.605263157894743}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39502.38, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37366.0, 35504.0, 33606.0, 41734.0, 38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22596971654125322, "mean_inference_ms": 1.9833117288293856, "mean_action_processing_ms": 0.06524816975247058, "mean_env_wait_ms": 1.934308027684048, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 167000, "timesteps_this_iter": 32, "agent_timesteps_total": 167000, "timers": {"load_time_ms": 0.226, "load_throughput": 141609.757, "learn_time_ms": 34.864, "learn_throughput": 917.85}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.002969936700537801, "mean_q": 12580.662109375, "min_q": 9747.67578125, "max_q": 12672.048828125, "cur_lr": 0.6}, "td_error": [-105.1298828125, -73.1298828125, -95.1298828125, -113.1298828125, -125.1298828125, 120.8701171875, 74.8701171875, 94.8701171875, -129.1298828125, 62.8701171875, -99.1298828125, -73.1298828125, 126.8701171875, 128.8701171875, 12394.048828125, -97.1298828125, -81.1298828125, -111.1298828125, 104.8701171875, -83.1298828125, 12420.048828125, -111.1298828125, -147.1298828125, -97.1298828125, 128.8701171875, 52.8701171875, 62.8701171875, -2833.5029296875, 16.8701171875, -111.1298828125, 128.8701171875, 32.8701171875], "mean_td_error": 670.80712890625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 167000, "num_agent_steps_sampled": 167000, "num_steps_trained": 1328032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1328032, "last_target_update_ts": 167000, "num_target_updates": 333}, "done": false, "episodes_total": 556, "training_iteration": 167, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-16-07", "timestamp": 1652706967, "time_this_iter_s": 14.616081953048706, "time_total_s": 2356.8201327323914, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2356.8201327323914, "timesteps_since_restore": 5344, "iterations_since_restore": 167, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.314285714285717, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39713.86, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [38752.0, 36510.0, 39732.0, 44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22606423383086566, "mean_inference_ms": 1.984234331656613, "mean_action_processing_ms": 0.06527558245921959, "mean_env_wait_ms": 1.9330126540323602, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 168000, "timesteps_this_iter": 32, "agent_timesteps_total": 168000, "timers": {"load_time_ms": 0.277, "load_throughput": 115615.236, "learn_time_ms": 38.829, "learn_throughput": 824.128}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.006341723259538412, "mean_q": 12379.6328125, "min_q": 9813.2607421875, "max_q": 12678.3349609375, "cur_lr": 0.6}, "td_error": [30.5625, 14.5625, -2258.263671875, 120.5625, -19.4375, -119.4375, -109.4375, 52.5625, 100.5625, -71.4375, 62.5625, 118.5625, -119.4375, -73.4375, -121.4375, 58.5625, 58.5625, -147.4375, 86.5625, 32.5625, 92.5625, -117.4375, -25.4375, -2744.51171875, -135.4375, -2226.345703125, 60.5625, -2347.09375, -145.4375, -91.4375, 116.5625, 120.5625], "mean_td_error": -304.5770263671875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 168000, "num_agent_steps_sampled": 168000, "num_steps_trained": 1336032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1336032, "last_target_update_ts": 168000, "num_target_updates": 335}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13070210191278492, "mean_inference_ms": 1.5825910252641566, "mean_action_processing_ms": 0.05801338381676832, "mean_env_wait_ms": 1.2045127798116317, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 560, "training_iteration": 168, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-16-23", "timestamp": 1652706983, "time_this_iter_s": 16.564318895339966, "time_total_s": 2373.3844516277313, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2373.3844516277313, "timesteps_since_restore": 5376, "iterations_since_restore": 168, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.869565217391305, "ram_util_percent": 18.604347826086965}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39751.22, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [44910.0, 31492.0, 40628.0, 38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2261253031376982, "mean_inference_ms": 1.9849117884639935, "mean_action_processing_ms": 0.06529352987505313, "mean_env_wait_ms": 1.9319952670968084, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 169000, "timesteps_this_iter": 32, "agent_timesteps_total": 169000, "timers": {"load_time_ms": 0.253, "load_throughput": 126310.679, "learn_time_ms": 37.994, "learn_throughput": 842.248}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.02475973404943943, "mean_q": 12379.8935546875, "min_q": 9853.1669921875, "max_q": 12703.029296875, "cur_lr": 0.6}, "td_error": [-2940.021484375, -130.1591796875, -136.1591796875, -104.1591796875, 105.8408203125, -74.1591796875, -106.1591796875, -2946.021484375, -72.1591796875, -124.1591796875, -66.1591796875, -130.1591796875, 39.8408203125, 133.8408203125, 79.8408203125, 103.8408203125, -2910.021484375, -68.1591796875, -102.1591796875, 79.8408203125, -128.1591796875, 129.8408203125, -128.1591796875, -112.1591796875, 119.8408203125, 129.8408203125, -1770.947265625, 133.8408203125, 137.8408203125, -138.1591796875, -86.1591796875, 12425.029296875], "mean_td_error": 42.053741455078125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 169000, "num_agent_steps_sampled": 169000, "num_steps_trained": 1344032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1344032, "last_target_update_ts": 169000, "num_target_updates": 337}, "done": false, "episodes_total": 563, "training_iteration": 169, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-16-37", "timestamp": 1652706997, "time_this_iter_s": 13.332470655441284, "time_total_s": 2386.7169222831726, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2386.7169222831726, "timesteps_since_restore": 5408, "iterations_since_restore": 169, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 24.084210526315786, "ram_util_percent": 18.605263157894743}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39635.52, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [38472.0, 41036.0, 33576.0, 45380.0, 33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2261842090179107, "mean_inference_ms": 1.9855720435784228, "mean_action_processing_ms": 0.06531103127436222, "mean_env_wait_ms": 1.9309688694114815, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 170000, "timesteps_this_iter": 32, "agent_timesteps_total": 170000, "timers": {"load_time_ms": 0.253, "load_throughput": 126417.753, "learn_time_ms": 38.15, "learn_throughput": 838.801}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.01941814087331295, "mean_q": 12426.14453125, "min_q": 9860.6357421875, "max_q": 12721.44921875, "cur_lr": 0.6}, "td_error": [75.3232421875, -2039.71875, -112.6767578125, -96.6767578125, 47.3232421875, 12457.44921875, -140.6767578125, -118.6767578125, -2203.912109375, 69.3232421875, 109.3232421875, -66.6767578125, -122.6767578125, 12469.44921875, -6.6767578125, 137.3232421875, 107.3232421875, 69.3232421875, 79.3232421875, -134.6767578125, -40.6767578125, -118.6767578125, 33.3232421875, 129.3232421875, 129.3232421875, 12447.44921875, -112.6767578125, -2723.490234375, -2435.3662109375, 137.3232421875, -130.6767578125, 129.3232421875], "mean_td_error": 875.7169799804688, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 170000, "num_agent_steps_sampled": 170000, "num_steps_trained": 1352032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1352032, "last_target_update_ts": 170000, "num_target_updates": 339}, "done": false, "episodes_total": 566, "training_iteration": 170, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-16-50", "timestamp": 1652707010, "time_this_iter_s": 12.908912181854248, "time_total_s": 2399.625834465027, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2399.625834465027, "timesteps_since_restore": 5440, "iterations_since_restore": 170, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.147368421052633, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39670.02, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [33746.0, 35456.0, 36516.0, 38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22626956693859657, "mean_inference_ms": 1.9865107268760491, "mean_action_processing_ms": 0.0653363107455074, "mean_env_wait_ms": 1.9296271771311668, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 171000, "timesteps_this_iter": 32, "agent_timesteps_total": 171000, "timers": {"load_time_ms": 0.302, "load_throughput": 105975.308, "learn_time_ms": 41.819, "learn_throughput": 765.208}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0038594966754317284, "mean_q": 12601.65234375, "min_q": 10677.197265625, "max_q": 12729.94921875, "cur_lr": 0.6}, "td_error": [-73.2158203125, 122.7841796875, -129.2158203125, -75.2158203125, 70.7841796875, -23.2158203125, -77.2158203125, 24.7841796875, -2019.9677734375, 124.7841796875, 124.7841796875, 18.7841796875, 94.7841796875, 66.7841796875, -113.2158203125, 124.7841796875, 122.7841796875, 124.7841796875, 92.7841796875, 94.7841796875, -45.2158203125, -2139.9677734375, -103.2158203125, 124.7841796875, 84.7841796875, -23.2158203125, 12483.94921875, -75.2158203125, -63.2158203125, 60.7841796875, -79.2158203125, 12433.94921875], "mean_td_error": 667.3724975585938, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 171000, "num_agent_steps_sampled": 171000, "num_steps_trained": 1360032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1360032, "last_target_update_ts": 171000, "num_target_updates": 341}, "done": false, "episodes_total": 570, "training_iteration": 171, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-17-05", "timestamp": 1652707025, "time_this_iter_s": 15.481477737426758, "time_total_s": 2415.1073122024536, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2415.1073122024536, "timesteps_since_restore": 5472, "iterations_since_restore": 171, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.363636363636363, "ram_util_percent": 18.60000000000001}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39715.8, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [38958.0, 42028.0, 35612.0, 40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22633409242467958, "mean_inference_ms": 1.9872056713743311, "mean_action_processing_ms": 0.06535501010614864, "mean_env_wait_ms": 1.928661035939675, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 172000, "timesteps_this_iter": 32, "agent_timesteps_total": 172000, "timers": {"load_time_ms": 0.237, "load_throughput": 134770.286, "learn_time_ms": 37.638, "learn_throughput": 850.196}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0030414096545428038, "mean_q": 12619.111328125, "min_q": 10704.927734375, "max_q": 12736.4921875, "cur_lr": 0.6}, "td_error": [90.2421875, -103.7578125, -17.7578125, 56.2421875, -125.7578125, 40.2421875, -85.7578125, 24.2421875, 126.2421875, -77.7578125, 124.2421875, 58.2421875, -107.7578125, -1692.38671875, 126.2421875, -1905.322265625, -17.7578125, 26.2421875, -97.7578125, 122.2421875, 4.2421875, -11.7578125, -137.7578125, -99.7578125, 112.2421875, 48.2421875, -117.7578125, -107.7578125, -117.7578125, 32.2421875, 52.2421875, -117.7578125], "mean_td_error": -121.82635498046875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 172000, "num_agent_steps_sampled": 172000, "num_steps_trained": 1368032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1368032, "last_target_update_ts": 172000, "num_target_updates": 343}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13097244938642755, "mean_inference_ms": 1.5872326315618208, "mean_action_processing_ms": 0.05817365280445874, "mean_env_wait_ms": 1.2075031517956683, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 573, "training_iteration": 172, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-17-21", "timestamp": 1652707041, "time_this_iter_s": 15.915963172912598, "time_total_s": 2431.023275375366, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2431.023275375366, "timesteps_since_restore": 5504, "iterations_since_restore": 172, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.847826086956523, "ram_util_percent": 18.60000000000001}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39737.22, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [40836.0, 38180.0, 42266.0, 40686.0, 40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22639818070453954, "mean_inference_ms": 1.9878902180243434, "mean_action_processing_ms": 0.0653735156806618, "mean_env_wait_ms": 1.927690585851194, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 173000, "timesteps_this_iter": 32, "agent_timesteps_total": 173000, "timers": {"load_time_ms": 0.232, "load_throughput": 137786.396, "learn_time_ms": 35.236, "learn_throughput": 908.155}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.012125247158110142, "mean_q": 12444.966796875, "min_q": 9959.853515625, "max_q": 12747.59375, "cur_lr": 0.6}, "td_error": [132.87109375, -75.12890625, 126.87109375, 64.87109375, -19.12890625, 22.87109375, -89.12890625, 102.87109375, -57.12890625, 132.87109375, -113.12890625, -1984.0009765625, -1875.138671875, -105.12890625, 12469.59375, -109.12890625, 40.87109375, -137.12890625, -2730.57421875, 134.87109375, -81.12890625, 78.87109375, 64.87109375, -65.12890625, -2884.869140625, -75.12890625, -73.12890625, -67.12890625, 74.87109375, -119.12890625, 134.87109375, 134.87109375], "mean_td_error": 95.51657104492188, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 173000, "num_agent_steps_sampled": 173000, "num_steps_trained": 1376032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1376032, "last_target_update_ts": 173000, "num_target_updates": 345}, "done": false, "episodes_total": 576, "training_iteration": 173, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-17-35", "timestamp": 1652707055, "time_this_iter_s": 14.132124900817871, "time_total_s": 2445.155400276184, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2445.155400276184, "timesteps_since_restore": 5536, "iterations_since_restore": 173, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.075, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39740.24, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [40810.0, 44464.0, 40538.0, 45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22647126900041606, "mean_inference_ms": 1.9886814465976266, "mean_action_processing_ms": 0.06539436816451405, "mean_env_wait_ms": 1.9263142831971618, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 174000, "timesteps_this_iter": 32, "agent_timesteps_total": 174000, "timers": {"load_time_ms": 0.134, "load_throughput": 239162.024, "learn_time_ms": 21.015, "learn_throughput": 1522.734}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.012978478334844112, "mean_q": 12625.6455078125, "min_q": 10012.208984375, "max_q": 12764.7939453125, "cur_lr": 0.6}, "td_error": [87.3154296875, -62.6845703125, -2.6845703125, 115.3154296875, -100.6845703125, 35.3154296875, 127.3154296875, 131.3154296875, -102.6845703125, 139.3154296875, -1562.8681640625, 137.3154296875, 131.3154296875, 71.3154296875, -72.6845703125, 75.3154296875, 99.3154296875, -52.6845703125, -94.6845703125, -106.6845703125, 81.3154296875, -106.6845703125, -2751.26953125, 137.3154296875, -62.6845703125, 12518.7939453125, -108.6845703125, -86.6845703125, 139.3154296875, -100.6845703125, -72.6845703125, 115.3154296875], "mean_td_error": 271.7126159667969, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 174000, "num_agent_steps_sampled": 174000, "num_steps_trained": 1384032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1384032, "last_target_update_ts": 174000, "num_target_updates": 347}, "done": false, "episodes_total": 580, "training_iteration": 174, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-17-48", "timestamp": 1652707068, "time_this_iter_s": 12.342907905578613, "time_total_s": 2457.4983081817627, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2457.4983081817627, "timesteps_since_restore": 5568, "iterations_since_restore": 174, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.944444444444443, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39767.1, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [45550.0, 36804.0, 41748.0, 37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2265111693265144, "mean_inference_ms": 1.9891268078396607, "mean_action_processing_ms": 0.06540506965283038, "mean_env_wait_ms": 1.925208408999099, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 175000, "timesteps_this_iter": 32, "agent_timesteps_total": 175000, "timers": {"load_time_ms": 0.132, "load_throughput": 242314.006, "learn_time_ms": 20.813, "learn_throughput": 1537.523}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.06772076338529587, "mean_q": 12252.103515625, "min_q": 10021.2470703125, "max_q": 12774.9130859375, "cur_lr": 0.6}, "td_error": [93.150390625, 15.150390625, -118.849609375, 97.150390625, 12520.9130859375, -96.849609375, -2840.515625, -78.849609375, 117.150390625, 131.150390625, 113.150390625, 31.150390625, -2117.42578125, 125.150390625, -2634.515625, -18.849609375, 77.150390625, 131.150390625, -2138.74609375, 117.150390625, 131.150390625, -76.849609375, 69.150390625, 29.150390625, 131.150390625, -2627.0712890625, 129.150390625, -2128.74609375, -2238.81640625, 117.150390625, -66.849609375, 59.150390625], "mean_td_error": -92.10357666015625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 175000, "num_agent_steps_sampled": 175000, "num_steps_trained": 1392032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1392032, "last_target_update_ts": 175000, "num_target_updates": 349}, "done": false, "episodes_total": 583, "training_iteration": 175, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-17-57", "timestamp": 1652707077, "time_this_iter_s": 9.158056259155273, "time_total_s": 2466.656364440918, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2466.656364440918, "timesteps_since_restore": 5600, "iterations_since_restore": 175, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 21.56923076923077, "ram_util_percent": 18.599999999999998}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39552.6, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37454.0, 41948.0, 45104.0, 36360.0, 44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22654761956884767, "mean_inference_ms": 1.9895329352045308, "mean_action_processing_ms": 0.0654144358160653, "mean_env_wait_ms": 1.924093186814794, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 176000, "timesteps_this_iter": 32, "agent_timesteps_total": 176000, "timers": {"load_time_ms": 0.212, "load_throughput": 151146.09, "learn_time_ms": 31.268, "learn_throughput": 1023.422}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.03150494396686554, "mean_q": 12472.408203125, "min_q": 10522.8818359375, "max_q": 12785.3837890625, "cur_lr": 0.6}, "td_error": [98.9970703125, -1718.591796875, -73.0029296875, -103.0029296875, -67.0029296875, -2281.7666015625, 34.9970703125, 12529.3837890625, -51.0029296875, -83.0029296875, 128.9970703125, -65.0029296875, -45.0029296875, -89.0029296875, -87.0029296875, -2305.7666015625, 46.9970703125, 134.9970703125, 68.9970703125, -57.0029296875, -65.0029296875, 136.9970703125, 134.9970703125, -1724.591796875, 12515.3837890625, 134.9970703125, -103.0029296875, 132.9970703125, -111.0029296875, -63.0029296875, -113.0029296875, -2127.5048828125], "mean_td_error": 461.4210205078125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 176000, "num_agent_steps_sampled": 176000, "num_steps_trained": 1400032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1400032, "last_target_update_ts": 176000, "num_target_updates": 351}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.13016140936866022, "mean_inference_ms": 1.5753994980650177, "mean_action_processing_ms": 0.05773630713650732, "mean_env_wait_ms": 1.201036671564151, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 586, "training_iteration": 176, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-18-11", "timestamp": 1652707091, "time_this_iter_s": 14.205294370651245, "time_total_s": 2480.861658811569, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2480.861658811569, "timesteps_since_restore": 5632, "iterations_since_restore": 176, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.755, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39479.48, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [44854.0, 37208.0, 43914.0, 36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22659367311001838, "mean_inference_ms": 1.9900456582724082, "mean_action_processing_ms": 0.06542492585850226, "mean_env_wait_ms": 1.9225534557829866, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 177000, "timesteps_this_iter": 32, "agent_timesteps_total": 177000, "timers": {"load_time_ms": 0.135, "load_throughput": 236215.642, "learn_time_ms": 20.618, "learn_throughput": 1552.025}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.011953161098062992, "mean_q": 12551.404296875, "min_q": 10105.7509765625, "max_q": 12812.328125, "cur_lr": 0.6}, "td_error": [-2674.9755859375, -56.3984375, -40.3984375, -88.3984375, 139.6015625, 77.6015625, 133.6015625, 97.6015625, -116.3984375, -56.3984375, -118.3984375, -1941.583984375, -66.3984375, 99.6015625, -1596.8271484375, 139.6015625, -46.3984375, 131.6015625, -1909.767578125, 31.6015625, 137.6015625, -72.3984375, -104.3984375, 69.6015625, 33.6015625, -116.3984375, -108.3984375, -82.3984375, -64.3984375, 91.6015625, -100.3984375, 12566.328125], "mean_td_error": 137.13800048828125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 177000, "num_agent_steps_sampled": 177000, "num_steps_trained": 1408032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1408032, "last_target_update_ts": 177000, "num_target_updates": 353}, "done": false, "episodes_total": 590, "training_iteration": 177, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-18-24", "timestamp": 1652707104, "time_this_iter_s": 12.27524709701538, "time_total_s": 2493.1369059085846, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2493.1369059085846, "timesteps_since_restore": 5664, "iterations_since_restore": 177, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.094444444444445, "ram_util_percent": 18.60555555555556}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39417.86, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36714.0, 37984.0, 41888.0, 33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22661250856315418, "mean_inference_ms": 1.99027489304809, "mean_action_processing_ms": 0.0654277337801045, "mean_env_wait_ms": 1.9213012907511966, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 178000, "timesteps_this_iter": 32, "agent_timesteps_total": 178000, "timers": {"load_time_ms": 0.135, "load_throughput": 236673.828, "learn_time_ms": 21.668, "learn_throughput": 1476.859}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.011644183658063412, "mean_q": 12688.5791015625, "min_q": 10132.3984375, "max_q": 12836.93359375, "cur_lr": 0.6}, "td_error": [124.07421875, 142.07421875, -83.92578125, 134.07421875, -59.92578125, 128.07421875, -85.92578125, -89.92578125, 52.07421875, -7.92578125, -59.92578125, -33.92578125, 142.07421875, 124.07421875, 138.07421875, 74.07421875, 142.07421875, 72.07421875, -91.92578125, 102.07421875, 70.07421875, -87.92578125, -2138.7265625, -31.92578125, 2.07421875, 100.07421875, 80.07421875, 36.07421875, 142.07421875, -59.92578125, -35.92578125, -2660.4609375], "mean_td_error": -116.342529296875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 178000, "num_agent_steps_sampled": 178000, "num_steps_trained": 1416032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1416032, "last_target_update_ts": 178000, "num_target_updates": 355}, "done": false, "episodes_total": 593, "training_iteration": 178, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-18-34", "timestamp": 1652707114, "time_this_iter_s": 10.105488061904907, "time_total_s": 2503.2423939704895, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2503.2423939704895, "timesteps_since_restore": 5696, "iterations_since_restore": 178, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.25714285714286, "ram_util_percent": 18.599999999999998}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30836.0, "episode_reward_mean": 39559.42, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [33846.0, 36612.0, 30836.0, 38178.0, 35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2266235027455237, "mean_inference_ms": 1.9904236542224734, "mean_action_processing_ms": 0.06542797540779753, "mean_env_wait_ms": 1.9200430206913979, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 179000, "timesteps_this_iter": 32, "agent_timesteps_total": 179000, "timers": {"load_time_ms": 0.169, "load_throughput": 188826.292, "learn_time_ms": 24.757, "learn_throughput": 1292.576}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.010748260654509068, "mean_q": 12698.892578125, "min_q": 10176.96875, "max_q": 12841.6318359375, "cur_lr": 0.6}, "td_error": [-119.607421875, -2634.2705078125, -89.607421875, 60.392578125, 12563.6318359375, 90.392578125, 136.392578125, 104.392578125, 22.392578125, -63.607421875, 62.392578125, 122.392578125, 12563.6318359375, -1964.6376953125, -13.607421875, 102.392578125, 72.392578125, -103.607421875, 134.392578125, -71.607421875, -61.607421875, 12567.6318359375, -83.607421875, -27.607421875, 12601.6318359375, 46.392578125, -73.607421875, -137.607421875, 136.392578125, 76.392578125, 136.392578125, -69.607421875], "mean_td_error": 1440.18212890625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 179000, "num_agent_steps_sampled": 179000, "num_steps_trained": 1424032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1424032, "last_target_update_ts": 179000, "num_target_updates": 357}, "done": false, "episodes_total": 596, "training_iteration": 179, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-18-44", "timestamp": 1652707124, "time_this_iter_s": 10.176051378250122, "time_total_s": 2513.4184453487396, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2513.4184453487396, "timesteps_since_restore": 5728, "iterations_since_restore": 179, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.013333333333332, "ram_util_percent": 18.613333333333333}}
{"episode_reward_max": 56514.0, "episode_reward_min": 31178.0, "episode_reward_mean": 39663.9, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [35478.0, 37672.0, 35710.0, 41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22662507139697097, "mean_inference_ms": 1.9904866248062598, "mean_action_processing_ms": 0.06542387434057685, "mean_env_wait_ms": 1.9183391082717092, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 180000, "timesteps_this_iter": 32, "agent_timesteps_total": 180000, "timers": {"load_time_ms": 0.136, "load_throughput": 234564.362, "learn_time_ms": 21.012, "learn_throughput": 1522.934}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.005150269716978073, "mean_q": 12495.18359375, "min_q": 10188.33203125, "max_q": 12862.251953125, "cur_lr": 0.6}, "td_error": [-52.1865234375, -80.1865234375, 145.8134765625, 145.8134765625, 145.8134765625, 145.8134765625, -96.1865234375, 145.8134765625, 12602.251953125, -32.1865234375, -88.1865234375, -54.1865234375, 71.8134765625, -1770.283203125, 141.8134765625, 111.8134765625, -72.1865234375, -1930.283203125, -142.1865234375, -106.1865234375, -2542.2255859375, -88.1865234375, -2566.2255859375, 67.8134765625, 81.8134765625, 1.8134765625, -106.1865234375, -46.1865234375, 91.8134765625, -70.1865234375, 111.8134765625, -2592.1064453125], "mean_td_error": 49.258697509765625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 180000, "num_agent_steps_sampled": 180000, "num_steps_trained": 1432032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1432032, "last_target_update_ts": 180000, "num_target_updates": 359}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12930027070500022, "mean_inference_ms": 1.5637529716607543, "mean_action_processing_ms": 0.05729738301448879, "mean_env_wait_ms": 1.1944337629687387, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 600, "training_iteration": 180, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-18-55", "timestamp": 1652707135, "time_this_iter_s": 10.93517255783081, "time_total_s": 2524.3536179065704, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2524.3536179065704, "timesteps_since_restore": 5760, "iterations_since_restore": 180, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 24.259999999999998, "ram_util_percent": 18.6}}
{"episode_reward_max": 56514.0, "episode_reward_min": 31178.0, "episode_reward_mean": 39790.04, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [41218.0, 37444.0, 37786.0, 35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2266250336893087, "mean_inference_ms": 1.9905281076735852, "mean_action_processing_ms": 0.06542084369012935, "mean_env_wait_ms": 1.9170872888688268, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 181000, "timesteps_this_iter": 32, "agent_timesteps_total": 181000, "timers": {"load_time_ms": 0.235, "load_throughput": 136164.886, "learn_time_ms": 37.907, "learn_throughput": 844.169}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.005441285204142332, "mean_q": 12479.7470703125, "min_q": 10242.439453125, "max_q": 12877.6171875, "cur_lr": 0.6}, "td_error": [-66.869140625, 135.130859375, -128.869140625, 109.130859375, 129.130859375, -2608.046875, -2048.7333984375, 135.130859375, 77.130859375, -1771.16796875, 113.130859375, -96.869140625, 117.130859375, -32.869140625, -108.869140625, -10.869140625, -10.869140625, 135.130859375, -2090.8134765625, 135.130859375, 31.130859375, 95.130859375, -2133.1259765625, 97.130859375, -94.869140625, -108.869140625, -70.869140625, -1879.16796875, -110.869140625, 135.130859375, -100.869140625, 115.130859375], "mean_td_error": -372.3016662597656, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 181000, "num_agent_steps_sampled": 181000, "num_steps_trained": 1440032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1440032, "last_target_update_ts": 181000, "num_target_updates": 361}, "done": false, "episodes_total": 603, "training_iteration": 181, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-19-10", "timestamp": 1652707150, "time_this_iter_s": 14.501659870147705, "time_total_s": 2538.855277776718, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2538.855277776718, "timesteps_since_restore": 5792, "iterations_since_restore": 181, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.16190476190476, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 56514.0, "episode_reward_min": 31178.0, "episode_reward_mean": 39735.16, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [35480.0, 44896.0, 40840.0, 39138.0, 49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22663459458829435, "mean_inference_ms": 1.9906545028668312, "mean_action_processing_ms": 0.0654206087185947, "mean_env_wait_ms": 1.9159214164622937, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 182000, "timesteps_this_iter": 32, "agent_timesteps_total": 182000, "timers": {"load_time_ms": 0.334, "load_throughput": 95733.044, "learn_time_ms": 38.692, "learn_throughput": 827.047}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.024964626878499985, "mean_q": 12271.7412109375, "min_q": 10243.26953125, "max_q": 12884.1318359375, "cur_lr": 0.6}, "td_error": [-2700.02734375, 104.8349609375, 36.8349609375, 118.8349609375, 138.8349609375, 54.8349609375, -2544.609375, 12586.1318359375, 138.8349609375, -2696.02734375, 12638.1318359375, 36.8349609375, -103.1650390625, -2082.603515625, 36.8349609375, -1954.7470703125, 130.8349609375, -121.1650390625, -59.1650390625, 70.8349609375, -123.1650390625, -117.1650390625, 0.8349609375, -2530.609375, -69.1650390625, -137.1650390625, -2536.609375, -85.1650390625, -63.1650390625, -2542.609375, 138.8349609375, 136.8349609375], "mean_td_error": 184.46218872070312, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 182000, "num_agent_steps_sampled": 182000, "num_steps_trained": 1448032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1448032, "last_target_update_ts": 182000, "num_target_updates": 363}, "done": false, "episodes_total": 606, "training_iteration": 182, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-19-26", "timestamp": 1652707166, "time_this_iter_s": 15.909142971038818, "time_total_s": 2554.764420747757, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2554.764420747757, "timesteps_since_restore": 5824, "iterations_since_restore": 182, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.382608695652177, "ram_util_percent": 18.604347826086965}}
{"episode_reward_max": 56514.0, "episode_reward_min": 31178.0, "episode_reward_mean": 39848.02, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [49330.0, 35656.0, 37898.0, 39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22666762413456798, "mean_inference_ms": 1.9910092841341043, "mean_action_processing_ms": 0.06542643915723391, "mean_env_wait_ms": 1.9145236720482837, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 183000, "timesteps_this_iter": 32, "agent_timesteps_total": 183000, "timers": {"load_time_ms": 0.191, "load_throughput": 167898.084, "learn_time_ms": 29.467, "learn_throughput": 1085.958}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.018033213913440704, "mean_q": 12587.7177734375, "min_q": 10264.7958984375, "max_q": 12874.1728515625, "cur_lr": 0.6}, "td_error": [-100.4248046875, -128.4248046875, 125.5751953125, -76.4248046875, 12582.1728515625, 59.5751953125, -132.4248046875, 45.5751953125, -84.4248046875, -116.4248046875, -112.4248046875, 95.5751953125, -144.4248046875, -148.4248046875, 125.5751953125, 12616.1728515625, -1765.9521484375, -82.4248046875, 83.5751953125, -130.4248046875, -2148.2578125, 121.5751953125, -116.4248046875, 123.5751953125, 12576.1728515625, -132.4248046875, -2613.8017578125, -130.4248046875, 83.5751953125, -2550.244140625, -116.4248046875, -120.4248046875], "mean_td_error": 865.23876953125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 183000, "num_agent_steps_sampled": 183000, "num_steps_trained": 1456032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1456032, "last_target_update_ts": 183000, "num_target_updates": 365}, "done": false, "episodes_total": 610, "training_iteration": 183, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-19-40", "timestamp": 1652707180, "time_this_iter_s": 14.905380249023438, "time_total_s": 2569.6698009967804, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2569.6698009967804, "timesteps_since_restore": 5856, "iterations_since_restore": 183, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.942857142857143, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 56514.0, "episode_reward_min": 31178.0, "episode_reward_mean": 39742.98, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [39388.0, 38226.0, 35022.0, 41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2266927197188232, "mean_inference_ms": 1.9912788960116585, "mean_action_processing_ms": 0.06543110415107481, "mean_env_wait_ms": 1.9134834071481641, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 184000, "timesteps_this_iter": 32, "agent_timesteps_total": 184000, "timers": {"load_time_ms": 0.221, "load_throughput": 144584.432, "learn_time_ms": 33.982, "learn_throughput": 941.68}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.016047688201069832, "mean_q": 12740.8125, "min_q": 10341.78125, "max_q": 12887.4775390625, "cur_lr": 0.6}, "td_error": [-46.810546875, -136.810546875, 95.189453125, 111.189453125, -100.810546875, 117.189453125, -108.810546875, -68.810546875, -2424.5068359375, 12635.4775390625, -156.810546875, -66.810546875, 12639.4775390625, -108.810546875, 133.189453125, 67.189453125, -90.810546875, -92.810546875, -78.810546875, -86.810546875, -2158.3896484375, -110.810546875, -110.810546875, -118.810546875, 131.189453125, -128.810546875, 129.189453125, 133.189453125, -52.810546875, 69.189453125, 29.189453125, -60.810546875], "mean_td_error": 624.4176025390625, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 184000, "num_agent_steps_sampled": 184000, "num_steps_trained": 1464032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1464032, "last_target_update_ts": 184000, "num_target_updates": 367}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12897192658777556, "mean_inference_ms": 1.5591957130152956, "mean_action_processing_ms": 0.05711839558568418, "mean_env_wait_ms": 1.1919263946553105, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 613, "training_iteration": 184, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-19-55", "timestamp": 1652707195, "time_this_iter_s": 14.693118333816528, "time_total_s": 2584.362919330597, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2584.362919330597, "timesteps_since_restore": 5888, "iterations_since_restore": 184, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.82857142857143, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 56514.0, "episode_reward_min": 31178.0, "episode_reward_mean": 39953.74, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [41516.0, 32878.0, 43546.0, 45398.0, 41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.226716132888728, "mean_inference_ms": 1.9915306890029194, "mean_action_processing_ms": 0.0654352360376033, "mean_env_wait_ms": 1.9124746084431181, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 185000, "timesteps_this_iter": 32, "agent_timesteps_total": 185000, "timers": {"load_time_ms": 0.226, "load_throughput": 141864.209, "learn_time_ms": 35.724, "learn_throughput": 895.762}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.008063350804150105, "mean_q": 12756.0380859375, "min_q": 10313.2861328125, "max_q": 12892.287109375, "cur_lr": 0.6}, "td_error": [-69.8134765625, -121.8134765625, 122.1865234375, 128.1865234375, -141.8134765625, 28.1865234375, 126.1865234375, -61.8134765625, 106.1865234375, 128.1865234375, -111.8134765625, 98.1865234375, 22.1865234375, -81.8134765625, 126.1865234375, -111.8134765625, 30.1865234375, -2704.814453125, -1884.830078125, -101.8134765625, -65.8134765625, 68.1865234375, -73.8134765625, 120.1865234375, -117.8134765625, 128.1865234375, 12602.287109375, 124.1865234375, 128.1865234375, -113.8134765625, -111.8134765625, -99.8134765625], "mean_td_error": 253.50161743164062, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 185000, "num_agent_steps_sampled": 185000, "num_steps_trained": 1472032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1472032, "last_target_update_ts": 185000, "num_target_updates": 369}, "done": false, "episodes_total": 616, "training_iteration": 185, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-20-11", "timestamp": 1652707211, "time_this_iter_s": 15.609523057937622, "time_total_s": 2599.9724423885345, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2599.9724423885345, "timesteps_since_restore": 5920, "iterations_since_restore": 185, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.904545454545456, "ram_util_percent": 18.604545454545462}}
{"episode_reward_max": 56514.0, "episode_reward_min": 31178.0, "episode_reward_mean": 39791.58, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [41580.0, 39478.0, 39146.0, 36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22672813137958714, "mean_inference_ms": 1.9916922766534964, "mean_action_processing_ms": 0.06543540763714101, "mean_env_wait_ms": 1.911009103733016, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 186000, "timesteps_this_iter": 32, "agent_timesteps_total": 186000, "timers": {"load_time_ms": 0.139, "load_throughput": 230971.826, "learn_time_ms": 24.415, "learn_throughput": 1310.668}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.025213032960891724, "mean_q": 12711.7705078125, "min_q": 10943.6416015625, "max_q": 12889.7392578125, "cur_lr": 0.6}, "td_error": [-114.685546875, -124.685546875, -158.685546875, -34.685546875, -88.685546875, -120.685546875, 10759.2880859375, -84.685546875, 95.314453125, 12637.7392578125, 119.314453125, -82.685546875, 111.314453125, -1995.13671875, 115.314453125, 119.314453125, 119.314453125, -2076.783203125, 59.314453125, -116.685546875, 63.314453125, 51.314453125, -104.685546875, -116.685546875, -22.685546875, -108.685546875, 29.314453125, -28.685546875, 81.314453125, -130.685546875, 13.314453125, -124.685546875], "mean_td_error": 585.6222534179688, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 186000, "num_agent_steps_sampled": 186000, "num_steps_trained": 1480032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1480032, "last_target_update_ts": 186000, "num_target_updates": 371}, "done": false, "episodes_total": 620, "training_iteration": 186, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-20-21", "timestamp": 1652707221, "time_this_iter_s": 10.386475563049316, "time_total_s": 2610.358917951584, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2610.358917951584, "timesteps_since_restore": 5952, "iterations_since_restore": 186, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.353333333333335, "ram_util_percent": 18.62}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30714.0, "episode_reward_mean": 39634.8, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36386.0, 36504.0, 35662.0, 32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0, 41132.0, 30714.0, 32680.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22673477684146334, "mean_inference_ms": 1.9917919592129234, "mean_action_processing_ms": 0.06543496334803552, "mean_env_wait_ms": 1.909901508434562, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 187000, "timesteps_this_iter": 32, "agent_timesteps_total": 187000, "timers": {"load_time_ms": 0.226, "load_throughput": 141475.417, "learn_time_ms": 35.279, "learn_throughput": 907.068}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.005540155805647373, "mean_q": 12673.9609375, "min_q": 10441.8857421875, "max_q": 12900.2900390625, "cur_lr": 0.6}, "td_error": [-107.1376953125, 128.8623046875, -119.1376953125, 124.8623046875, 126.8623046875, 128.8623046875, 128.8623046875, -19.1376953125, -1832.8232421875, 126.8623046875, -77.1376953125, 88.8623046875, 72.8623046875, -115.1376953125, 80.8623046875, -1427.359375, 12638.2900390625, -13.1376953125, 128.8623046875, 126.8623046875, -103.1376953125, -111.1376953125, 12646.2900390625, -115.1376953125, 12652.2900390625, 12646.2900390625, 56.8623046875, -129.1376953125, -2403.5419921875, 128.8623046875, -139.1376953125, -1641.359375], "mean_td_error": 1364.961669921875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 187000, "num_agent_steps_sampled": 187000, "num_steps_trained": 1488032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1488032, "last_target_update_ts": 187000, "num_target_updates": 373}, "done": false, "episodes_total": 623, "training_iteration": 187, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-20-36", "timestamp": 1652707236, "time_this_iter_s": 14.89449429512024, "time_total_s": 2625.253412246704, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2625.253412246704, "timesteps_since_restore": 5984, "iterations_since_restore": 187, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.063636363636366, "ram_util_percent": 18.60000000000001}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30714.0, "episode_reward_mean": 39770.56, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [32176.0, 42326.0, 41802.0, 37818.0, 55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0, 41132.0, 30714.0, 32680.0, 43342.0, 42710.0, 36076.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22673269700919402, "mean_inference_ms": 1.9918129419496435, "mean_action_processing_ms": 0.06543179638168183, "mean_env_wait_ms": 1.9087831664733381, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 188000, "timesteps_this_iter": 32, "agent_timesteps_total": 188000, "timers": {"load_time_ms": 0.235, "load_throughput": 136178.701, "learn_time_ms": 35.492, "learn_throughput": 901.614}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.043202467262744904, "mean_q": 12931.1728515625, "min_q": 12931.1728515625, "max_q": 12931.1728515625, "cur_lr": 0.6}, "td_error": [82.7978515625, 140.7978515625, 4.7978515625, 116.7978515625, 142.7978515625, -63.2021484375, -87.2021484375, -77.2021484375, 6.7978515625, 12649.1728515625, 142.7978515625, 142.7978515625, 142.7978515625, 110.7978515625, -85.2021484375, 142.7978515625, 112.7978515625, -29.2021484375, -105.2021484375, 84.7978515625, 140.7978515625, -107.2021484375, 122.7978515625, 78.7978515625, 140.7978515625, -101.2021484375, 134.7978515625, 142.7978515625, 140.7978515625, -105.2021484375, 140.7978515625, 142.7978515625], "mean_td_error": 451.4970703125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 188000, "num_agent_steps_sampled": 188000, "num_steps_trained": 1496032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1496032, "last_target_update_ts": 188000, "num_target_updates": 375}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12879051745221104, "mean_inference_ms": 1.55813556538855, "mean_action_processing_ms": 0.05706553171394683, "mean_env_wait_ms": 1.1905311447586135, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 626, "training_iteration": 188, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-20-51", "timestamp": 1652707251, "time_this_iter_s": 14.828652381896973, "time_total_s": 2640.082064628601, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2640.082064628601, "timesteps_since_restore": 6016, "iterations_since_restore": 188, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 21.514285714285716, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30714.0, "episode_reward_mean": 39784.32, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [55942.0, 40336.0, 42672.0, 38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0, 41132.0, 30714.0, 32680.0, 43342.0, 42710.0, 36076.0, 42078.0, 36334.0, 35196.0, 41890.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2267241203553028, "mean_inference_ms": 1.9917865415080207, "mean_action_processing_ms": 0.06542585452493524, "mean_env_wait_ms": 1.907298301784864, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 189000, "timesteps_this_iter": 32, "agent_timesteps_total": 189000, "timers": {"load_time_ms": 0.26, "load_throughput": 123169.43, "learn_time_ms": 37.617, "learn_throughput": 850.671}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.013163542374968529, "mean_q": 12598.3291015625, "min_q": 10415.1533203125, "max_q": 12938.1630859375, "cur_lr": 0.6}, "td_error": [100.876953125, 16.876953125, -2596.1328125, -2384.064453125, 112.876953125, 68.876953125, -113.123046875, 64.876953125, 64.876953125, -121.123046875, 74.876953125, 110.876953125, -141.123046875, -137.123046875, 128.876953125, -1868.6123046875, -137.123046875, -99.123046875, -121.123046875, -9.123046875, 132.876953125, 12698.1630859375, 132.876953125, -1861.8671875, 12670.1630859375, 132.876953125, -61.123046875, 132.876953125, -139.123046875, -105.123046875, -2115.62109375, -111.123046875], "mean_td_error": 453.8110046386719, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 189000, "num_agent_steps_sampled": 189000, "num_steps_trained": 1504032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1504032, "last_target_update_ts": 189000, "num_target_updates": 377}, "done": false, "episodes_total": 630, "training_iteration": 189, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-21-06", "timestamp": 1652707266, "time_this_iter_s": 14.294827699661255, "time_total_s": 2654.3768923282623, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2654.3768923282623, "timesteps_since_restore": 6048, "iterations_since_restore": 189, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.859999999999996, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 56514.0, "episode_reward_min": 30714.0, "episode_reward_mean": 39539.64, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [38414.0, 56514.0, 42966.0, 36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0, 41132.0, 30714.0, 32680.0, 43342.0, 42710.0, 36076.0, 42078.0, 36334.0, 35196.0, 41890.0, 32254.0, 37358.0, 44870.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22671042552110549, "mean_inference_ms": 1.9916910694471952, "mean_action_processing_ms": 0.06541897536035485, "mean_env_wait_ms": 1.9060914472015456, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 190000, "timesteps_this_iter": 32, "agent_timesteps_total": 190000, "timers": {"load_time_ms": 0.216, "load_throughput": 148012.492, "learn_time_ms": 32.968, "learn_throughput": 970.638}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.022195838391780853, "mean_q": 12761.3037109375, "min_q": 10553.267578125, "max_q": 12954.2607421875, "cur_lr": 0.6}, "td_error": [-1695.6005859375, -119.47265625, 128.52734375, 114.52734375, 102.52734375, 132.52734375, 68.52734375, -107.47265625, 12670.2607421875, 90.52734375, -117.47265625, -135.47265625, -109.47265625, -121.47265625, -63.47265625, -71.47265625, -103.47265625, -95.47265625, 78.52734375, 132.52734375, -1888.974609375, -141.47265625, -119.47265625, -69.47265625, -87.47265625, 124.52734375, -71.47265625, 30.52734375, -57.47265625, 60.52734375, -2278.4658203125, -155.47265625], "mean_td_error": 191.37454223632812, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 190000, "num_agent_steps_sampled": 190000, "num_steps_trained": 1512032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1512032, "last_target_update_ts": 190000, "num_target_updates": 379}, "done": false, "episodes_total": 633, "training_iteration": 190, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-21-16", "timestamp": 1652707276, "time_this_iter_s": 10.434961080551147, "time_total_s": 2664.8118534088135, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2664.8118534088135, "timesteps_since_restore": 6080, "iterations_since_restore": 190, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.393333333333334, "ram_util_percent": 18.6}}
{"episode_reward_max": 52464.0, "episode_reward_min": 30714.0, "episode_reward_mean": 39376.72, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36218.0, 43256.0, 33862.0, 39156.0, 31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0, 41132.0, 30714.0, 32680.0, 43342.0, 42710.0, 36076.0, 42078.0, 36334.0, 35196.0, 41890.0, 32254.0, 37358.0, 44870.0, 40688.0, 39710.0, 41204.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22669224840065091, "mean_inference_ms": 1.9915427613362922, "mean_action_processing_ms": 0.06541031890764124, "mean_env_wait_ms": 1.904837652507117, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 191000, "timesteps_this_iter": 32, "agent_timesteps_total": 191000, "timers": {"load_time_ms": 0.248, "load_throughput": 128943.922, "learn_time_ms": 40.596, "learn_throughput": 788.252}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.01670214906334877, "mean_q": 12696.2109375, "min_q": 10458.23828125, "max_q": 12970.5302734375, "cur_lr": 0.6}, "td_error": [-64.880859375, 12686.5302734375, -2463.1728515625, -1773.0751953125, 121.119140625, -130.880859375, 139.119140625, 139.119140625, 139.119140625, -84.880859375, 12724.5302734375, 12698.5302734375, -96.880859375, -1357.654296875, 12674.5302734375, 83.119140625, -60.880859375, 79.119140625, -2.880859375, -114.880859375, 37.119140625, 81.119140625, 131.119140625, -1439.654296875, 139.119140625, -100.880859375, -96.880859375, 37.119140625, -96.880859375, -1747.0751953125, 101.119140625, 12688.5302734375], "mean_td_error": 1720.89501953125, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 191000, "num_agent_steps_sampled": 191000, "num_steps_trained": 1520032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1520032, "last_target_update_ts": 191000, "num_target_updates": 381}, "done": false, "episodes_total": 636, "training_iteration": 191, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-21-30", "timestamp": 1652707290, "time_this_iter_s": 14.195587873458862, "time_total_s": 2679.0074412822723, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2679.0074412822723, "timesteps_since_restore": 6112, "iterations_since_restore": 191, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.13809523809524, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 52464.0, "episode_reward_min": 29502.0, "episode_reward_mean": 39369.2, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [31376.0, 44784.0, 42874.0, 36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0, 41132.0, 30714.0, 32680.0, 43342.0, 42710.0, 36076.0, 42078.0, 36334.0, 35196.0, 41890.0, 32254.0, 37358.0, 44870.0, 40688.0, 39710.0, 41204.0, 38148.0, 29502.0, 31776.0, 52314.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22666189211740473, "mean_inference_ms": 1.9912851330265042, "mean_action_processing_ms": 0.06539707183035509, "mean_env_wait_ms": 1.903143799807456, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 192000, "timesteps_this_iter": 32, "agent_timesteps_total": 192000, "timers": {"load_time_ms": 0.148, "load_throughput": 216166.416, "learn_time_ms": 22.783, "learn_throughput": 1404.551}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.01660054735839367, "mean_q": 12961.8603515625, "min_q": 12961.8603515625, "max_q": 12961.8603515625, "cur_lr": 0.6}, "td_error": [12721.8603515625, 123.673828125, 12717.8603515625, 97.673828125, 127.673828125, -70.326171875, -102.326171875, 12701.8603515625, 65.673828125, 89.673828125, -90.326171875, 12717.8603515625, 57.673828125, 107.673828125, -94.326171875, -110.326171875, -16.326171875, -98.326171875, 127.673828125, -108.326171875, -98.326171875, -104.326171875, -110.326171875, -108.326171875, 59.673828125, 125.673828125, -68.326171875, -114.326171875, -78.326171875, 61.673828125, 12711.8603515625, -138.326171875], "mean_td_error": 1972.0155029296875, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 192000, "num_agent_steps_sampled": 192000, "num_steps_trained": 1528032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1528032, "last_target_update_ts": 192000, "num_target_updates": 383}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12783333374621034, "mean_inference_ms": 1.5449603227565425, "mean_action_processing_ms": 0.05657369416171119, "mean_env_wait_ms": 1.1830177913729611, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 640, "training_iteration": 192, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-21-46", "timestamp": 1652707306, "time_this_iter_s": 15.524716138839722, "time_total_s": 2694.532157421112, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2694.532157421112, "timesteps_since_restore": 6144, "iterations_since_restore": 192, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.018181818181816, "ram_util_percent": 18.60000000000001}}
{"episode_reward_max": 52464.0, "episode_reward_min": 29502.0, "episode_reward_mean": 39286.42, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [36334.0, 44346.0, 42804.0, 37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0, 41132.0, 30714.0, 32680.0, 43342.0, 42710.0, 36076.0, 42078.0, 36334.0, 35196.0, 41890.0, 32254.0, 37358.0, 44870.0, 40688.0, 39710.0, 41204.0, 38148.0, 29502.0, 31776.0, 52314.0, 37904.0, 37040.0, 35812.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22662986352335643, "mean_inference_ms": 1.991001044185519, "mean_action_processing_ms": 0.06538439082164828, "mean_env_wait_ms": 1.9018347866002816, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 193000, "timesteps_this_iter": 32, "agent_timesteps_total": 193000, "timers": {"load_time_ms": 0.14, "load_throughput": 229196.94, "learn_time_ms": 20.364, "learn_throughput": 1571.41}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.02384817600250244, "mean_q": 12802.1123046875, "min_q": 10977.169921875, "max_q": 12978.162109375, "cur_lr": 0.6}, "td_error": [66.8994140625, -1771.4248046875, 136.8994140625, 106.8994140625, -75.1005859375, -91.1005859375, 12734.162109375, -99.1005859375, -1920.0927734375, 110.8994140625, 140.8994140625, -71.1005859375, -63.1005859375, -1.1005859375, 132.8994140625, -135.1005859375, -1677.4248046875, -61.1005859375, -81.1005859375, 40.8994140625, -89.1005859375, -29.1005859375, -91.1005859375, 132.8994140625, 132.8994140625, 82.8994140625, -85.1005859375, 138.8994140625, 128.8994140625, -123.1005859375, 140.8994140625, -83.1005859375], "mean_td_error": 240.01260375976562, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 193000, "num_agent_steps_sampled": 193000, "num_steps_trained": 1536032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1536032, "last_target_update_ts": 193000, "num_target_updates": 385}, "done": false, "episodes_total": 643, "training_iteration": 193, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-21-58", "timestamp": 1652707318, "time_this_iter_s": 11.741791486740112, "time_total_s": 2706.273948907852, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2706.273948907852, "timesteps_since_restore": 6176, "iterations_since_restore": 193, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.470588235294116, "ram_util_percent": 18.60588235294118}}
{"episode_reward_max": 52464.0, "episode_reward_min": 29502.0, "episode_reward_mean": 39427.52, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37016.0, 49818.0, 33186.0, 35028.0, 44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0, 41132.0, 30714.0, 32680.0, 43342.0, 42710.0, 36076.0, 42078.0, 36334.0, 35196.0, 41890.0, 32254.0, 37358.0, 44870.0, 40688.0, 39710.0, 41204.0, 38148.0, 29502.0, 31776.0, 52314.0, 37904.0, 37040.0, 35812.0, 51432.0, 39258.0, 46904.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.2265964430618808, "mean_inference_ms": 1.990704095486783, "mean_action_processing_ms": 0.06537168840833558, "mean_env_wait_ms": 1.9005611623736147, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 194000, "timesteps_this_iter": 32, "agent_timesteps_total": 194000, "timers": {"load_time_ms": 0.133, "load_throughput": 240318.224, "learn_time_ms": 20.211, "learn_throughput": 1583.277}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.00632550148293376, "mean_q": 12733.779296875, "min_q": 10995.0947265625, "max_q": 12961.0390625, "cur_lr": 0.6}, "td_error": [-125.392578125, -117.392578125, -41.392578125, -119.392578125, 122.607421875, 52.607421875, -1641.4248046875, 120.607421875, -1765.1533203125, 10.607421875, 100.607421875, -1869.95703125, 118.607421875, -79.392578125, 120.607421875, -91.392578125, -137.392578125, 88.607421875, 88.607421875, -17.392578125, -131.392578125, 88.607421875, 54.607421875, -125.392578125, 122.607421875, 118.607421875, 58.607421875, -17.392578125, 122.607421875, -2045.3369140625, -131.392578125, -21.392578125], "mean_td_error": -221.52700805664062, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 194000, "num_agent_steps_sampled": 194000, "num_steps_trained": 1544032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1544032, "last_target_update_ts": 194000, "num_target_updates": 387}, "done": false, "episodes_total": 646, "training_iteration": 194, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-22-10", "timestamp": 1652707330, "time_this_iter_s": 12.623916864395142, "time_total_s": 2718.8978657722473, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2718.8978657722473, "timesteps_since_restore": 6208, "iterations_since_restore": 194, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.188888888888886, "ram_util_percent": 18.633333333333333}}
{"episode_reward_max": 52464.0, "episode_reward_min": 29502.0, "episode_reward_mean": 39513.64, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [44782.0, 38622.0, 42838.0, 37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0, 41132.0, 30714.0, 32680.0, 43342.0, 42710.0, 36076.0, 42078.0, 36334.0, 35196.0, 41890.0, 32254.0, 37358.0, 44870.0, 40688.0, 39710.0, 41204.0, 38148.0, 29502.0, 31776.0, 52314.0, 37904.0, 37040.0, 35812.0, 51432.0, 39258.0, 46904.0, 41232.0, 40820.0, 43986.0, 37622.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22654006309841423, "mean_inference_ms": 1.9901979130524776, "mean_action_processing_ms": 0.06535123977491997, "mean_env_wait_ms": 1.8988023787538617, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 195000, "timesteps_this_iter": 32, "agent_timesteps_total": 195000, "timers": {"load_time_ms": 0.138, "load_throughput": 232451.902, "learn_time_ms": 20.401, "learn_throughput": 1568.56}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.027854956686496735, "mean_q": 12757.767578125, "min_q": 11153.330078125, "max_q": 12968.904296875, "cur_lr": 0.6}, "td_error": [108.6572265625, -81.3427734375, -1752.9169921875, -115.3427734375, -1618.89453125, -119.3427734375, -1746.009765625, -79.3427734375, -119.3427734375, 12702.904296875, -89.3427734375, -1689.9619140625, 68.6572265625, 66.6572265625, 126.6572265625, -75.3427734375, 98.6572265625, -81.3427734375, 128.6572265625, -107.3427734375, 120.6572265625, -131.3427734375, -69.3427734375, 62.6572265625, -63.3427734375, 128.6572265625, -79.3427734375, 126.6572265625, -111.3427734375, -55.3427734375, -125.3427734375, -75.3427734375], "mean_td_error": 167.27706909179688, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 195000, "num_agent_steps_sampled": 195000, "num_steps_trained": 1552032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1552032, "last_target_update_ts": 195000, "num_target_updates": 389}, "done": false, "episodes_total": 650, "training_iteration": 195, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-22-22", "timestamp": 1652707342, "time_this_iter_s": 11.262449264526367, "time_total_s": 2730.1603150367737, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2730.1603150367737, "timesteps_since_restore": 6240, "iterations_since_restore": 195, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.466666666666665, "ram_util_percent": 18.6}}
{"episode_reward_max": 52464.0, "episode_reward_min": 29502.0, "episode_reward_mean": 39319.74, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [37266.0, 38864.0, 37770.0, 43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0, 41132.0, 30714.0, 32680.0, 43342.0, 42710.0, 36076.0, 42078.0, 36334.0, 35196.0, 41890.0, 32254.0, 37358.0, 44870.0, 40688.0, 39710.0, 41204.0, 38148.0, 29502.0, 31776.0, 52314.0, 37904.0, 37040.0, 35812.0, 51432.0, 39258.0, 46904.0, 41232.0, 40820.0, 43986.0, 37622.0, 35072.0, 37690.0, 34090.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22649006031192007, "mean_inference_ms": 1.9897418076378088, "mean_action_processing_ms": 0.06533250675502622, "mean_env_wait_ms": 1.8974217583234134, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 196000, "timesteps_this_iter": 32, "agent_timesteps_total": 196000, "timers": {"load_time_ms": 0.237, "load_throughput": 135055.069, "learn_time_ms": 36.641, "learn_throughput": 873.343}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.032502431422472, "mean_q": 12798.103515625, "min_q": 10754.9814453125, "max_q": 12967.177734375, "cur_lr": 0.6}, "td_error": [20.1298828125, 70.1298828125, 124.1298828125, 124.1298828125, -59.8701171875, 124.1298828125, 124.1298828125, 62.1298828125, 124.1298828125, -1585.62890625, 68.1298828125, 122.1298828125, 12721.177734375, -75.8701171875, 18.1298828125, 86.1298828125, 106.1298828125, -97.8701171875, -2124.06640625, 124.1298828125, -113.8701171875, -17.8701171875, -113.8701171875, -1584.3193359375, 64.1298828125, -81.8701171875, -17.8701171875, -115.8701171875, 62.1298828125, 26.1298828125, -119.8701171875, -153.8701171875], "mean_td_error": 247.14999389648438, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 196000, "num_agent_steps_sampled": 196000, "num_steps_trained": 1560032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1560032, "last_target_update_ts": 196000, "num_target_updates": 391}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12808114961250874, "mean_inference_ms": 1.549262026119991, "mean_action_processing_ms": 0.05670904537486621, "mean_env_wait_ms": 1.1863139411817802, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": false, "episodes_total": 653, "training_iteration": 196, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-22-32", "timestamp": 1652707352, "time_this_iter_s": 10.396312713623047, "time_total_s": 2740.5566277503967, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2740.5566277503967, "timesteps_since_restore": 6272, "iterations_since_restore": 196, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 21.87333333333333, "ram_util_percent": 18.6}}
{"episode_reward_max": 52464.0, "episode_reward_min": 29502.0, "episode_reward_mean": 39354.96, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [43178.0, 37394.0, 50296.0, 38490.0, 42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0, 41132.0, 30714.0, 32680.0, 43342.0, 42710.0, 36076.0, 42078.0, 36334.0, 35196.0, 41890.0, 32254.0, 37358.0, 44870.0, 40688.0, 39710.0, 41204.0, 38148.0, 29502.0, 31776.0, 52314.0, 37904.0, 37040.0, 35812.0, 51432.0, 39258.0, 46904.0, 41232.0, 40820.0, 43986.0, 37622.0, 35072.0, 37690.0, 34090.0, 37652.0, 37904.0, 41866.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22643409592763636, "mean_inference_ms": 1.9892306998107019, "mean_action_processing_ms": 0.06531235415879932, "mean_env_wait_ms": 1.8960142783602363, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 197000, "timesteps_this_iter": 32, "agent_timesteps_total": 197000, "timers": {"load_time_ms": 0.235, "load_throughput": 136040.673, "learn_time_ms": 34.91, "learn_throughput": 916.652}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.00771897379308939, "mean_q": 12823.2890625, "min_q": 10606.0927734375, "max_q": 12980.69140625, "cur_lr": 0.6}, "td_error": [39.1015625, -114.8984375, 73.1015625, -1452.0439453125, -2305.4970703125, -92.8984375, 131.1015625, 125.1015625, 57.1015625, 131.1015625, -110.8984375, 127.1015625, 12700.69140625, -1400.0439453125, 59.1015625, 127.1015625, -166.8984375, -74.8984375, -122.8984375, -90.8984375, -64.8984375, 93.1015625, 131.1015625, 127.1015625, -116.8984375, -130.8984375, 129.1015625, 131.1015625, -96.8984375, -98.8984375, -136.8984375, 105.1015625], "mean_td_error": 240.93594360351562, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 197000, "num_agent_steps_sampled": 197000, "num_steps_trained": 1568032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1568032, "last_target_update_ts": 197000, "num_target_updates": 393}, "done": false, "episodes_total": 656, "training_iteration": 197, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-22-45", "timestamp": 1652707365, "time_this_iter_s": 12.699241399765015, "time_total_s": 2753.2558691501617, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2753.2558691501617, "timesteps_since_restore": 6304, "iterations_since_restore": 197, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.177777777777777, "ram_util_percent": 18.600000000000005}}
{"episode_reward_max": 52464.0, "episode_reward_min": 29502.0, "episode_reward_mean": 39367.52, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 4, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [42314.0, 37620.0, 38796.0, 31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0, 41132.0, 30714.0, 32680.0, 43342.0, 42710.0, 36076.0, 42078.0, 36334.0, 35196.0, 41890.0, 32254.0, 37358.0, 44870.0, 40688.0, 39710.0, 41204.0, 38148.0, 29502.0, 31776.0, 52314.0, 37904.0, 37040.0, 35812.0, 51432.0, 39258.0, 46904.0, 41232.0, 40820.0, 43986.0, 37622.0, 35072.0, 37690.0, 34090.0, 37652.0, 37904.0, 41866.0, 34914.0, 45062.0, 46830.0, 43808.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22634166421145652, "mean_inference_ms": 1.9883757621948, "mean_action_processing_ms": 0.06527972239666703, "mean_env_wait_ms": 1.894018095105443, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 198000, "timesteps_this_iter": 32, "agent_timesteps_total": 198000, "timers": {"load_time_ms": 0.128, "load_throughput": 250126.217, "learn_time_ms": 20.141, "learn_throughput": 1588.776}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.003781101666390896, "mean_q": 12770.9111328125, "min_q": 10837.056640625, "max_q": 12992.9560546875, "cur_lr": 0.6}, "td_error": [128.8115234375, 130.8115234375, -15.1884765625, -1526.1962890625, 128.8115234375, -91.1884765625, 128.8115234375, 130.8115234375, 68.8115234375, -1957.30078125, -1475.599609375, 52.8115234375, 126.8115234375, 12690.9560546875, 62.8115234375, -75.1884765625, -13.1884765625, -2217.087890625, 130.8115234375, -65.1884765625, -117.1884765625, 128.8115234375, 90.8115234375, 76.8115234375, -113.1884765625, -81.1884765625, 130.8115234375, 96.8115234375, 128.8115234375, -17.1884765625, -17.1884765625, 76.8115234375], "mean_td_error": 210.27133178710938, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 198000, "num_agent_steps_sampled": 198000, "num_steps_trained": 1576032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1576032, "last_target_update_ts": 198000, "num_target_updates": 395}, "done": false, "episodes_total": 660, "training_iteration": 198, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-22-55", "timestamp": 1652707375, "time_this_iter_s": 10.122724771499634, "time_total_s": 2763.3785939216614, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2763.3785939216614, "timesteps_since_restore": 6336, "iterations_since_restore": 198, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.033333333333335, "ram_util_percent": 18.6}}
{"episode_reward_max": 52464.0, "episode_reward_min": 29502.0, "episode_reward_mean": 39318.76, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [31178.0, 36354.0, 37928.0, 46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0, 41132.0, 30714.0, 32680.0, 43342.0, 42710.0, 36076.0, 42078.0, 36334.0, 35196.0, 41890.0, 32254.0, 37358.0, 44870.0, 40688.0, 39710.0, 41204.0, 38148.0, 29502.0, 31776.0, 52314.0, 37904.0, 37040.0, 35812.0, 51432.0, 39258.0, 46904.0, 41232.0, 40820.0, 43986.0, 37622.0, 35072.0, 37690.0, 34090.0, 37652.0, 37904.0, 41866.0, 34914.0, 45062.0, 46830.0, 43808.0, 46318.0, 31566.0, 35970.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22627414877759766, "mean_inference_ms": 1.9876864672387933, "mean_action_processing_ms": 0.065255975973858, "mean_env_wait_ms": 1.8925312816535993, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 199000, "timesteps_this_iter": 32, "agent_timesteps_total": 199000, "timers": {"load_time_ms": 0.219, "load_throughput": 146127.085, "learn_time_ms": 34.434, "learn_throughput": 929.318}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0037312752101570368, "mean_q": 12773.587890625, "min_q": 10668.2724609375, "max_q": 13013.52734375, "cur_lr": 0.6}, "td_error": [-101.681640625, 148.318359375, -2406.9365234375, 38.318359375, -85.681640625, 144.318359375, -1372.3359375, 130.318359375, -1885.3251953125, 78.318359375, -1.681640625, 52.318359375, -81.681640625, -43.681640625, -41.681640625, -81.681640625, -2000.2041015625, 126.318359375, 148.318359375, -89.681640625, -87.681640625, 42.318359375, 12711.52734375, 110.318359375, 140.318359375, 146.318359375, -61.681640625, 144.318359375, 148.318359375, -73.681640625, -9.681640625, 148.318359375], "mean_td_error": 188.54129028320312, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 199000, "num_agent_steps_sampled": 199000, "num_steps_trained": 1584032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1584032, "last_target_update_ts": 199000, "num_target_updates": 397}, "done": false, "episodes_total": 663, "training_iteration": 199, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-23-09", "timestamp": 1652707389, "time_this_iter_s": 13.459162473678589, "time_total_s": 2776.83775639534, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2776.83775639534, "timesteps_since_restore": 6368, "iterations_since_restore": 199, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 23.352631578947367, "ram_util_percent": 18.605263157894743}}
{"episode_reward_max": 52464.0, "episode_reward_min": 29502.0, "episode_reward_mean": 39491.78, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 3, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [46900.0, 41130.0, 38542.0, 35342.0, 36460.0, 39430.0, 34406.0, 40222.0, 40478.0, 38040.0, 36278.0, 37908.0, 41366.0, 46718.0, 52464.0, 34788.0, 41246.0, 34210.0, 37246.0, 31196.0, 43802.0, 38440.0, 38034.0, 33278.0, 40838.0, 41586.0, 37390.0, 40478.0, 49626.0, 40638.0, 32756.0, 40450.0, 38244.0, 38470.0, 39142.0, 42832.0, 39500.0, 37720.0, 32788.0, 40452.0, 48918.0, 40362.0, 37142.0, 45218.0, 38628.0, 39508.0, 34244.0, 41230.0, 43528.0, 48954.0, 37824.0, 36598.0, 35060.0, 37640.0, 41132.0, 30714.0, 32680.0, 43342.0, 42710.0, 36076.0, 42078.0, 36334.0, 35196.0, 41890.0, 32254.0, 37358.0, 44870.0, 40688.0, 39710.0, 41204.0, 38148.0, 29502.0, 31776.0, 52314.0, 37904.0, 37040.0, 35812.0, 51432.0, 39258.0, 46904.0, 41232.0, 40820.0, 43986.0, 37622.0, 35072.0, 37690.0, 34090.0, 37652.0, 37904.0, 41866.0, 34914.0, 45062.0, 46830.0, 43808.0, 46318.0, 31566.0, 35970.0, 45822.0, 37568.0, 39372.0], "episode_lengths": [300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.22621189339726155, "mean_inference_ms": 1.9870475890273023, "mean_action_processing_ms": 0.06523422925628149, "mean_env_wait_ms": 1.891100131526252, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 0, "timesteps_total": 200000, "timesteps_this_iter": 32, "agent_timesteps_total": 200000, "timers": {"load_time_ms": 0.234, "load_throughput": 136469.474, "learn_time_ms": 35.978, "learn_throughput": 889.423}, "info": {"learner": {"default_policy": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 0.0411396250128746, "mean_q": 12958.572265625, "min_q": 11383.400390625, "max_q": 13009.384765625, "cur_lr": 0.6}, "td_error": [12753.384765625, 40.1591796875, 98.1591796875, 34.1591796875, 20.1591796875, 32.1591796875, -63.8408203125, -115.8408203125, -119.8408203125, 50.1591796875, -111.8408203125, -1591.8251953125, 72.1591796875, -87.8408203125, 124.1591796875, 130.1591796875, -31.8408203125, 94.1591796875, 132.1591796875, 120.1591796875, -135.8408203125, 118.1591796875, -109.8408203125, 132.1591796875, 58.1591796875, 66.1591796875, -85.8408203125, 130.1591796875, 34.1591796875, 72.1591796875, -67.8408203125, 132.1591796875], "mean_td_error": 372.5729675292969, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 32.0}}, "num_steps_sampled": 200000, "num_agent_steps_sampled": 200000, "num_steps_trained": 1592032, "num_steps_trained_this_iter": 32, "num_agent_steps_trained": 1592032, "last_target_update_ts": 200000, "num_target_updates": 399}, "evaluation": {"episode_reward_max": 29448.0, "episode_reward_min": 29448.0, "episode_reward_mean": 29448.0, "episode_len_mean": 300.0, "episode_media": {}, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [29448.0], "episode_lengths": [300]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.12850826576974947, "mean_inference_ms": 1.5548758566852379, "mean_action_processing_ms": 0.05693672800149912, "mean_env_wait_ms": 1.1898655627904022, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "timesteps_this_iter": 300}, "done": true, "episodes_total": 666, "training_iteration": 200, "trial_id": "d0374_00000", "experiment_id": "7c64f8455f27473995e70a25e5aeae69", "date": "2022-05-16_13-23-25", "timestamp": 1652707405, "time_this_iter_s": 16.052628755569458, "time_total_s": 2792.8903851509094, "pid": 14109, "hostname": "dt-oran-nr-ric", "node_ip": "132.72.81.250", "config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 4, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": true, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {"num_workers": 0, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 1, "batch_mode": "complete_episodes", "gamma": 0.99, "lr": 0.6, "train_batch_size": 32, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "CNN", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "Single-CityFlow", "observation_space": null, "action_space": null, "env_config": {"config_path": "examples/hangzhou_1x1_bc-tyc_18041607_1h/config.json", "steps_per_episode": 600, "reward_func": "waiting_count"}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "eager_max_retraces": 20, "explore": false, "exploration_config": {"type": "EpsilonGreedy", "initial_epsilon": 1.0, "final_epsilon": 0.02, "epsilon_timesteps": 10000}, "evaluation_interval": 4, "evaluation_duration": 1, "evaluation_duration_unit": "episodes", "evaluation_parallel_to_training": false, "in_evaluation": true, "evaluation_config": {"explore": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "evaluation_num_workers": 0, "custom_eval_function": null, "always_attach_evaluation_results": false, "keep_per_episode_custom_metrics": false, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "metrics_episode_collection_timeout_s": 180, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_reporting": 1, "min_train_timesteps_per_reporting": null, "min_sample_timesteps_per_reporting": 1000, "seed": 123, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"default_policy": ["<class 'ray.rllib.policy.policy_template.DQNTorchPolicy'>", "Box([[[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]\n\n\n [[[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]]], [[[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]\n\n\n [[[255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   ...\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]\n   [255. 255. 255. ... 255. 255. 255.]]]], (4, 1, 32, 40), float64)", "Discrete(9)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": null, "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_execution_plan_api": false, "disable_env_checking": false, "simple_optimizer": false, "monitor": -1, "evaluation_num_episodes": -1, "metrics_smoothing_episodes": -1, "timesteps_per_iteration": 1000, "min_iter_time_s": -1, "collect_metrics_timeout": -1, "target_network_update_freq": 500, "buffer_size": -1, "replay_buffer_config": {"type": "MultiAgentReplayBuffer", "capacity": 50000}, "store_buffer_in_checkpoints": false, "replay_sequence_length": 1, "lr_schedule": null, "adam_epsilon": 1e-08, "grad_clip": 40, "learning_starts": 1000, "num_atoms": 1, "v_min": -10.0, "v_max": 10.0, "noisy": false, "sigma0": 0.5, "dueling": true, "hiddens": [256], "double_q": true, "n_step": 1, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "final_prioritized_replay_beta": 0.4, "prioritized_replay_beta_annealing_timesteps": 20000, "prioritized_replay_eps": 1e-06, "before_learn_on_batch": null, "training_intensity": null, "worker_side_prioritization": false}, "time_since_restore": 2792.8903851509094, "timesteps_since_restore": 6400, "iterations_since_restore": 200, "warmup_time": 2.1482925415039062, "perf": {"cpu_util_percent": 22.94347826086957, "ram_util_percent": 18.660869565217386}}
